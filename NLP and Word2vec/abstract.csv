ID,BIBID,ABST
1,Zhao2018, Recently image forensics community has paied attention to the research on the design of effective algorithms based on deep learning technology and facts proved that combining the domain knowledge of image forensics and deep learning would achieve more robust and better performance than the traditional schemes. Instead of improving it in this paper the safety of deep learning based methods in the field of image forensics is taken into account. To the best of our knowledge this is a first work focusing on this topic. Specifically we experimentally find that the method using deep learning would fail when adding the slight noise into the images (adversarial images). Furthermore two kinds of strategys are proposed to enforce security of deep learning-based method. Firstly an extra penalty term to the loss function is added which is referred to the 2-norm of the gradient of the loss with respect to the input images and then an novel training method are adopt to train the model by fusing the normal and adversarial images. Experimental results show that the proposed algorithm can achieve good performance even in the case of adversarial images and provide a safety consideration for deep learning-based image forensics
2,Kamilaris2018, Monitoring of disasters is crucial for mitigating their effects on the environment and human population and can be facilitated by the use of unmanned aerial vehicles (UAV) equipped with camera sensors that produce aerial photos of the areas of interest. A modern technique for recognition of events based on aerial photos is deep learning. In this paper we present the state of the art work related to the use of deep learning techniques for disaster identification. We demonstrate the potential of this technique in identifying disasters with high accuracy by means of a relatively simple deep learning model. Based on a dataset of 544 images (containing disaster images such as fires earthquakes collapsed buildings tsunami and flooding as well as non-disaster scenes) our results show an accuracy of 91% achieved indicating that deep learning combined with UAV equipped with camera sensors have the potential to predict disasters with high accuracy.
3,Liang2018, X-ray Computed Tomography (CT) imaging has been widely used in clinical diagnosis non-destructive examination and public safety inspection. Sparse-view (sparse view) CT has great potential in radiation dose reduction and scan acceleration. However sparse view CT data is insufficient and traditional reconstruction results in severe streaking artifacts. In this work based on deep learning we compared image reconstruction performance for sparse view CT reconstruction with projection domain network image domain network and comprehensive network combining projection and image domains. Our study is executed with numerical simulated projection of CT images from real scans. Results demonstrated deep learning networks can effectively reconstruct rich high frequency structural information without streaking artefact commonly seen in sparse view CT. A comprehensive network combining deep learning in both projection domain and image domain can get best results.
4,Chikka2018, Mining relationships between treatment(s) and medical problem(s) is vital in the biomedical domain. This helps in various applications such as decision support system safety surveillance and new treatment discovery. We propose a deep learning approach that utilizes both word level and sentence-level representations to extract the relationships between treatment and problem. While deep learning techniques demand a large amount of data for training we make use of a rule-based system particularly for relationship classes with fewer samples. Our final relations are derived by jointly combining the results from deep learning and rule-based models. Our system achieved a promising performance on the relationship classes of I2b2 2010 relation extraction task.
5,Kuper2018, The increasing use of deep neural networks for safety-critical applications such as autonomous driving and flight control raises concerns about their safety and reliability. Formal verification can address these concerns by guaranteeing that a deep learning system operates as intended but the state of the art is limited to small systems. In this work-in-progress report we give an overview of our work on mitigating this difficulty by pursuing two complementary directions: devising scalable verification techniques and identifying design choices that result in deep learning systems that are more amenable to verification.
6,Meng2017, Deep learning has shown promising results on hard perceptual problems in recent years. However deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective. In this paper we propose MagNet a framework for defending neural network classifiers against adversarial examples. MagNet does not modify the protected classifier or know the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. Different from previous work MagNet learns to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since it does not rely on any process for generating adversarial examples it has substantial generalization power. Moreover MagNet reconstructs adversarial examples by moving them towards the manifold which is effective for helping classify adversarial examples with small perturbation correctly. We discuss the intrinsic difficulty in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography we propose to use diversity to strengthen MagNet. We show empirically that MagNet is effective against most advanced state-of-the-art attacks in blackbox and graybox scenarios while keeping false positive rate on normal examples very low.
7,Dubal2018, This paper demonstrates the effectiveness of our customized deep learning based video analytics system in various applications focused on security safety customer analytics and process compliance. We describe our video analytics system comprising of Search Summarize Statistics and real-time alerting and outline its building blocks. These building blocks include object detection tracking face detection and recognition human and face sub-attribute analytics. In each case we demonstrate how custom models trained using data from the deployment scenarios provide considerably superior accuracies than off-the-shelf models. Towards this end we describe our data processing and model training pipeline which can train and fine-tune models from videos with a quick turnaround time. Finally since most of these models are deployed on-site it is important to have resource constrained models which do not require GPUs. We demonstrate how we custom train resource constrained models and deploy them on embedded devices without significant loss in accuracy. To our knowledge this is the first work which provides a comprehensive evaluation of different deep learning models on various real-world customer deployment scenarios of surveillance video analytics. By sharing our implementation details and the experiences learned from deploying customized deep learning models for various customers we hope that customized deep learning based video analytics is widely incorporated in commercial products around the world.
8,Strauss2017, Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example small adversarial perturbations can cause the system to make errors in important tasks such as classifying traffic signs or detecting pedestrians. Hence in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.
9,Nguyen2018, In a world of global trading maritime safety security and efficiency are crucial issues. We propose a multi-task deep learning framework for vessel monitoring using Automatic Identification System (AIS) data streams. We combine recurrent neural networks with latent variable modeling and an embedding of AIS messages to a new representation space to jointly address key issues to be dealt with when considering AIS data streams: massive amount of streaming data noisy data and irregular timesampling. We demonstrate the relevance of the proposed deep learning framework on real AIS datasets for a three-task setting namely trajectory reconstruction anomaly detection and vessel type identification.
10,Yuan2017, With rapid progress and significant successes in a wide spectrum of applications deep learning is being applied in many safety-critical environments. However deep neural networks have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore attacks and defenses on adversarial examples draw great attention. In this paper we review recent findings on adversarial examples for deep neural networks summarize the methods for generating adversarial examples and propose a taxonomy of these methods. Under the taxonomy applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.
11,Cai2016, Night vision systems get more and more attention in the field of automotive active safety field. In this area a number of researchers have proposed far-infrared sensor based night-time vehicle detection algorithm. However existing algorithms have low performance in some indicators such as the detection rate and processing time. To solve this problem we propose a far-infrared image vehicle detection algorithm based on visual saliency and deep learning. Firstly most of the nonvehicle pixels will be removed with visual saliency computation. Then vehicle candidate will be generated by using prior information such as camera parameters and vehicle size. Finally classifier trained with deep belief networks will be applied to verify the candidates generated in last step. The proposed algorithm is tested in around 6000 images and achieves detection rate of 92.3% and processing time of 25 Hz which is better than existing methods.
12,Feng2018, In recent years pluvial floods caused by extreme rainfall events have occurred frequently. Especially in urban areas they lead to serious damages and endanger the citizens’ safety. Therefore real-time information about such events is desirable. With the increasing popularity of social media platforms such as Twitter or Instagram information provided by voluntary users becomes a valuable source for emergency response. Many applications have been built for disaster detection and flood mapping using crowdsourcing. Most of the applications so far have merely used keyword filtering or classical language processing methods to identify disaster relevant documents based on user generated texts. As the reliability of social media information is often under criticism the precision of information retrieval plays a significant role for further analyses. Thus in this paper high quality eyewitnesses of rainfall and flooding events are retrieved from social media by applying deep learning approaches on user generated texts and photos. Subsequently events are detected through spatiotemporal clustering and visualized together with these high quality eyewitnesses in a web map application. Analyses and case studies are conducted during flooding events in Paris London and Berlin.
13,Huang2018, The ultrasound imaging is one of the most common schemes to detect diseases in the clinical practice. There are many advantages of ultrasound imaging such as safety convenience and low cost. However reading ultrasound imaging is not easy. To support the diagnosis of clinicians and reduce the load of doctors many ultrasound computer-aided diagnosis (CAD) systems are proposed. In recent years the success of deep learning in the image classification and segmentation led to more and more scholars realizing the potential of performance improvement brought by utilizing the deep learning in the ultrasound CAD system. This paper summarized the research which focuses on the ultrasound CAD system utilizing machine learning technology in recent years. This study divided the ultrasound CAD system into two categories. One is the traditional ultrasound CAD system which employed the manmade feature and the other is the deep learning ultrasound CAD system. The major feature and the classifier employed by the traditional ultrasound CAD system are introduced. As for the deep learning ultrasound CAD newest applications are summarized. This paper will be useful for researchers who focus on the ultrasound CAD system.
14,Zhang2018, At present intelligent video analysis technology has been widely used in various fields. Object tracking is one of the important part of intelligent video surveillance but the traditional target tracking technology based on the pixel coordinate system in images still exists some unavoidable problems. Target tracking based on pixel can’t reflect the real position information of targets and it is difficult to track objects across scenes. Based on the analysis of Zhengyou Zhang's camera calibration method this paper presents a method of target tracking based on the target's space coordinate system after converting the 2-D coordinate of the target into 3-D coordinate. It can be seen from the experimental results: Our method can restore the real position change information of targets well and can also accurately get the trajectory of the target in space.
15,Kim2018, Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete the number of bridges that need to be inspected increases and they require much maintenance cost. Therefore a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First a point cloud-based background model is generated in the preliminary flight. Then cracks on the structural surface are detected with the deep learning algorithm and their thickness and length are calculated. In the deep learning method region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.
16,Pan2017, This paper explores the idea of applying a machine learning approach to develop a global road safety performance function (SFP) that can be used to predict the expected crash frequencies of different highways from different regions. A deep belief network (DBN)   one of the most popular deep learning models is introduced as an alternative to the traditional regression models for crash modelling. An extensive empirical study is conducted using three real world crash data sets covering six classes of highways as defined by location (urban vs. rural) number of lanes access control and region. The study involves a number of experiments aiming at addressing several critical questions pertaining to the relative performance of the DBN in terms of network structure training method data size and generalization ability as compared to the traditional regression models. The experimental results have shown that a DBN model could be trained with different crash datasets with prediction performance being at least comparable to that of the locally calibrated negative binomial (NB) model.
17,Yang2018, Recently short-term traffic prediction under conditions with corrupted or missing data has become a popular topic. Since a road section has predictive power regarding the adjacent roads at a specific location this paper proposes a novel hybrid convolutional long short-term memory neural network model based on critical road sections (CRS-ConvLSTM NN) to predict the traffic evolution of global networks. The critical road sections that have the most powerful impact on the subnetwork are identified by a spatiotemporal correlation algorithm. Subsequently the traffic speed of the critical road sections is used as the input to the ConvLSTM to predict the future traffic states of the entire network. The experimental results from a Beijing traffic network indicate that the CRS-ConvLSTM outperforms prevailing deep learning (DL) approaches for cases that consider critical road sections and the results validate the capability and generalizability of the model when predicting with different numbers of critical road sections.
18,Ma2017, This paper proposes a convolutional neural network (CNN)-based method that learns traffic as images and predicts large-scale network-wide traffic speed with a high accuracy. Spatiotemporal traffic dynamics are converted to images describing the time and space relations of traffic flow via a two-dimensional time-space matrix. A CNN is applied to the image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. The effectiveness of the proposed method is evaluated by taking two real-world transportation networks the second ring road and north-east transportation network in Beijing as examples and comparing the method with four prevailing algorithms namely ordinary least squares k-nearest neighbors artificial neural network and random forest and three deep learning architectures namely stacked autoencoder recurrent neural network and long-short-term memory network. The results show that the proposed method outperforms other algorithms by an average accuracy improvement of 42.91% within an acceptable execution time. The CNN can train the model in a reasonable time and thus is suitable for large-scale transportation networks.
19,Ma2018, Because they are key components of aircraft improving the safety reliability and economy of engines is crucial. To ensure flight safety and reduce the cost of maintenance during aircraft engine operation a prognostics and health management system that focuses on fault diagnosis health assessment and life prediction is introduced to solve the problems. Predicting the remaining useful life (RUL) is the most important information for making decisions about aircraft engine operation and maintenance and it relies largely on the selection of performance degradation features. The choice of such features is highly significant but there are some weaknesses in the current algorithm for RUL prediction notably the inability to obtain tendencies from the data. Especially with aircraft engines extracting useful degradation features from multisensor data with complex correlations is a key technical problem that has hindered the implementation of degradation assessment. To solve these problems deep learning has been proposed in recent years to exploit multiple layers of nonlinear information processing for unsupervised self-learning of features. This paper presents a deep learning approach to predict the RUL of an aircraft engine based on a stacked sparse autoencoder and logistic regression. The stacked sparse autoencoder is used to automatically extract performance degradation features from multiple sensors on the aircraft engine and to fuse multiple features through multilayer self-learning. Logistic regression is used to predict the remaining useful life. However the hyperparameters of the deep learning which significantly impact the feature extraction and prediction performance are determined based on expert experience in most cases. The grid search method is introduced in this paper to optimize the hyperparameters of the proposed aircraft engine RUL prediction model. An application of this method of predicting the RUL of an aircraft engine with a benchmark dataset is employed to demonstrate the effectiveness of the proposed approach.
20,Samir2017, We propose a new algorithm for automatic detect violations of traffic rules for improving the people safety on the unregulated pedestrian crossing. The algorithm uses multi-step proceedings. They are zebra detection cars detection and pedestrian detection. For car detection we use faster R-CNN deep learning tool. The algorithm shows promising results in the detection violations of traffic rules.
21,Zhang2018a, Virtual Geographic Environment Cognition is the attempt to understand the human cognition of surface features geographic processes and human behaviour as well as their relationships in the real world. From the perspective of human cognition behaviour analysis and simulation previous work in Virtual Geographic Environments (VGEs) has focused mostly on representing and simulating the real world to create an ‘interpretive’ virtual world and improve an individual’s active cognition. In terms of reactive cognition building a user ‘evaluative’ environment in a complex virtual experiment is a necessary yet challenging task. This paper discusses the outlook of VGEs and proposes a framework for virtual cognitive experiments. The framework not only employs immersive virtual environment technology to create a realistic virtual world but also involves a responsive mechanism to record the user’s cognitive activities during the experiment. Based on the framework this paper presents two potential implementation methods: first training a deep learning model with several hundred thousand street view images scored by online volunteers with further analysis of which visual factors produce a sense of safety for the individual and second creating an immersive virtual environment and Electroencephalogram (EEG)-based experimental paradigm to both record and analyse the brain activity of a user and explore what type of virtual environment is more suitable and comfortable. Finally we present some preliminary findings based on the first method.
22,Xu2018, Railway subgrade defect is the serious threat to train safety. Vehicle-borne GPR method has become the main railway subgrade detection technology with its advantages of rapidness and nondestructiveness. However due to the large amount of detection data and the variety in defect shape and size defect recognition is a challenging task. In this work the method based on deep learning is proposed to recognize defects from the ground penetrating radar (GPR) profile of subgrade detection data. Based on the Faster R-CNN framework the improvement strategies of feature cascade adversarial spatial dropout network (ASDN) Soft-NMS and data augmentation have been integrated to improve recognition accuracy according to the characteristics of subgrade defects. The experimental results indicates that compared with traditional SVM+HOG method and the baseline Faster R-CNN the improved model can achieve better performance. The model robustness is demonstrated by a further comparison experiment of various defect types. In addition the improvements to model performance of each improvement strategy are verified by an ablation experiment of improvement strategies. This paper tries to explore the new thinking for the application of deep learning method in the field of railway subgrade defect recognition.
23,Wei2018, Collision avoidance is a critical task in many applications such as ADAS (advanced driver-assistance systems) industrial automation and robotics. In an industrial automation setting certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g. GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR (Light Detection and Ranging) and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University&rsquo;s Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.
24,Namba2018, Slip-induced fall is one of the main factors causing serious fracture injuries among the elderly. In this paper we propose a fall risk reduction measures for the elderly based on deep reinforcement learning using mobile assistant 
25,Yuan2018, Track status classification is essential for the stability and safety of railway operations nowadays when railway networks are becoming more and more complex and broad. In this situation monitoring systems are already a key element in applications dedicated to evaluating the status of a certain track section often determining whether it is free or occupied by a train. Different technologies have already been involved in the design of monitoring systems including ultrasonic guided waves (UGW). This work proposes the use of the UGW signals captured by a track monitoring system to extract the features that are relevant for determining the corresponding track section status. For that purpose three features of UGW signals have been considered: the root mean square value the energy and the main frequency components. Experimental results successfully validated how these features can be used to classify the track section status into free occupied and broken. Furthermore spatial and temporal dependencies among these features were analysed in order to show how they can improve the final classification performance. Finally a preliminary high-level classification system based on deep learning networks has been envisaged for future works.
26,Mao2018, The food supply chain is a complex system that involves a multitude of &ldquo;stakeholders&rdquo; such as farmers production factories distributors retailers and consumers. &ldquo;Information asymmetry&rdquo; between stakeholders is one of the major factors that lead to food fraud. Some current researches have shown that applying blockchain can help ensure food safety. However they tend to study the traceability of food but not its supervision. This paper provides a blockchain-based credit evaluation system to strengthen the effectiveness of supervision and management in the food supply chain. The system gathers credit evaluation text from traders by smart contracts on the blockchain. Then the gathered text is analyzed directly by a deep learning network named Long Short Term Memory (LSTM). Finally traders&rsquo; credit results are used as a reference for the supervision and management of regulators. By applying blockchain traders can be held accountable for their actions in the process of transaction and credit evaluation. Regulators can gather more reliable authentic and sufficient information about traders. The results of experiments show that adopting LSTM results in better performance than traditional machine learning methods such as Support Vector Machine (SVM) and Navie Bayes (NB) to analyze the credit evaluation text. The system provides a friendly interface for the convenience of users.
27,Yu2017, Predicting large-scale transportation network traffic has become an important and challenging topic in recent decades. Inspired by the domain knowledge of motion prediction in which the future motion of an object can be predicted based on previous scenes we propose a network grid representation method that can retain the fine-scale structure of a transportation network. Network-wide traffic speeds are converted into a series of static images and input into a novel deep architecture namely spatiotemporal recurrent convolutional networks (SRCNs) for traffic forecasting. The proposed SRCNs inherit the advantages of deep convolutional neural networks (DCNNs) and long short-term memory (LSTM) neural networks. The spatial dependencies of network-wide traffic can be captured by DCNNs and the temporal dynamics can be learned by LSTMs. An experiment on a Beijing transportation network with 278 links demonstrates that SRCNs outperform other deep learning-based algorithms in both short-term and long-term traffic prediction.
28,Noguchi2016, We propose an architecture of neural network that can learn and integrate sequential multimodal information using Long Short Term Memory. Our model consists of encoder and decoder LSTMs and multimodal autoencoder. For integrating sequential multimodal information firstly the encoder LSTM encodes a sequential input to a fixed range feature vector for each modality. Secondly the multimodal autoencoder integrates the feature vectors from each modality and generate a fused feature vector which contains sequential multimodal information in a mixed form. The original feature vectors from each modality are re-generated from the fused feature vector in the multimodal autoencoder. The decoder LSTM decodes the sequential inputs from the regenerated feature vector. Our model is trained with the visual and motion sequences of humans and is tested by recall tasks. The experimental results show that our model can learn and remember the sequential multimodal inputs and decrease the ambiguity generated at the learning stage of LSTMs using integrated multimodal information. Our model can also recall the visual sequences from the only motion sequences and vice versa.
29,Wang2014, Vision based vehicle detection is a critical technology that plays an important role in not only vehicle active safety but also road video surveillance application. Traditional shallow model based vehicle detection algorithm still cannot meet the requirement of accurate vehicle detection in these applications. In this work a novel deep learning based vehicle detection algorithm with 2D deep belief network (2D-DBN) is proposed. In the algorithm the proposed 2D-DBN architecture uses second-order planes instead of first-order vector as input and uses bilinear projection for retaining discriminative information so as to determine the size of the deep architecture which enhances the success rate of vehicle detection. On-road experimental results demonstrate that the algorithm performs better than state-of-the-art vehicle detection algorithm in testing data sets.
30,Gao2017, Various biological factors have been implicated in convulsive seizures involving side effects of drugs. For the preclinical safety assessment of drug development it is difficult to predict seizure-inducing side effects. Here we introduced a machine learning-based in vitro system designed to detect seizure-inducing side effects. We recorded local field potentials from the CA1 alveus in acute mouse neocortico-hippocampal slices while 14 drugs were bath-perfused at 5 different concentrations each. For each experimental condition we collected seizure-like neuronal activity and merged their waveforms as one graphic image which was further converted into a feature vector using Caffe an open framework for deep learning. In the space of the first two principal components the support vector machine completely separated the vectors (i.e. doses of individual drugs) that induced seizure-like events and identified diphenhydramine enoxacin strychnine and theophylline as “seizure-inducing” drugs which indeed were reported to induce seizures in clinical situations. Thus this artificial intelligence-based classification may provide a new platform to detect the seizure-inducing side effects of preclinical drugs.
31,Sohaib2018, Due to enhanced safety cost-effectiveness and reliability requirements fault diagnosis of bearings using vibration acceleration signals has been a key area of research over the past several decades. Many fault diagnosis algorithms have been developed that can efficiently classify faults under constant speed conditions. However the performances of these traditional algorithms deteriorate with fluctuations of the shaft speed. In the past couple of years deep learning algorithms have not only improved the classification performance in various disciplines (e.g. in image processing and natural language processing) but also reduced the complexity of feature extraction and selection processes. In this study using complex envelope spectra and stacked sparse autoencoder- (SSAE-) based deep neural networks (DNNs) a fault diagnosis scheme is developed that can overcome fluctuations of the shaft speed. The complex envelope spectrum made the frequency components associated with each fault type vibrant hence helping the autoencoders to learn the characteristic features from the given input signals more readily. Moreover the implementation of SSAE-DNN for bearing fault diagnosis has avoided the need of handcrafted features that are used in traditional fault diagnosis schemes. The experimental results demonstrate that the proposed scheme outperforms conventional fault diagnosis algorithms in terms of fault classification accuracy when tested with variable shaft speed data.
32,Shaheryar2016, Sensors health monitoring is essentially important for reliable functioning of safety-critical chemical and nuclear power plants. Autoassociative neural network (AANN) based empirical sensor models have widely been reported for sensor calibration monitoring. However such ill-posed data driven models may result in poor generalization and robustness. To address above-mentioned issues several regularization heuristics such as training with jitter weight decay and cross-validation are suggested in literature. Apart from these regularization heuristics traditional error gradient based supervised learning algorithms for multilayered AANN models are highly susceptible of being trapped in local optimum. In order to address poor regularization and robust learning issues here we propose a denoised autoassociative sensor model (DAASM) based on deep learning framework. Proposed DAASM model comprises multiple hidden layers which are pretrained greedily in an unsupervised fashion under denoising autoencoder architecture. In order to improve robustness dropout heuristic and domain specific data corruption processes are exercised during unsupervised pretraining phase. The proposed sensor model is trained and tested on sensor data from a PWR type nuclear power plant. Accuracy autosensitivity spillover and sequential probability ratio test (SPRT) based fault detectability metrics are used for performance assessment and comparison with extensively reported five-layer AANN model by Kramer.
33,Toth2018, The ongoing proliferation of remote sensing technologies in the consumer market has been rapidly reshaping the geospatial data acquisition world and subsequently the data processing as well as information dissemination processes. Smartphones have clearly established themselves as the primary crowdsourced data generators recently and provide an incredible volume of remote sensed data with fairly good georeferencing. Besides the potential to map the environment of the smartphone users they provide information to monitor the dynamic content of the object space. For example real-time traffic monitoring is one of the most known and widely used real-time crowdsensed application where the smartphones in vehicles jointly contribute to an unprecedentedly accurate traffic flow estimation. Now we are witnessing another milestone to happen as driverless vehicle technologies will become another major source of crowdsensed data. Due to safety concerns the requirements for sensing are higher as the vehicles should sense other vehicles and the road infrastructure under any condition not just daylight in favorable weather conditions and at very fast speed. Furthermore the sensing is based on using redundant and complementary sensor streams to achieve a robust object space reconstruction needed to avoid collisions and maintain normal travel patterns. At this point the remote sensed data in assisted and autonomous vehicles are discarded or partially recorded for R&amp;D purposes. However in the long run as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication technologies mature recording data will become a common place and will provide an excellent source of geospatial information for road mapping traffic monitoring etc. This paper reviews the key characteristics of crowdsourced vehicle data based on experimental data and then the processing aspects including the Data Science and Deep Learning components.
34,Winkle2018, Abstract Introduction Automated vehicles in everyday real-world traffic are predicted to be developed soon (Gasser et al. Rechtsfolgen zunehmender Fahrzeugautomatisierung Wirtschaftsverlag NW Berichte der Bundesanstalt f r Stra enwesen F83 2012). New technologies such as advanced object detection and artificial intelligence (AI) that use machine or deep-learning algorithms will support meeting all the maneuvering challenges involved in different degrees of automation (Society of Automotive Engineers - SAE international Levels of driving automation for on road vehicles Warrendale PA. 2014; National Highway Traffic Safety Administration   NHTSA Preliminary statement of policy concerning automated vehicles Washington DC 2018). For automated series production these vehicles of course must be safe in real-world traffic under all weather conditions. Therefore system validation ethical aspects and testing of automated vehicle functions are fundamental basics for successfully developing market launching ethical and social acceptance. Method In order to test and validate critical poor visibility detection challenges of automated vehicles with reasonable expenditure a first area-wide analysis has been conducted. Because poor visibility restricts human perception similar corresponding to machine perception it was based on a text analysis of 1.28 million area-wide police accident reports   followed by an in-depth case-by-case analysis of 374 identified cases concerning bad weather conditions (see chap. 1.3). For this purpose the first time ever a nationwide analysis included all police reports in the whole area within the state of Saxony from the year 2004 until 2014. Results Within this large database 374 accidents were found due to perception limitations   caused by “rain” “fog” “snow” “glare”/“blinding” and “visual obstruction”   for the detailed case-by-case investigation. All those challenging traffic scenarios are relevant for automated driving. They will form a key aspect for safe development validation and testing of machine perception within automated driving functions. Conclusions This first area-wide analysis does not only rely on samples as in previous in-depth analyses. It provides relevant real-world traffic scenarios for testing of automated vehicles. For the first time this analysis is carried out knowing the place time and context of each accident over the total investigated area of an entire federal state. Thus the accidents that have been analyzed include all kinds of representative situations that can occur on motorways highways main roads side streets or urban traffic. The scenarios can be extrapolated to include similar road networks worldwide. These results additionally will be taken into account for developing standards regarding early simulations as well as for the subsequent real-life testing. In the future vehicle operation data and traffic simulations could be included as well. Based on these relevant real-world accidents culled from the federal accident database for Saxony the authors recommend further development of internationally valid guidelines based on ethical legal requirements and social acceptance.
35,Jakaria2018, Poster Abstract: Safety Analysis for UAV Networks
36,Kang2018, Poster Abstract: DeepRT: A Predictable Deep Learning Inference Framework for IoT Devices
37,Zhu2018, Ensuring the health and safety of independent-living senior citizens is a growing societal concern. Researchers have developed sensor based systems to monitor senior citizens' Activity of Daily Living (ADL) a set of daily activities that can indicate their self-caring ability. However most ADL monitoring systems are designed for one specific sensor modality resulting in less generalizable models that is not flexible to account variations in real-life monitoring settings. Current classic machine learning and deep learning methods do not provide a generalizable solution to recognize complex ADLs for different sensor settings. This study proposes a novel Sequence-to-Sequence model based deep-learning framework to recognize complex ADLs leveraging an activity state representation. The proposed activity state representation integrated motion and environment sensor data without labor-intense feature engineering. We evaluated our proposed framework against several state-of-the-art machine learning and deep learning benchmarks. Overall our approach outperformed baselines in most performance metrics accurately recognized complex ADLs from different types of sensor input. This framework can generalize to different sensor settings and provide a viable approach to understand senior citizen's daily activity patterns with smart home health monitoring systems.
38,Munkhdalai2018, Medication and adverse drug event (ADE) information extracted from electronic health record (EHR) notes can be a rich resource for drug safety surveillance. Existing observational studies have mainly relied on structured EHR data to obtain ADE information; however ADEs are often buried in the EHR narratives and not recorded in structured data. To unlock ADE-related information from EHR narratives there is a need to extract relevant entities and identify relations among them. In this study we focus on relation identification. This study aimed to evaluate natural language processing and machine learning approaches using the expert-annotated medical entities and relations in the context of drug safety surveillance and investigate how different learning approaches perform under different configurations. We have manually annotated 791 EHR notes with 9 named entities (eg medication indication severity and ADEs) and 7 different types of relations (eg medication-dosage medication-ADE and severity-ADE). Then we explored 3 supervised machine learning systems for relation identification: (1) a support vector machines (SVM) system (2) an end-to-end deep neural network system and (3) a supervised descriptive rule induction baseline system. For the neural network system we exploited the state-of-the-art recurrent neural network (RNN) and attention models. We report the performance by macro-averaged precision recall and F1-score across the relation types. Our results show that the SVM model achieved the best average F1-score of 89.1% on test data outperforming the long short-term memory (LSTM) model with attention (F1-score of 65.72%) as well as the rule induction baseline system (F1-score of 7.47%) by a large margin. The bidirectional LSTM model with attention achieved the best performance among different RNN models. With the inclusion of additional features in the LSTM model its performance can be boosted to an average F1-score of 77.35%. It shows that classical learning models (SVM) remains advantageous over deep learning models (RNN variants) for clinical relation identification especially for long-distance intersentential relations. However RNNs demonstrate a great potential of significant improvement if more training data become available. Our work is an important step toward mining EHRs to improve the efficacy of drug safety surveillance. Most importantly the annotated data used in this study will be made publicly available which will further promote drug safety research in the community.
39,Kwon2018a, In-hospital cardiac arrest is a major burden to public health which affects patient safety. Although traditional track-and-trigger systems are used to predict cardiac arrest early they have limitations with low sensitivity and high false-alarm rates. We propose a deep learning-based early warning system that shows higher performance than the existing track-and-trigger systems. This retrospective cohort study reviewed patients who were admitted to 2 hospitals from June 2010 to July 2017. A total of 52 131 patients were included. Specifically a recurrent neural network was trained using data from June 2010 to January 2017. The result was tested using the data from February to July 2017. The primary outcome was cardiac arrest and the secondary outcome was death without attempted resuscitation. As comparative measures we used the area under the receiver operating characteristic curve (AUROC) the area under the precision-recall curve (AUPRC) and the net reclassification index. Furthermore we evaluated sensitivity while varying the number of alarms. The deep learning-based early warning system (AUROC: 0.850; AUPRC: 0.044) significantly outperformed a modified early warning score (AUROC: 0.603; AUPRC: 0.003) a random forest algorithm (AUROC: 0.780; AUPRC: 0.014) and logistic regression (AUROC: 0.613; AUPRC: 0.007). Furthermore the deep learning-based early warning system reduced the number of alarms by 82.2% 13.5% and 42.1% compared with the modified early warning system random forest and logistic regression respectively at the same sensitivity. An algorithm based on deep learning had high sensitivity and a low false-alarm rate for detection of patients with cardiac arrest in the multicenter study.
40,Moccia2018, Microsurgical procedures such as petroclival meningioma resection require careful surgical actions in order to remove tumor tissue while avoiding brain and vessel damaging. Such procedures are currently performed under microscope magnification. Robotic tools are emerging in order to filter surgeons' unintended movements and prevent tools from entering forbidden regions such as vascular structures. The present work investigates the use of a handheld robotic tool (Micron) to automate vessel avoidance in microsurgery. In particular we focused on vessel segmentation implementing a deep-learning-based segmentation strategy in microscopy images and its integration with a feature-based passive 3D reconstruction algorithm to obtain accurate and robust vessel position. We then implemented a virtual-fixture-based strategy to control the handheld robotic tool and perform vessel avoidance. Clay vascular phantoms lying on a background obtained from microscopy images recorded during petroclival meningioma surgery were used for testing the segmentation and control algorithms. When testing the segmentation algorithm on 100 different phantom images a median Dice similarity coefficient equal to 0.96 was achieved. A set of 25 Micron trials of 80 s in duration each involving the interaction of Micron with a different vascular phantom were recorded with a safety distance equal to 2 mm which was comparable to the median vessel diameter. Micron's tip entered the forbidden region 24% of the time when the control algorithm was active. However the median penetration depth was 16.9 μm which was two orders of magnitude lower than median vessel diameter. Results suggest the system can assist surgeons in performing safe vessel avoidance during neurosurgical procedures.
41,Kim2018a, Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete the number of bridges that need to be inspected increases and they require much maintenance cost. Therefore a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First a point cloud-based background model is generated in the preliminary flight. Then cracks on the structural surface are detected with the deep learning algorithm and their thickness and length are calculated. In the deep learning method region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.
42,Pesapane2018, Worldwide interest in artificial intelligence (AI) applications is growing rapidly. In medicine devices based on machine/deep learning have proliferated especially for image analysis presaging new significant challenges for the utility of AI in healthcare. This inevitably raises numerous legal and ethical questions. In this paper we analyse the state of AI regulation in the context of medical device development and strategies to make AI applications safe and useful in the future. We analyse the legal framework regulating medical devices and data protection in Europe and in the United States assessing developments that are currently taking place. The European Union (EU) is reforming these fields with new legislation (General Data Protection Regulation [GDPR] Cybersecurity Directive Medical Devices Regulation In Vitro Diagnostic Medical Device Regulation). This reform is gradual but it has now made its first impact with the GDPR and the Cybersecurity Directive having taken effect in May 2018. As regards the United States (U.S.) the regulatory scene is predominantly controlled by the Food and Drug Administration. This paper considers issues of accountability both legal and ethical. The processes of medical device decision-making are largely unpredictable therefore holding the creators accountable for it clearly raises concerns. There is a lot that can be done in order to regulate AI applications. If this is done properly and timely the potentiality of AI based technology in radiology as well as in other fields will be invaluable.   AI applications are medical devices supporting detection/diagnosis work-flow cost-effectiveness.   Regulations for safety privacy protection and ethical use of sensitive information are needed.   EU and U.S. have different approaches for approving and regulating new medical devices.   EU laws consider cyberattacks incidents (notification and minimisation) and service continuity.   U.S. laws ask for opt-in data processing and use as well as for clear consumer consent.
43,Fan2018, Despite widespread use the safety of dietary supplements is open to doubt due to the fact that they can interact with prescribed medications leading to dangerous clinical outcomes. Electronic health records (EHRs) provide a potential way for active pharmacovigilance on dietary supplements since a fair amount of dietary supplement information especially those on use status can be found in clinical notes. Extracting such information is extremely significant for subsequent supplement safety research. In this study we collected 2500 sentences for 25 commonly used dietary supplements and annotated into four classes: Continuing (C) Discontinued (D) Started (S) and Unclassified (U). Both rule-based and machine learning-based classifiers were developed on the same training set and evaluated using the hold-out test set. The performances of the two classifiers were also compared. The rule-based classifier achieved F-measure of 0.90 0.85 0.90 and 0.86 in C D S and U status respectively. The optimal machine learning-based classifier (Maximum Entropy) achieved F-measure of 0.90 0.92 0.91 and 0.88 in C D S and U status respectively. The comparison result shows that the machine learning-based classifier has a better performance which is more efficient and scalable especially when the sample size doubles. Machine learning-based classifier outperforms rule-based classifier in categorization of the use status of dietary supplements in clinical notes. Future work includes applying deep learning methods and developing a hybrid system to approach use status classification task.
44,Hu2018, The aim of this study was to investigate if a machine learning algorithm utilizing triaxial accelerometer gyroscope and magnetometer data from an inertial motion unit (IMU) could detect surface- and age-related differences in walking. Seventeen older and eighteen young healthy adults walked over flat and uneven brick surfaces wearing an inertial measurement unit (IMU) over the L5 vertebra. IMU data were binned into smaller data segments using 4-s sliding windows with 1-s step lengths. Ninety percent of the data were used as training inputs and the remaining ten percent were saved for testing. A deep learning network with long short-term memory units was used for training (fully supervised) prediction and implementation. Four models were trained using the following inputs: all nine channels from every sensor in the IMU (fully trained model) accelerometer signals alone gyroscope signals alone and magnetometer signals alone. The fully trained models for surface and age outperformed all other models (area under the receiver operator curve AUC,0.97 and 0.96, respectively; p.045). The fully trained models for surface and age had high accuracy (96.3, 94.7%), precision (96.4, 95.2%), recall (96.3, 94.7%), and f1-score (96.3, 94.6%). These results demonstrate that processing the signals of a single IMU device with machine-learning algorithms enables the detection of surface conditions and age-group status from an individual's walking behavior which, with further learning, may be utilized to facilitate identifying and intervening on fall risk.
45,Ambe2018, In silico prediction for toxicity of chemicals is required to reduce cost time and animal testing. However predicting hepatocellular hypertrophy which often affects the derivation of the No-Observed-Adverse-Effect Level in repeated dose toxicity studies is difficult because pathological findings are diverse mechanisms are largely unknown and a wide variety of chemical structures exists. Therefore a method for predicting the hepatocellular hypertrophy of diverse chemicals without complete understanding of their mechanisms is necessary. In this study we developed predictive classification models of hepatocellular hypertrophy using machine learning-specifically deep learning random forest and support vector machine. We extracted hepatocellular hypertrophy data on rats from 2 toxicological databases our original database developed from risk assessment reports such as pesticides and the Hazard Evaluation Support System Integrated Platform. Then we constructed prediction models based on molecular descriptors and evaluated their performance using independent test chemicals datasets which differed from the training chemicals datasets. Further we defined the applicability domain (AD) which generally limits the application for chemicals as structurally similar to the training chemicals dataset. The best model was found to be the support vector machine model using the Hazard Evaluation Support System Integrated Platform dataset which was trained with 251 chemicals and predicted 214 test chemicals inside the applicability domain. It afforded a prediction accuracy of 0.76 sensitivity of 0.90 and area under the curve of 0.81. These in silico predictive classification models could be reliable tools for hepatocellular hypertrophy assessments and can facilitate the development of in silico models for toxicity prediction.
46,Ma2018b, Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality the lack of interpretability in a DL system makes system analysis and defect detection difficult which could potentially hinder its real-world deployment. In this paper we propose DeepGauge a set of multi-granularity testing criteria for DL systems which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets five DL systems and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.
47,Mueller2018, Driving Policy Transfer via Modularity and Abstraction
48,Driving Policy Transfer via Modularity and Abstraction, End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap safe and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions with no finetuning on two continents. The supplementary video can be viewed at https://youtu.be/BrMDJqI6H5U
49,Ma2018c, Deep learning (DL) has achieved remarkable progress over the past decade and been widely applied to many safety-critical applications. However the robustness of DL systems recently receives great concerns such as adversarial examples against computer vision systems which could potentially result in severe consequences. Adopting testing techniques could help to evaluate the robustness of a DL system and therefore detect vulnerabilities at an early stage. The main challenge of testing such systems is that its runtime state space is too large: if we view each neuron as a runtime state for DL then a DL system often contains massive states rendering testing each state almost impossible. For traditional software combinatorial testing (CT) is an effective testing technique to reduce the testing space while obtaining relatively high defect detection abilities. In this paper we perform an exploratory study of CT on DL systems. We adapt the concept in CT and propose a set of coverage criteria for DL systems as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems. We further pose several open questions and interesting directions for combinatorial testing of DL systems.
50,Liu2018a, Fault diagnosis of rotating machinery plays a significant role for the reliability and safety of modern industrial systems. As an emerging field in industrial applications and an effective solution for fault recognition artificial intelligence (AI) techniques have been receiving increasing attention from academia and industry. However great challenges are met by the AI methods under the different real operating conditions. This paper attempts to present a comprehensive review of AI algorithms in rotating machinery fault diagnosis from both the views of theory background and industrial applications. A brief introduction of different AI algorithms is presented first including the following methods: k-nearest neighbour naive Bayes support vector machine artificial neural network and deep learning. Then a broad literature survey of these AI algorithms in industrial applications is given. Finally the advantages limitations practical implications of different AI algorithms as well as some new research trends are discussed.
51,Hirose2018, We present semi-supervised deep learning approaches for traversability estimation from fisheye images. Our method GONet and the proposed extensions leverage Generative Adversarial Networks (GANs) to effectively predict whether the area seen in the input image(s) is safe for a robot to traverse. These methods are trained with many positive images of traversable places but just a small set of negative images depicting blocked and unsafe areas. This makes the proposed methods practical. Positive examples can be collected easily by simply operating a robot through traversable spaces while obtaining negative examples is time consuming costly and potentially dangerous. Through extensive experiments and several demonstrations we show that the proposed traversability estimation approaches are robust and can generalize to unseen scenarios. Further we demonstrate that our methods are memory efficient and fast allowing for real-time operation on a mobile robot with single or stereo fisheye cameras. As part of our contributions we open-source two new datasets for traversability estimation. These datasets are composed of approximately 24h of videos from more than 25 indoor environments. Our methods outperform baseline approaches for traversability estimation on these new datasets.
52,Jaques2018, In the quest towards general artificial intelligence (AI) researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus we establish that implicit social feedback can improve the output of a deep learning model.
53,Sun2018, The surging availability of electronic medical records (EHR) leads to increased research interests in medical predictive modeling. Recently many deep learning based predicted models are also developed for EHR data and demonstrated impressive performance. However a series of recent studies showed that these deep models are not safe: they suffer from certain vulnerabilities. In short a well-trained deep network can be extremely sensitive to inputs with negligible changes. These inputs are referred to as adversarial examples. In the context of medical informatics such attacks could alter the result of a high performance deep predictive model by slightly perturbing a patient's medical records. Such instability not only reflects the weakness of deep architectures more importantly it offers guide on detecting susceptible parts on the inputs. In this paper we propose an efficient and effective framework that learns a time-preferential minimum attack targeting the LSTM model with EHR inputs and we leverage this attack strategy to screen medical records of patients and identify susceptible events and measurements. The efficient screening procedure can assist decision makers to pay extra attentions to the locations that can cause severe consequence if not measured correctly. We conduct extensive empirical studies on a real-world urgent care cohort and demonstrate the effectiveness of the proposed screening approach.
54,PutraTwinanda2018, Objective: Accurate surgery duration estimation is necessary for optimal OR planning which plays an important role for patient comfort and safety as well as resource optimization. It is however challenging to preoperatively predict surgery duration since it varies significantly depending on the patient condition surgeon skills and intraoperative situation. We present an approach for intraoperative estimation of remaining surgery duration which is well suited for deployment in the OR. Methods: We propose a deep learning pipeline named RSDNet which automatically estimates the remaining surgery duration intraoperatively by using only visual information from laparoscopic videos. An interesting feature of RSDNet is that it does not depend on any manual annotation during training. Results: The experimental results show that the proposed network significantly outperforms the method that is frequently used in surgical facilities for estimating surgery duration. Further the generalizability of the approach is demonstrated by testing the pipeline on two large datasets containing different types of surgeries 120 cholecystectomy and 170 gastric bypass videos. Conclusion: Creation of manual annotations requires expert knowledge and is a time-consuming process especially considering the numerous types of surgeries performed in a hospital and the large number of laparoscopic videos available. Since the proposed pipeline is not reliant on manual annotation it is easily scalable to many types of surgeries. Significance: An improved OR management system could be developed with RSDNet as a result of its superior performance and ability to be efficiently scaled up to many kinds of surgeries.
55,Carrio2018, Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV) navigation. While solutions have been proposed for static obstacle avoidance systems enabling avoidance of dynamic objects such as drones are hard to implement due to the detection range and field-of-view (FOV) requirements as well as the constraints for integrating such systems on-board small UAVs. In this work a dataset of 6k synthetic depth maps of drones has been generated and used to train a state-of-the-art deep learning-based drone detection model. While many sensing technologies can only provide relative altitude and azimuth of an obstacle our depth map-based approach enables full 3D localization of the obstacle. This is extremely useful for collision avoidance as 3D localization of detected drones is key to perform efficient collision-free path planning. The proposed detection technique has been validated in several real depth map sequences with multiple types of drones flying at up to 2 m/s achieving an average precision of 98.7\% an average recall of 74.7\% and a record detection range of 9.5 meters.
56,Eykholt2018, Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work we extend physical attacks to more challenging object detection models a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack in which we cause a Stop sign to ``disappear'' according to the detector-either by covering thesign with an adversarial Stop sign poster or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85\% of the video frames. In an outdoor experiment YOLO was fooled by the poster and sticker attacks in 72.5\% and 63.5\% of the video frames respectively. We also use Faster R-CNN a different object detection model to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9\% of the video frames in a controlled lab environment and 40.2\% of the video frames in an outdoor environment. Finally we present preliminary results with a new Creation Attack where in innocuous physical stickers fool a model into detecting nonexistent objects.
57,Sharma2018, Communications connect the flight deck to the ground and the flight deck to the passengers. On-board communications are provided by public-address systems and aircraft intercoms. Inefficient communication may lead to a catastrophe risking lives and resources. SKYVOIX an intelligent system on board that enhances safety and improves the communication of the aircraft by reducing human error and increasing the awareness. The system is intended to be centralized and installed in addition to the current systems. With high air traffic and busy airspace the transmissions are often misheard causing communication failure which is undesirable. A backup for enabling communication with the ground in emergencies is required which ensures active transmissions and is intelligent enough to handle the gravity of situation. The passengers who are often unaware about the routine and emergency protocols busy with their electronics listening to music cutting them off with the environment shall be benefited. The system works on encrypted conversions in real time converting ATC cabin crew\rsquos instructions and delivering apt information to appropriate sections which is achieved by Deep learning framework TensorFlow. The system shall also be used for real time flight planning by ATC based upon traffic density. The stability of the system is studied with effective layouts reducing the errors and maintaining the overall neutrality. The on-board data shall be stored analyzed which shall help in the maintenance and crash investigations. The system proves to be efficient reliable and practical enhancing the aviation safety and communication that can never be compromised in aviation.
58,Ofei-Manu2018, Quality learning for sustainability can have a transformative effect in terms of promoting empowerment leadership and wise investments in individual and collective lives and regenerating the local economies of cities making them more inclusive safe resilient and sustainable. It can also help cities move towards achieving the United Nations Sustainable Development Goals (SDGs). Effecting the transformation of cities into Learning Cities however requires changes in the structure of governance. Drawing on interviews with key informants as well as secondary data this article examines how collaborative governance has facilitated quality learning for sustainability in Bristol (United Kingdom) Kitakyushu (Japan) and Tongyeong (Republic of Korea). Focusing on a conceptual framework and practical application of learning initiatives this comparative study reveals how these cities' governance mechanisms and institutional structures supported initiatives premised on cooperative learning relationships. While recognising differences in the scope and depth of the learning initiatives and the need for further improvements the authors found evidence of general support for the governance structures and mechanisms for learning in these cities. The authors conclude by recommending that (1) to implement the Learning Cities concept based on UNESCO's Key Features of Learning Cities recognition should be given to existing sustainability-related learning initiatives in cities; (2) collaborative governance of the Learning Cities concept at both local and international levels should be streamlined; and (3) UNESCO's Global Network of Learning Cities could serve as a hub for sharing education/learning resources and experiences for other international city-related programmes as an important contribution to the implementation of the SDGs.
59,AshrafulAlamMilton2018, The convolutional neural network is the crucial tool for the recent success of deep learning based methods on various computer vision tasks like classification segmentation and detection. Convolutional neural networks achieved state-of-the-art performance in these tasks and every day pushing the limit of computer vision and AI. However adversarial attack on computer vision systems is threatening their application in the real life and in safety-critical applications. Necessarily Finding adversarial examples are important to detect susceptible models to attack and take safeguard measures to overcome the adversarial attacks. In this regard MCS 2018 Adversarial Attacks on Black Box Face Recognition challenge aims to facilitate the research of finding new adversarial attack techniques and their effectiveness in generating adversarial examples. In this challenge the attack``s nature is targeted-attack on the black-box neural network where we have no knowledge about black-block''s inner structure. The attacker must modify a set of five images of a single person so that the neural network miss-classify them as target image which is a set of five images of another person. In this competition we applied Momentum Diverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) to make an adversarial attack on black-box face recognition system. We tested our method on MCS 2018 Adversarial Attacks on Black Box Face Recognition challenge and found competitive result. Our solution got validation score 1.404 which better than baseline score 1.407 and stood 14 place among 132 teams in the leader-board. Further improvement can be achieved by finding improved feature extraction from source image carefully chosen hyper-parameters finding improved substitute model of the black-box and better optimization method.
60,Eaton-Rosen2018, Automated medical image segmentation specifically using deep learning has shown outstanding performance in semantic segmentation tasks. However these methods rarely quantify their uncertainty which may lead to errors in downstream analysis. In this work we propose to use Bayesian neural networks to quantify uncertainty within the domain of semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty and calibrate the accuracy and reliability of confidence intervals of derived measurements. When applied to a tumour volume estimation application we demonstrate that by using such modelling of uncertainty deep learning systems can be made to report volume estimates with well-calibrated error-bars making them safer for clinical use. We also show that the uncertainty estimates extrapolate to unseen data and that the confidence intervals are robust in the presence of artificial noise. This could be used to provide a form of quality control and quality assurance and may permit further adoption of deep learning tools in the clinic.
61,Feth2018, Vehicles of higher automation levels require the creation of situation awareness. One important aspect of this situation awareness is an understanding of the current risk of a driving situation. In this work we present a novel approach for the dynamic risk assessment of driving situations based on images of a front stereo camera using deep learning. To this end we trained a deep neural network with recorded monocular images disparity maps and a risk metric for diverse traffic scenes. Our approach can be used to create the aforementioned situation awareness of vehicles of higher automation levels and can serve as a heterogeneous channel to systems based on radar or lidar sensors that are used traditionally for the calculation of risk metrics.
62,Breier2018, As deep learning systems are widely adopted in safety- and security-critical applications such as autonomous vehicles banking systems etc. malicious faults and attacks become a tremendous concern which potentially could lead to catastrophic consequences. In this paper we initiate the first study of leveraging physical fault injection attacks on Deep Neural Networks (DNNs) by using laser injection technique on embedded systems. In particular our exploratory study targets four widely used activation functions in DNNs development that are the general main building block of DNNs that creates non-linear behaviors -- ReLu softmax sigmoid and tanh. Our results show that by targeting these functions it is possible to achieve a misclassification by injecting faults into the hidden layer of the network. Such result can have practical implications for real-world applications where faults can be introduced by simpler means (such as altering the supply voltage).
63,Jungo2018, Uncertainty estimates of modern neuronal networks provide additional information next to the computed predictions and are thus expected to improve the understanding of the underlying model. Reliable uncertainties are particularly interesting for safety-critical computer-assisted applications in medicine e.g. neurosurgical interventions and radiotherapy planning. We propose an uncertainty-driven sanity check for the identification of segmentation results that need particular expert review. Our method uses a fully-convolutional neural network and computes uncertainty estimates by the principle of Monte Carlo dropout. We evaluate the performance of the proposed method on a clinical dataset with 30 postoperative brain tumor images. The method can segment the highly inhomogeneous resection cavities accurately (Dice coefficients 0.792 $\pm$ 0.154). Furthermore the proposed sanity check is able to detect the worst segmentation and three out of the four outliers. The results highlight the potential of using the additional information from the model's parameter uncertainty to validate the segmentation performance of a deep learning model.
64,Duan2018, Nighttime vehicle detection is part of an intelligent transportation system for road safety driving navigation and surveillance at night. However previous nighttime vehicle detection methods only deal with a single class or two classes of vehicles. This paper presents an effective system based on cascade feature selection and a coarse-to-fine mechanism for detecting preceding multiclass vehicles at night. First we train a coarse-level classifier using contrast features. Second we combine a cascade selection framework with feature mutual correlation similarity and regions' overlap to select a set of image regions where we can extract effective features. Three features including local binary pattern histogram of oriented gradients and four direction features are extracted from the selected regions for training a fine-level multiclass vehicle classifier. During the detection stage we utilize a coarse-to-fine mechanism. In the coarse level a multiscale sliding window is classified by the coarse-level classifier to find regions of interest (ROIs) that are likely to be vehicles. In the fine level these ROIs are identified by the trained multiclass vehicle classifier. Evaluations on a Hong Kong nighttime multiclass vehicle dataset show that our proposed system successfully detects a car taxi bus and minibus within nighttime images under different scenes. Quantitatively our proposed system obtains 95.48\% detection rate at 0.055 false positives per image outperforming some state-of-the-art detection approaches including two current nighttime vehicle detection methods and two deep learning-based methods.
65,Sharma2018a, In this paper we train a recurrent neural network to learn dynamics of a chaotic road environment and to project the future of the environment on an image. Future projection can be used to anticipate an unseen environment for example in autonomous driving. Road environment is highly dynamic and complex due to the interaction among traffic participants such as vehicles and pedestrians. Even in this complex environment a human driver is efficacious to safely drive on chaotic roads irrespective of the number of traffic participants. The proliferation of deep learning research has shown the efficacy of neural networks in learning this human behavior. In the same direction we investigate recurrent neural networks to understand the chaotic road environment which is shared by pedestrians vehicles (cars trucks bicycles etc.) and sometimes animals as well. We propose $\backslash$emph$\$Foresee$\$ a unidirectional gated recurrent units (GRUs) network with attention to project future of the environment in the form of images. We have collected several videos on Delhi roads consisting of various traffic participants background and infrastructure differences (like 3D pedestrian crossing) at various times on various days. We train $\backslash$emph$\$Foresee$\$ in an unsupervised way and we use online training to project frames up to $0.5$ seconds in advance. We show that our proposed model performs better than state of the art methods (prednet and Enc. Dec. LSTM) and finally we show that our trained model generalizes to a public dataset for future projections.
66,Miseikis2018, A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification or self-driving cars however it is very limited for the industrial robotics field. In previous work we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image but it was limited to a range of robots produced by Universal Robots (UR). In this work we extend our method to work with a new robot arm - Kuka LBR iiwa which has a significantly different appearance and an additional joint. However instead of collecting large datasets once again we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field but it requires smaller well-prepared training datasets trains significantly faster and reaches similar accuracy compared to the original method even improving it on some aspects.
67,Scaman2018, Deep neural networks are notorious for being sensitive to small well-chosen perturbations and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First we show that even for two layer neural networks the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then we both extend and improve previous estimation methods by providing AutoLip the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation allowing efficient computations even on large convolutions. Second for sequential neural networks we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds.
68,Amini2018, End-to-end trained neural networks (NNs) are a compelling approach to autonomous vehicle control because of their ability to learn complex tasks without manual engineering of rule-based decisions. However challenging road conditions ambiguous navigation situations and safety considerations require reliable uncertainty estimation for the eventual adoption of full-scale autonomous vehicles. Bayesian deep learning approaches provide a way to estimate uncertainty by approximating the posterior distribution of weights given a set of training data. Dropout training in deep NNs approximates Bayesian inference in a deep Gaussian process and can thus be used to estimate model uncertainty. In this paper we propose a Bayesian NN for end-to-end control that estimates uncertainty by exploiting feature map correlation during training. This approach achieves improved model fits as well as tighter uncertainty estimates than traditional element-wise dropout. We evaluate our algorithms on a challenging dataset collected over many different road types times of day and weather conditions and demonstrate how uncertainties can be used in conjunction with a human controller in a parallel autonomous setting.
69,Zhu2018a, Geological hazards such as landslides debris flows and collapses are potential hazards affecting the safety of nearby roads and people. Land and Resources Bureau and other relevant departments to undertake the responsibility of prevention and control of geological disasters an important body how to deal with the characteristics of sudden geological disasters in the region according to pre-established emergency measures quickly and accurately survey is an important issue to be solved. Based on the analysis of the types and effects of typical geological disasters this paper studies the relevant methods of identifying typical geological disasters through artificial neural networks and proposes and designs intelligent geological survey methods and systems based on deep learning to provide relevant departments such as Land and Resources Bureau Related Mountain Geological Survey and Information Support.
70,Liang2018a, The Transportation Security Administration safeguards all United States air travel. To do so they employ human inspectors to screen x-ray images of carry-on baggage for threats and other prohibited items which can be challenging. On the other hand recent research applying deep learning techniques to computer-aided security screening to assist operators has yielded encouraging results. Deep learning is a subfield of machine learning based on learning abstractions from data as opposed to engineering features by hand. These techniques have proven to be quite effective in many domains including computer vision natural language processing speech recognition self-driving cars and geographical mapping technology. In this paper we present initial results of a collaboration between Smiths Detection and Duke University funded by the Transportation Security Administration. Using convolutional object detection algorithms trained on annotated x-ray images we show real-time detection of prohibited items in carry-on luggage. Results of the work so far indicate that this approach can detect selected prohibited items with high accuracy and minimal impact on operational false alarm rates.
71,Feng2018a, To assure that an autonomous car is driving safely on public roads its deep learning-based object detector should not only predict correctly but show its prediction confidence as well. In this work we present practical methods to capture uncertainties in object detection for autonomous driving. We propose a probabilistic 3D vehicle detector for Lidar point clouds that can model both classification and spatial uncertainty. Experimental results show that our method captures reliable uncertainties related to the detection accuracy vehicle distance and occlusion. The results also show that we can improve the detection performance by 1\%-5\% by modeling the uncertainty.
72,Oyama2018, NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used in deep learning. Specifically cuDNN implements several equivalent convolution algorithms whose performance and memory footprint may vary considerably depending on the layer dimensions. When an algorithm is automatically selected by cuDNN the decision is performed on a per-layer basis and thus it often resorts to slower algorithms that fit the workspace size constraints. We present $\$$\backslash$mu$\$-cuDNN a transparent wrapper library for cuDNN which divides layers' mini-batch computation into several micro-batches. Based on Dynamic Programming and Integer Linear Programming $\$$\backslash$mu$\$-cuDNN enables faster algorithms by decreasing the workspace requirements. At the same time $\$$\backslash$mu$\$-cuDNN keeps the computational semantics unchanged so that it decouples statistical efficiency from the hardware efficiency safely. We demonstrate the effectiveness of $\$$\backslash$mu$\$-cuDNN over two frameworks Caffe and TensorFlow achieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2 GPU. These results indicate that using micro-batches can seamlessly increase the performance of deep learning while maintaining the same memory footprint.
73,Guo2018, In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements design source code test cases and other artifacts however creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links however current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.
74,Chen2018, Regularly inspecting the components inside nuclear power plants is necessary to ensure their safe performance. Current inspection practices however are time consuming tedious and subjective that need technicians to watch videos and manually annotate cracks. While an autonomous inspection approach is desirable state-of-the-art crack detection algorithms cannot perform well since the cracks on nuclear power plant reactors are typically very tiny with low contrast. The existence of scratches welds and grind marks on the surface makes the autonomous detection even more challenging. This study introduces a new framework that consists of a convolutional neural network (CNN) based on deep learning a spatiotemporal registration process and a Nave Bayes data fusion scheme based on statistics. The proposed framework is evaluated using several inspection videos and achieves 98.3\% hit rate against 0.1 false positives per frame where the hit rate is much higher than other state-of-the-art algorithms.
75,Kang2018a, Civil infrastructure is important to ensure the ongoing functionality of human living environments. However in North America much of the infrastructure is aging and requires continuous monitoring and maintenance to ensure the safety of people. Traditionally visual inspection has been carried out to monitor the health of such structures. However assessments require trained inspectors and monitoring methods are difficult due to the size and location of the infrastructure. Recently data acquisition using unmanned aerial vehicles (UAVs) equipped with cameras has been growing in popularity and research has been conducted concerning the use of UAVs for the visual inspection of infrastructure. However UAV inspection requires skilled pilots and the use of a global positioning system (GPS) for autonomous flight. Unfortunately for some locations a GPS signal cannot be reached for autonomous flight of the UAV. For example the GPS signal on the inside of a building or underneath a bridge deck is unreliable but these locations also require inspections to ensure structural health. In order to address this issue autonomous UAV methods using ultrasonic beacons have been proposed. Beacons are able to provide positional data allowing UAVs to perform the autonomous mission. As an example of structural damage we report the successful detection of concrete cracks using a deep convolutional neural network by processing the video data collected from an autonomous UAV.
76,Dao2018, It is safe to assume that for the foreseeable future machine learning especially deep learning will remain both data- and computation-hungry. In this paper we ask: Can we build a global exchange where everyone can contribute computation and data to train the next generation of machine learning applications  We present an early but running prototype of DataBright a system that turns the creation of training examples and the sharing of computation into an investment mechanism. Unlike most crowdsourcing platforms where the contributor gets paid when they submit their data DataBright pays dividends whenever a contributor's data or hardware is used by someone to train a machine learning model. The contributor becomes a shareholder in the dataset they created. To enable the measurement of usage a computation platform that contributors can trust is also necessary. DataBright thus merges both a data market and a trusted computation market. We illustrate that trusted computation can enable the creation of an AI market where each data point has an exact value that should be paid to its creator. DataBright allows data creators to retain ownership of their contribution and attaches to it a measurable value. The value of the data is given by its utility in subsequent distributed computation done on the DataBright computation market. The computation market allocates tasks and subsequent payments to pooled hardware. This leads to the creation of a decentralized AI cloud. Our experiments show that trusted hardware such as Intel SGX can be added to the usual ML pipeline with no additional costs. We use this setting to orchestrate distributed computation that enables the creation of a computation market. DataBright is available for download at https://github.com/ds3lab/databright.
77,Ritika2018, Train Pilot is a very tedious and stressful job. Pilots must be vigilant at all times and its easy for them to lose track of time of shift. In countries like USA the pilots are mandated by law to adhere to 8 hour shifts. If they exceed 8 hours of shift the railroads may be penalized for over-tiring their drivers. The problem happens when the 8 hour shift may end in middle of a journey. In such case the new drivers must be moved to the location locomotive is operating for shift change. Hence accurate monitoring of drivers during their shift and making sure the shifts are scheduled correctly is very important for railroads. Here we propose an automated camera system that uses camera mounted inside Locomotive cabs to continuously record video feeds. These feeds are analyzed in real time to detect the face of driver and recognize the driver using state of the art deep Learning techniques. The outcome is an increased safety of train pilots. Cameras continuously capture video from inside the cab which is stored on an on board data acquisition device. Using advanced computer vision and deep learning techniques the videos are analyzed at regular intervals to detect presence of the pilot and identify the pilot. Using a time based analysis it is identified for how long that shift has been active. If this time exceeds allocated shift time an alert is sent to the dispatch to adjust shift hours.
78,HaiPhuongNguyen2018, With robots leaving factories and entering less controlled domains possibly sharing the space with humans safety is paramount and multimodal awareness of the body surface and the surrounding environment is fundamental. Taking inspiration from peripersonal space representations in humans we present a framework on a humanoid robot that dynamically maintains such a protective safety zone composed of the following main components: (i) a human 2D keypoints estimation pipeline employing a deep learning based algorithm extended here into 3D using disparity; (ii) a distributed peripersonal space representation around the robot's body parts; (iii) a reaching controller that incorporates all obstacles entering the robot's safety zone on the fly into the task. Pilot experiments demonstrate that an effective safety margin between the robot's and the human's body parts is kept. The proposed solution is flexible and versatile since the safety zone around individual robot and human body parts can be selectively modulated---here we demonstrate stronger avoidance of the human head compared to rest of the body. Our system works in real time and is self-contained with no external sensory equipment and use of onboard cameras only.
79,Miseikis2018a, Many works in collaborative robotics and human-robot interaction focuses on identifying and predicting human behaviour while considering the information about the robot itself as given. This can be the case when sensors and the robot are calibrated in relation to each other and often the reconfiguration of the system is not possible or extra manual work is required. We present a deep learning based approach to remove the constraint of having the need for the robot and the vision sensor to be fixed and calibrated in relation to each other. The system learns the visual cues of the robot body and is able to localise it as well as estimate the position of robot joints in 3D space by just using a 2D color image. The method uses a cascaded convolutional neural network and we present the structure of the network describe our own collected dataset explain the network training and achieved results. A fully trained system shows promising results in providing an accurate mask of where the robot is located and a good estimate of its joints positions in 3D. The accuracy is not good enough for visual servoing applications yet however it can be sufficient for general safety and some collaborative tasks not requiring very high precision. The main benefit of our method is the possibility of the vision sensor to move freely. This allows it to be mounted on moving objects for example a body of the person or a mobile robot working in the same environment as the robots are operating in.
80,Bryant2019, Qualitative data collected from International Space Station (ISS) postflight crew debriefs was used to evaluate the performance of a convolutional neural network (ConvNet) model. While the ISS postflight debriefs cover a broad range of spaceflight and on-orbit operations related topics this model was specifically trained and tested to classify debrief comments as safety related or not based on a previously coded subset of debrief comments that were manually evaluated by human factors engineers to determine if a comment had safety implications. This evaluation revealed that a ConvNet can adequately determine whether textual debrief comments contain safety data. These methods can potentially save large amounts of manual effort on the part of human factors engineers and improve the ability to identify and act on crew knowledge that informs or identifies risk to spaceflight crew.
81,Sameen2019, This study investigates the power of deep learning in predicting the severity of injuries when accidents occur due to traffic on Malaysian highways. Three network architectures based on a simple feedforward Neural Networks (NN) Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) were proposed and optimized through a grid search optimization to fine tune the hyperparameters of the models that can best predict the outputs with less computational costs. The results showed that among the tested algorithms the RNN model with an average accuracy of 73.76% outperformed the NN model (68.79%) and the CNN (70.30%) model based on a 10-fold cross-validation approach. On the other hand the sensitivity analysis indicated that the best optimization algorithm is “Nadam” in all the three network architectures. In addition the best batch size for the NN and RNN was determined to be 4 and 8 for CNN. The dropout with keep probability of 0.2 and 0.5 was found critical for the CNN and RNN models respectively. This research has shown that deep learning models such as CNN and RNN provide additional information inherent in the raw data such as temporal and spatial correlations that outperform the traditional NN model in terms of both accuracy and stability.
82,Amato2019, The cloud is emerging as a data-centric intelligent platform ready to deal with the next generation of applications and workloads. At the center of this emerging intelligent cloud is deep learning which is undoubtedly the most disruptive technology of this decade supported by the rise of powerful computing and storage environments. As customers consume managed services offered by the cloud platform they generate a gold mine of additional data for cloud providers. The cloud has all essential components to deliver compelling deep learning capabilities thus offering services for building applications based on cognitive computing predictive analytics intelligent Internet of Things interactive personal assistants and bots. Since data can be effectively considered as the new oil of the digital economy new capabilities for an intelligent processing are required. In this work we propose a cloud based federation of cognitive services aiming to support intelligent applications to trust unknown data sources.
83,Gulgec2019, Structures experience large vibrations and stress variations during their life cycles. This causes reduction in their load-carrying capacity which is the main design criteria for many structures. Therefore it is important to accurately establish the performance of structures after construction that often needs full-field strain or stress measurements. Many traditional inspection methods collect strain measurements by using wired strain gauges. These strain gauges carry a high installation cost and have high power demand. In contrast this paper introduces a new methodology to replace this high cost with utilizing inexpensive data coming from wireless sensor networks. The study proposes to collect acceleration responses coming from a structure and give them as an input to deep learning framework to estimate the stress or strain responses. The obtained stress or strain time series then can be used in many applications to better understand the conditions of the structures. In this paper designed deep learning architecture consists of multi-layer neural networks and Long Short-Term Memory (LSTM). The network achieves to learn the relationship between input and output by exploiting the temporal dependencies of them. In the evaluation of the method a three-story steel building is simulated by using various dynamic wind and earthquake loading scenarios. The acceleration time histories under these loading cases are utilized to predict the stress time series. The learned architecture is tested on acceleration time series that the structure has never experienced.
84,Demongeot2019, This chapter aims to show that big data techniques can serve for dealing with the information coming from medical signal devices such as bio-arrays electro-physiologic recorders mass spectrometers and wireless sensors in e-health applications in which data fusion is needed for the personalization of Internet services allowing chronic patients such as patients suffering cardio-respiratory diseases to be monitored and educated in order to maintain a comfortable lifestyle at home or at their place of life. Therefore after describing the main tools available in the big data approach for analyzing and interpreting data several examples of medical signal devices are presented such as physiologic recorders and actimetric sensors used to monitor a person at home. The information provided by the pathologic profiles detected and clustered thanks to big data algorithms is exploited to calibrate the surveillance at home personalize alarms and give adapted preventive and therapeutic education.
85,Lin2019, Surface defects detection plays a significant role in quality enhancement in steel manufacturing. However manual inspection of steel surface slows down the entire manufacturing process and is time consuming. Currently many methods have been proposed for automatic defect detection on hot-rolled steel surfaces. These methods usually follow two steps: pre-processing and segmentation. The pre-processing step is intended to overcome the uneven illumination of images while the segmentation step generates a binary map to identify defects. This kind of method heavily depends on feature selection approaches but the defect features are usually not easy to obtain. In this paper we propose an automatic steel surface defects detection method based on deep learning. Two deep learning models for defect detection are evaluated. The experimental results show that the evaluated methods can detect steel surface defects more effectively and accurately than the traditional methods. This approach can be also applied to other industrial applications.
86,Yang2019, Accident diagnosis is regarded as one of the complex tasks for nuclear power plant (NPP) operators. In addition if the accident occurs during the startup operation it is hard to cope with the situation appropriately because the initial conditions are different from the normal operation mode. Although operating procedures are provided to operators accident diagnosis and control for recovery are difficult tasks under extremely stressful conditions. In order to achieve safe operation during the startup operation this study proposes algorithms not only for accident diagnosis but also for protection control using long short-term memory (LSTM) which is an advanced version of recurrent neural networks and functional requirement analysis (FRA). Using the LSTM the network structures of algorithms are built. In addition FRA is performed to define the goal functions processes systems and components for protection control. This approach was trained and validated with a compact nuclear simulator for several accidents to demonstrate the feasibility of diagnosis and correct response under startup operation.
87,Kong2019, In an emergency such as fire in a building visually impaired people are prone to danger more than non-impaired people for they cannot be aware of it quickly. Current fire detection methods such as smoke detector is very slow and unreliable. But by using vision sensor instead fire can be proven to be detected much faster as shown in our experiments. Previous studies have applied various image processing and machine learning techniques to detect fire but they usually don’t generalize well because those techniques use hand-crafted features. With the recent advancements in the field of deep learning this research can be conducted to help solve the problem by using deep learning-based object detector to detect fire. Such approach can learn features automatically so they can usually generalize well to various scenes. We introduced two object detection models (R1 and R2) with slightly different model’s complexity. R1 can detect fire at 90% average precision and 85% recall at 33 FPS while R2 has 90% average precision and 61% recall at 50 FPS. The reason why we introduced two models is because we want to have a benchmark comparison as no other research on fire detection with similar techniques exists. We also want to give two model choices when we wish to integrate the model into an IoT platform.
88,Kobayashi2019, Integration of medical imaging into the practice of brachytherapy has a potential to improve the precision in estimating clinical outcomes. Here we will discuss state-of-the-art computational techniques to incorporate spatial consideration into the prediction of radiation toxicities in the various contexts of brachytherapy.
89,Lee2019, Obstacles such as ramps steps and irregular floor surfaces are commonly encountered in homes offices and other public spaces. These obstacles frequently limit the daily activities of people who use mobility aids. In this paper a new offsetting mechanism for climbing a step while reducing the required horizontal climbing force and its step-climbing wheel prototype are proposed. Specifically the proposed wheel allows to integrate into an existing personal assist devices. The proposed step-climbing wheel can help to reduce this limitation. The physical and mental burdens of caregivers and medical staff can also be reduced by making the users of the mechanism more self-sufficient. This paper provides details on this offsetting mechanism and its prototype. The mechanism is analyzed mathematically and its functionality is verified by experiments using a prototype.
90,Horn2019, Presently we observe a shift of human activity from the traditional methods of manufacturing products for increasingly specialized and evolving robotic and IT systems. For obvious economic and technological reasons this change is first strongly visible in industrial production. Along with technological advances is a dramatic shift of man’s place in the production process from the position at the machine to the back of this process as designer supervisor and controller of information systems that manage production process. This seemingly obvious change results in completely new challenges for both industrial architecture but also the wider built environment as it dramatically reduces the number of jobs with completely new requirements for workplace and its architecture. The author of this article discusses the above issue on the example of the design of technologically advanced 3D printing plant from the point of view of the designer.
91,Gravvanis2019, We live in the era of big data in all fields of activity and intensity. From econometrics and bioinformatics to robotics and aviation and from computational linguistics and social networks to traffic and transportation analytics big data is the dominating factor of progress. Especially in the field of Intelligent Transportation Systems (ITS) the plethora of multisource traffic data has given a tremendous boost to the development of sophisticated systems for the confrontation of the several traffic related problems. One of the most challenging and at the same time crucial traffic related problems which has significant impact in many ITS systems (e.g. Advanced Traveler Information Systems multimodal routing systems dynamic pricing systems etc.) is the accurate and real-time traffic forecasting. The task of traffic forecasting i.e. predicting the state of traffic in large scale urban and inter-urban networks within multiple intervals ahead in time includes addressing several subproblems like data acquisition from multiple sources (e.g. inductive loop detectors moving vehicles traffic cameras etc.) preprocessing (outlier detection missing data imputation map-matching etc.) integration and storage design and development of complex algorithmic methods overall network coverage of the forecasting results performance issues etc. In this chapter the several state-of-the-art methods used in all aspects of the traffic forecasting problems are presented with particular emphasis given on both the algorithmic and the efficiency aspects of the problem in the light of the large amounts of available traffic data. In particular the design of advanced traffic forecasting algorithms in large scale urban and inter-urban road networks are described along with their implementation and utilization on large amounts of real world traffic data.
92,Kim2019, The[aut]Kim Kwang Ilprediction[aut]Lee Keon Myung of ship destinations in the harbor can be utilized to identify future routes for navigating ships. The maritime traffic data are broadlyMaritime data analysis classified into the ship trajectory data and the port information management data. These data have been accumulated for many years on the shore base station of different agencies and are being utilized for evaluation of collision risk prediction of vessel trafficVessel traffic and other maritime statistical analysis. This paper presents a new destination prediction modelPrediction model of navigating ships in the harbor which consists of the candidate harbor proposal moduleHarbor proposal module and the position direction filterPosition direction filter module. The candidate harbor proposal module is trained by a deep neural networkDeep neural network which makes use of the characteristics of ships and the occupancy distributions of piers. The position direction filter module leaves out non-promising ones from the harbor list provided by the candidate proposal module with respect to the current position and direction of navigating ship. In the experiments on real vessel traffic data the proposed method has shown that its accuracy is higher than the frequency-based baseline method by about 10 15%.
93,Mohamad2019, Machine learning approaches for non-intrusive load monitoring (NILM) have focused on supervised algorithms. Unsupervised approaches can be more interesting and of more practical use in real case scenarios. More specifically they do not require labelled training data to be collected from individual appliances and the algorithm can be deployed to operate on the measured aggregate data directly. In this paper we propose a fully unsupervised NILM framework based on Deep Belief network (DBN) and online Latent Dirichlet Allocation (LDA). Firstly the raw signals of the house utilities are fed into DBN to extract low-level generic features in an unsupervised fashion and then the hierarchical Bayesian model LDA learns high-level features that capture the correlations between the low-level ones. Thus the proposed method (DBN-LDA) harnesses the DBN’s ability of learning distributed hierarchies of features to extract sophisticated appliances-specific features without the need of precise human-crafted input representations. The clustering power of the hierarchical Bayesian models helps further summarise the input data by extracting higher-level information representing the residents’ energy consumption patterns. Using Deep-Hierarchical models reduces the computational complexity since LDA is not directly applied to the raw data. The computational efficiency is crucial as our application involves massive data from different types of utility usages. Moreover we develop a novel online inference algorithm to cope with this big data. Another novelty of this work is that the data is a combination of different utilities (e.g. electricity water and gas) and some sensors measurements. Finally we propose different methods to evaluate the results and preliminary experiments show that the DBN-LDA is promising to extract useful patterns.
94,Pandey2019, Mechanized labeling of pavement distress is of preponderant usefulness in transportation segment for warrant of safety. Typically non-automated techniques are obligatory for conventional classification algorithms thus having constrained breadth of usage. In the matter herein presents a modus operandi for finding and classifying pavement distress on road which makes use of a deep neural network technique called as convolutional neural network (CNN) to classify the given images of distress into their different categories by making use of “activation function” to proclaim distinct identification of likely features by selecting the features automatically. A comparative result is given for three activation functions viz. ReLU (Rectified Linear Unit) Sigmoid and Tanh. Denouement from the results herein points out that ReLU surpasses Sigmoid and Tanh. Amidst Sigmoid and Tanh Tanh furnishes exceeding accomplishment in terms of time.
95,Ferraz2019, Learning Disabilities (LD) constitute a diverse group of disorders in which children who generally possess at least average intelligence have problems processing information or generating output i.e. LD may be interpreted as a neurologically-based processing problem. The causes and treatment of LD namely reading disorders has been the subject of considerable thought and study. Being one among others this is the reason why this work will focus on dyscalculia and in its different manifestations and how they may interfere with the children natural development. It will be assessed it in terms of a measurement of the child’s entropy a thermodynamic quantity representing the unavailability of a child brain energy for conversion into mental work and seen as the degree of disorder or randomness in the brain i.e. lack of order or predictability; gradual decline into disorder; an arena where entropy reigns supreme. In one’s work it reigns in a specific interval i.e. one may have two scenarios namely the worst and the best one. The formal background will be grounded in the use of Logic Programming to set the architecture of a Function Machine to assess LD and built on base of a Deep Learning approach to Knowledge Representation and Reasoning.
96,Sahu2019, Reliability is an important issue for deciding the quality of the software. Reliability prediction is a statistical procedure that purpose to expect the future reliability values based on known information during development processes. It is considered as a basic function of software development. A review-based research has been done in this work to evaluate the previously established methodologies for reliability prediction. In this paper authors give a critical review related to successful research of reliability prediction. This paper also provides many challenges and keys of reliability estimation during software development process. Further this paper gives a precarious discussion on previous work and identified factors which are important for reliability of software but still ignored. This work helps to developers for predicting the reliability of software with minimum risks.
97,Hu2019, Over the past few decades rapid adoption of sensing computing and communications technologies has created one of the key capabilities of modern engineered systems: the ability to at a low cost to gather store and process large volumes of sensor data from an engineered system during operation.
98,Aubin2019, This paper discusses two examples of how ergonomics practitioners implemented prospective ergonomics. The first resulted in the development of a scoring approach to lending in a West African bank that had previously been deemed impossible due to the lack of relevant data. By using cognitive ergonomics practitioners demonstrated to risk managers that scores could be developed using local customer characteristics. The second initiative was the application of new technology using deep neural networks for image processing. Through a cognitive task analysis of hundreds of risk managers we determined that they spend most of their time verifying information. By combining advanced image processing and scoring we were able to fully automate the loan adjudication process. We believe that this insight could transform the lending industry. We also identify a need for upgraded models and approaches to predict possible transformations and their potential impact.
99,Hovorushchenko2019, Importance of the task of automated evaluation of initial stages of the software lifecycle on the basis of software requirements specifications (SRS) analysis and the need for information technology of new generation for the software engineering domain necessitates the development of agent-oriented information technology for evaluating initial stages of the software lifecycle on the basis of ontological approach. The purpose of this study is the development of the method of activity of ontology-based intelligent agent for evaluating initial stages of the software lifecycle. The intelligent agent which works on the basis of the developed method evaluates the sufficiency of information in the SRS for assessing the non-functional software features provides the conclusion about the sufficiency or insufficiency of information the numerical evaluation of the level of sufficiency of information in the SRS for assessment of each non-functional feature in particular and all non-functional features in general the list of attributes (measures) and/or indicators which should be supplemented in the SRS for increasing the level of sufficiency of SRS information. During the experiments the intelligent agent examined the SRS for the transport logistics decision support system and found that the information in this SRS is not sufficient for assessing the quality by ISO 25010 and for assessing quality by metric analysis.
100,Tao2019, In this paper a structured learning based sinusoidal modelling approach is developed for the prognostics of gear tooth cracking process. According to the vibration signal properties a learning structure is firstly proposed for gear mesh vibration signal modelling. The learning model is deigned to be a representation of the vibration signal including all harmonic components generated by the normal gear operation the residual periodic signal affected by the system load and equipment wear and the residual non-periodic part induced by gear tooth crack and system disturbances. Motivated by neural network learning the signal model parameters are optimized by the error back-propagation process using stochastic gradient descent. The proposed structured learning approach is applied to model the gear tooth cracking process using a set of rig test data. The excellent performance of this approach shows that different parts of the vibration signal are decomposed and the gear tooth cracking development can be successfully monitored.
101,Takeuchi2019, In this research we considered projects to develop systems that use AI technologies including machine learning techniques for office environment. In many AI system development projects both developers and users need to be involved in order to reach a consensus on discussion items before starting a project. To facilitate this we propose a method of assessing an AI system development project by using an assurance case based on quality sub-characteristics of functionality to derive project success factors.
102,Nishizaki2019, This study presents a prototype of an augmented reality (AR) mobile application aimed at visualizing infants’ natural-setting motor and cognitive development for prevention of accidents and promotion of development. To provide adults with better understanding about the infants’ encounters with surrounding objects by visualization of actual recoded data the study comprised three steps. First we conducted longitudinal observations of 10 infants (6 males 4 females) aged 4 12 months at their homes in Tokyo Japan. Second we developed an AR application for iPhones and iPads by using vision-based marker tracking technique. Based on the observations 10 most frequently observed objects were adapted as markers for AR. To ensure privacy and to focus on infant object interactions we converted all AR movies to line drawings. Third we conducted informal user interviews and user tests to confirm the method’s validity and usability. The longitudinal observational data shed light on the infants’ perceptions and actions helping to clarify not only developmental possibilities but also risks of accidents. The user interview suggested that our prototype could scope out the home from infants and can help adults to obtain insights for encouraging child development and reevaluating childproofing home as an ongoing process.
103,Wang2019, In recent years natural disasters such as earthquakes landslides and others have caused significant damage to people’s lives and property. Victims are often trapped in collapsed buildings. Thus the development and understanding of modern techniques for disaster relief are of immense current interest and need. As a significant advancement in wireless communication the emerging UWB Radar Technology is a key technology that UWB is applied in object identification which is characterized by high resolution good anti-interference ability and strong penetrability and so on has been widely used in various fields including natural disaster detection through-wall radar imaging ground penetrating radar technology medical imaging target ranging and personnel positioning disaster relief and so on. In this chapter the author will describe some algorithms for human detection based on UWB radar sensor network in natural disaster. Firstly we study the fuzzy pattern recognition and genetic algorithm which is used to identify the multi-status human being after the brick wall. The main characteristic parameters are selected and extracted from the received signal and each feature parameters corresponding to a sub membership function. Through the genetic algorithm to optimize the sub membership function for constructing the membership function set. According to fuzzy pattern recognition principle of maximum degree of membership function to establish target prediction function and used MATLAB to carry on the simulation for it. Secondly we study the stacked denoising autoencoder algorithm in deep learning to study the through wall human target recognition under imbalanced samples of single sensor and multi-sensor data respectively. The experimental results show that the stacked denoising autoencoder algorithm in deep learning adopted herein allows more effective classification and identification of through wall human targets under imbalanced sample conditions than other algorithms and that the identification effect with multiple sensors under a certain imbalance rate is better than that with a single sensor.
104,Batalden2019, Advanced autonomous maritime operations are today an emerging academic field where the implementation of autonomous or semi-autonomous control support and maintenance systems. The semi-autonomous operations often require a complex interaction between human knowledge and experience as well as suitable intelligent based programs. In this simulated approach of a ship’s berthing operation the captains’ experience and knowledge is the basis for training the fuzzy logic system. The human-machine interaction can further be enhanced by a second fuzzy logic system to feedback the out-put fuzzy logic signal and adjust the berthing maneuver to find near-optimal solutions. The paper will present an Artificial Intelligent based semi-autonomous solution in maritime operations and discuss the related human factors as well as the sensors needed to define the decision support system for ship berthing operation and demonstrating by a proposed fuzzy logic-based solution.
105,Makino2019, A peach fruit moth is very serious problem while exporting the peach because the peach fruit moth affects the ecological system of the destination country where it does not live in. This paper aims to develop a new type of a peach fruit moth inspection system and to evaluate its performance in terms of motion planning theoretically. The validity of the equation for system evaluation is confirmed by numerical computer simulation. Finally the rotation type of the real system is developed and the validity of the theoretical values is verified.
106,Tandiya2019, Recent years have seen continuous rapid growth in popularity and capabilities of artificial intelligence and broadly speaking of other computational techniques inspired by biological analogies. It is most appropriate therefore for this book to explore how such techniques might contribute to enhancing cyber resilience. This chapter argues that the fast-paced development of new cyber-related technologies complicates the classical approach of designing problem-specific algorithms for cyber resilience. Instead “general-purpose” algorithms such as biologically inspired artificial Intelligence (BIAI) are more suited for such problems. BIAI techniques allow learning adaptability and robustness which are compatible with cyber resilience scenarios like self-organization dynamic operation conditions and performance in adversarial environment. This chapter introduces the readers to BIAI techniques and describes various BIAI techniques and their taxonomy. It also proposes metrics which can be used to compare the techniques in terms of their performance implementation ease and requirements. Finally the chapter illustrates the potential of such techniques via several case studies applications pertaining to wireless communication systems.
107,Sulkowski2019, The Internet of Things (IoT) is a new technology paradigm envisioned as a global network of machines and devices capable of interacting with each other. It can change everything - including ourselves because technology and accompanying digital transformation change a character and a way of the current learning process. On the one hand it refers to a man that acquires more and more advanced digital skills and competencies supported by smart education systems. On the other hand it refers to smart machines based on artificial intelligence (AI) which analyse people’s opinions decisions and behaviour thanks to which they acquire knowledge and learn their emotions. As a result they perform activities that used to be characteristic only for people. In such an approach education becomes a natural activity at the same time being a natural sequence of remaining in the ecosystem of mutual connections. But it is also a source of dangers of informational rubbish bin and post-true messages that could spoil a real learning effort. Undoubtedly it creates a great number of new challenges facing enterprises which while combining innovative technologies with their core resources will build their own ecosystem of Smart Business Intelligence. The aim of the paper is to identify and explore possibilities of IoT as a new paradigm for learning with particular focus on its significance for business.
108,Sivils2019, Cyber-physical systems (CPS) are control systems that facilitate the integration of physical systems and computer-based algorithms. These systems are commonly used in control system and critical infrastructure for control and monitoring applications. The internet-of-things (IoT) is a subset of CPS in which multiple physical embedded devices and sensors are connected via a distributed network to communicate and transfer data while being driven by computational algorithms for data delivery and decision-making tasks.
109,Wodecki2019, Artificial intelligence systems not only change the way the organization operates but also enable the creation of new business models and ecosystems. Below is a model proposal that can describe new value creation logics which result in the spread of intelligent systems.
110,Serin2019, Green transportationSerin Dilara Albayrak is anBoyaci Ali integral part of green environment concept. NextYarkan Serhan generation transportation zp nar Alper systems are desired to achieve high performances with reduced fuel consumption and carbon emission. In this regard vehicle maintenance status along with some other critical diagnostic data should steadily be checked and tracked. It is known that vehicles that are poorly maintained or not maintained in a timely manner lead to both emissions exceeding the standards and low performance. Vehicle telematics along with some other conveniences such as infotainment systems location based services and applications are expected to improve safety availability and reliability of next generation transportation systems. From this perspective intelligent transport systems (ITS) seems to be a promising solution candidate which encompasses all of the aforementioned topics as well as vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) and vehicle-to-cloud (V2C) opportunities. Therefore in this study a conceptual model that links vehicle telematics to the cloud along with V2V communications facility is proposed and a prototype based on IEEE 802.11x protocol suite is implemented. Mobile data collection and measurements are obtained. Results are presented along with relevant discussions as well as the end point storage and usage of the data are introduced. In the proposed model the mobile data are transferred to cloud computing platform to create the big data for further research opportunities for car manufacturers policy makers and researchers with the concern of ethics and security issues.
111,Onykiienko2019, The paper deals with problems of determination of the informative indices which characterize the function of the human balance (human static equilibrium). The kefalographic method for research of the human static equilibrium is suggested. The kefalographic plant for this method implementation was modified. The informative indices which characterize the space dynamic range and features of the human body oscillations relative to the axis z were determined. Such indices represent the coefficients which characterize changes of the sampling mathematical expectation $$ K_\tildem_r  $$Km~r variance $$ K_\tildeD_r  $$KD~r skewness $$ K_\tildea_r  $$Ka~r and kurtosis $$ K_\tildee_r  $$Ke~r for the vector projection of central position the human body.
112,Fang2019, Urban rail transit demand analysis and forecasting is an essential prerequisite for daily operations and management. This paper categorizes the proposed demand forecasting methods and focuses on traditional models statistical models and machine learning approaches according to their features and fields. Especially influential and widely-used methods including the four-stage model land use models time series methods Logit regression Artificial Neural Networks (ANNs) and other referring methods are all taken into discussion.
113,Mukai2019, It is said that the most cause of traffic accidents is the lack of confirming the safety. Visual information from both eyes is one of the important factors for safe driving. In this paper we collect eye-gaze data of drivers who watch a driving video and try to develop a model of their eye movements to identify factors to enhance their safety. For the purpose of modeling we adopted a recurrent neural network and Long Short-Term Memory (LSTM) to the collected eye-gaze data because the LSTM is able to deal with a time-series data such as the eye-gaze data. Moreover we performed an experiment to evaluate the identification accuracy of drivers. The results indicated that the driver’s intention and habit can be approximated partially by the trained network but it was insufficient to identify a personal driver for practical use.
114,Shladover2019, The 2017 Automated Vehicles Symposium built on the successes of the predecessor meetings with an even larger and more diverse roster of participants and a broader selection of breakout sessions. The plenary and poster presentations and breakout discussions continued to provide the meeting participants with the most up-to-date and authoritative information about the current international state of development and deployment of road vehicle automation systems making this the essential meeting for industry government and research practitioners in the field.
115,Taylor2019, Automation has improved transportation systems in various domains over the last several decades. Increasing autonomy in these systems has gradually reduced the role of the human operator to that of system monitor with the ultimate goal of eliminating the human from the control system entirely. Commercial aviation has benefited from automation but it operates with the support of a broad infrastructure of safety when compared to vehicular road traffic. While not designed to operate in a fully autonomous mode the computer sensor and software technology developed for aircraft are being applied to self-driving cars with the expectation that driving will also see significant improvements in accident rates and efficiency through the elimination of human error and negligence [1]. A sophisticated combination of hardware sensors and computer software analyzes the environment and controls the speed and direction of the car without input from its human occupants and their opaque interactions increase the complexity of the system. This approach has potential benefits but also potential problems. Autonomous vehicles will present ethical challenges while being developed and after deployment. The purpose of this paper is to consider the many ethical implications involved with the implementation and oversight of autonomous vehicle (AV) technology. This paper examines primary ethical dilemmas present in the use of autonomous cars including liability and moral agency.
116,Wodecki2019a, The aim of this work is to present the influence of artificial intelligence on the management value generation competitive advantage of contemporary organizations.
117,Hernandez2019, Companies require highly qualified electricians because they have to be competitive in a globalized world and a changing environment. Additionally they have to face the generational change since knowledgeable personnel is in the retirement process and new human resources have to be trained to get the required competencies: knowledge skills and attitudes. Talent management is concerned with employee recruitment development and administration and suggests the integration of learning as a fundament for talent management strategies. We have developed a training model which provides online and personalized instruction within the talent management framework based on three axes: knowledge skills and attitudes. The training model aims to complement traditional training with computer-based training. An important aspect of the model is an intelligent training system. This intelligent system adapts the instruction relying on a trainee model which represents the state of the trainee. The trainee model includes knowledge skills attitudes affect and learning styles and it is built as the trainee interacts with the training system. Emotions are recognized using facial expressions and training situation. An animated agent who uses the trainee model to show emotions and empathy presents the instruction. Here we present the training model and we discuss the current results in the implantation of the training model.
118,Siu2019, According to its oldest definitions civil engineeringCivil engineering is a very broad discipline that deals with the designDesign construction and maintenance of the physical and naturally built environmentBuilt environment. It has been widely accepted that built environmentBuilt environment is one of the major supports to public health and quality of lifeQuality of life. This chapter is dedicated to a service learning subject lead by the Department of Civil and Environmental Engineering at the Hong Kong Polytechnic UniversityThe Hong Kong Polytechnic University that engages students in studies of the built environmentBuilt environment. Students work alongside with the service clients to gain first-hand understanding of the problems faced by the end-users which helps them to reflect on the conventional top-down designDesign approach. The quantitativeQuantitative and qualitativeQualitative data obtained by students not only helped them in devising improvementImprovement suggestions to service clients and public authorities these data also forms part of district database and can be used in setting performancePerformance targets and monitoring progress. Apart from the benefits to communityCommunity there has been observationsObservations in students’ academicAcademic professional and personal developments as well as positive impacts in service clients’ quality of lifeQuality of lifeattributesAttributes.
119,Zaroug2019, There is an emerging need to synchronise wearable function with user intention as many exoskeletons reported in current literature have limited capability to predict user intention. In order to achieve good synchronization closed loop feedback is required. Overcoming these limitations necessitates an architecture composed of networked sensors and actuators with smart control algorithms to fuse sensor data and create smooth actuation. This review chapter discusses the growing need to deploy computational intelligence (CI) techniques as well as machine learning (ML) algorithms so that exoskeletons are able to predict the user intentions and consequently operate in parallel with human intention. A comprehensive review of major portable active exoskeletons are provided for both upper and lower limbs with a focus on the need for smart algorithms integration to drive them. The application areas include rehabilitation and human performance augmentation.
120,Meraoumia2019, Of course in all world countries commercial and industrial enterprises play an important role in the development of their economics. Recently the change in enterprise operation mode such as their activities speed in terms of production and transport requires integration of these activities into a private and/or public networks structure in order to achieve many benefits such as the time and the cost. So for an effective development these enterprises must be connected with them in order to exchange necessary information. Thus the resultant of this leads to emergence of special structure called Enterprises Network (EN) which allows such enterprise to more effectively interact with others enterprises inside and outside their activities. The challenges that oppose this structure are the concerns of transmitted data security which are exchanged between them as well as the privacy issues which ensure the secrecy of enterprise information. In contrast with other approaches presented in literature a complete biometric cryptosystem is presented. Based on the obtained results it can be inferred that the proposed authentication system is highly secure effective and cheap which are vital for any authentication system to gain enterprise confidence for implementation in real time secure access control systems.
121,Chandra2019, There have been huge contributions to online communities and social media websites through posts comments and blogs day in and day out. Some of this contribution is unstructured and unclassified. It is difficult to find similarities in terms of textual data in the posts as it comprises of mix of structured and unstructured data. The overall objective of this paper is to help identify similar text through natural language processing techniques. The approach has been demonstrated through linguistic features that points to similarity and use those features for the automatic identification of analogous data in offline data repository. To demonstrate the approach we have used a collection of documents as an offline repository having similar text and a text corpus as a resource to identify analogous data. The proposed methodology processes a document against repository based on document preprocessing through lexical analysis stop word elimination and synonym substitution check. Java data structure is used to hold and process data parsed from the file syntactic analysis is carried out with the help of WordNet  database configured within the process. Part of speech (POS) and synonym check capabilities of WordNet API are being used in the process.
122,Sun2018a, Recently deep learning has attracted substantial attention as a promising solution to many problems in computer vision. Among various deep learning architectures convolutional neural network (CNN) has demonstrated superior performance as a feature learning method. In this paper we present a novel hybrid model of CNN and extreme learning machine (ELM) for object tracking. Training a conventional CNN requires a substantial amount of computation and a large dataset. ELM randomly generates the parameters of hidden layers and calculates network weights between output and hidden layers via the regularized least-square method thereby dramatically reducing the learning time while producing accurate results with minimal training data. Therefore we integrate the ELM auto-encoder architecture into the CNN model. In addition an effective updating scheme is designed for the model training to overcome the tracking drift problem. The joint CNN-ELM tracker is robust to object variations such as illumination occlusion and rotation in a video sequence. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favourably compared to several state-of-the-art methods.
123,Zhou2018, With the increasing number of malicious attacks the way how to detect malicious Apps has drawn attention in mobile technology market. In this paper we proposed a detection model to seek and track malware Apps actions in such devices. To characterize the behaviors of Apps dynamic features of each App were constrained in 166-dimension and a novel machine learning classifier is employed to detect malware Apps and alarm will be triggered if an Android-based App is detected as malicious. With such we can avoid a detected malware spreading out in larger scale affecting extensively our society. Detailed description of the detection model is provided as well the core technologies of this novel machine learning classifier are presented. From experiments performed on a set of Android-based malware and benign Apps we observe that the proposed classification algorithm achieves highest accuracy true-positive rate false-positive rate precision recall f-measure in comparison to other methods as K-Nearest Neighbor (KNN) Naive Bayesian (NB) Support Vector Machine (SVM) Random Forest (RF) Logistic Regression (LR) Decision tree (DT) Linear Discriminant Analysis (LDA) and Back Propagation (BP). The proposed detection model is promising and can effectively be applied to Android malware detection providing early detection and the prospect of warning users of threatens ahead.
124,Chai2018, This review identifies 20 studies pertaining to teacher professional development for STEM education. Using a mixture of content analysis with reference to the TPACK framework and open and axial coding a descriptive model was constructed. The model describes the connection of the various categories of variables associated with teacher professional development for STEM. How content pedagogy and technology are featured in current STEM research are treated as properties of the core phenomenon of teacher professional development for STEM. Design considerations for future research are presented. The study recommends that design thinking epistemic fluency and technological pedagogical engineering knowledge could be the anchors of future research.
125,Wang2018e, Emerging evidence has shown long non-coding RNAs (lncRNAs) play important roles in cancer drug response. Here we report a lncRNA pharmacogenomic landscape by integrating multi-dimensional genomic data of 1005 cancer cell lines and drug response data of 265 anti-cancer compounds. Using Elastic Net (EN) regression our analysis identifies 27341 lncRNA-drug predictive pairs. We validate the robustness of the lncRNA EN-models using two independent cancer pharmacogenomic datasets. By applying lncRNA EN-models of 49 FDA approved drugs to the 5605 tumor samples from 21 cancer types we show that cancer cell line based lncRNA EN-models can predict therapeutic outcome in cancer patients. Further lncRNA-pathway co-expression analysis suggests lncRNAs may regulate drug response through drug-metabolism or drug-target pathways. Finally we experimentally validate that EPIC1 the top predictive lncRNA for the Bromodomain and Extra-Terminal motif (BET) inhibitors strongly promotes iBET762 and JQ-1 resistance through activating MYC transcriptional activity.In this study the authors build lncRNA-drug response models for 265 anti-cancer agents across 27 cancer types. They report their cancer cell line based lncRNA EN-models are able to effectively predict therapeutic outcome for breast cancer ovarian cancer endometrial cancer and stomach cancer patients.
126,Abdolmaleki2018, Multi-target (mt) therapy is an attractive approach as well as a challenging task in drug discovery research and pharmaceutical industry. The multi-target drug design strategy is an opportunity to find new drugs for the treatment of two or more targets simultaneously. Advanced characterization of bioactive molecules computational science and molecular biology have contributed to planning of new bioactive compounds and evaluating different features of multi-targeted drugs. Computational methods have different roles in drug candidate searching analysis and prediction in this field. Here we discuss several in silico methodologies and computer-aided drug design (CADD) as structure-activity relationship (SAR) quantitative SAR (QSAR) pharmacophore modeling and molecular docking in the process of drug discovery in the field of multi-targeted drugs (MTDs). Computational efficiency of each method has been stated in relation to their key strength and weakness. These capacities for binding affinity prediction are rationally effective with promising potential in easing drug discovery directed at selective multiple targets.
127,Mohan2018, Scheduling procedures implemented in wireless networks consists of varied workflows such as resource allocation channel gain improvement and reduction in packet arrival delay. Among these techniques Long term evolution (LTE) scheduling is most preferable due to its high speed communication and low bandwidth consumption. LTE allocates resources to the workflow based on time and frequency domains. Normally the information gathered prior to scheduling increases the processing time since each attributes of the users have to be verified. In order to solve this issue parallel processing via data mining is analyzed in recent research studies. The label that is assigned to the user attributes contributes primarily on scheduling time slots effectively. The label assignment and parallel processing via data mining reduces the delay and increases the throughput respectively. Additionally the matched data extraction from the library and the prediction of available channels with fewer dimensions posed major challenges in the LTE scheduling. This paper surveys about various LTE scheduling algorithms dimensionality reduction techniques optimal feature selection techniques multi-level classification techniques and data mining combined with LTE techniques. A brief survey illustrates the impact of each technique on 3G/4G networks channel availability prediction scheduling of time slots in detail. A brief comparison of the techniques involved in the respective LTE processes via tabular form reveals that the verification of channel and user availability are the primary functions of the LTE scheduling. The survey of this paper identifies the limitations such as computational complexity and poor scheduling performance in the existing systems and encourages researchers to develop novel algorithms for LTE scheduling.
128,Roshkovan2018, Purpose of reviewCT angiography has become the gold standard for evaluation of suspected pulmonary embolism; however continuous evolution in radiology has led to new imaging approaches that offer improved options for detection and characterization of pulmonary embolism while exposing patients to lower contrast and radiation dose. The purpose of this review is to summarize state of the art imaging approaches for the evaluation of pulmonary embolism focusing on technical innovations in this field.Recent findingsThe introduction of dual-energy CT has resulted in the ability to add functional and prognostic information beyond the morphologic assessment of the pulmonary arteries and potentially offer improved image quality without additional radiation burden.New approaches and strategies in CT scanning have resulted in decreased radiation exposure as well as a significant decrease in contrast material used without decreasing the sensitivity for detection of pulmonary embolism.Continuous developments and improvements in MR angiography techniques offer a valuable and efficient option for certain patient populations without the risk of radiation exposure. Improvements in the technical success rate and reliability of this modality will mean more widespread use in the future.Moving beyond planar ventilation/perfusion (V/Q) scintigraphy nuclear imaging offers several new approaches including the use of single photon emission computed tomography (SPECT) and SPECT/CT resulting in superior diagnostic performance and a decrease in nondiagnostic studies potentially surpassing the diagnostic capabilities of computed tomography pulmonary angiography. Ongoing research in the use of V/Q PET/CT demonstrates superior temporal and spatial resolution and quantitative capabilities compared to SPECT-CT; this modality will likely play an increasing role in the detection and characterization of pulmonary embolism.SummaryThe field of pulmonary embolism imaging has demonstrated continuous evolution in both development of novel techniques and improvement in current technologies resulting in better detection decreased radiation exposure and enhanced functional information beyond morphologic characterization of the pulmonary vasculature.
129,Yoon2018, The accuracies of driver’s gaze detection by previous researches are affected by the various sitting positions and heights of drivers in case that initial calibration of driver is not performed. By using dual cameras the driver’s calibration can be omitted but processing time with complexity is increased. In addition the problem of disappearing corneal specular reflection (SR) in the eye image as the driver severely turns his/her head has not been dealt in previous researches. To consider these issues we propose a gaze tracking method based on driver’s one-point calibration using both corneal SR and medial canthus (MC) based on maximum entropy criterion. An experiment with collected data from 26 subjects (wearing nothing glasses sunglasses hat or taking various hand pose) in a vehicle showed that the accuracy of the proposed method is higher than that of other gaze tracking methods. In addition we showed the effectiveness of our method in the real driving environment.
130,French2018, ObjectiveTo explore interest feasibility perceived effectiveness and acceptability of a standardized national physiology curriculum for neonatal perinatal medicine (NPM) fellows using online videos for knowledge acquisition paired with flipped classrooms (FCs) for knowledge application.Study designTwo educational programs pairing online videos with FCs were developed and peer-reviewed. These programs were piloted at five institutions. Fellows completed surveys and fellows and educators participated in focus groups after their FC experiences.ResultsThirty-five fellows responded to the survey. Forty-one fellows and six educators participated in focus groups. Fellows and educators preferred online videos paired with FCs over didactic teaching and perceived them to be effective for knowledge acquisition and application.ConclusionFellows and educators preferred FC learning over traditional didactics and reported that FCs facilitated creation of a learning community fostering active learning. The favorable response toward this pilot project and the feasibility of its use supports further development of a standardized NPM physiology curriculum for fellowship training.
131,Li2018d, Facial Landmark Localization (FLL) on unconstrained images still remains challenging as they poses complex variation in face spatial structure and appearance. To address this problem we propose a Spatial Alignment Network (SAN) which consist of two modules like the transformation sub-network and the estimation sub-network. In the first module we propose two methods to achieving spatial transformation one is the handcrafted method which can ensure model stability and the other is the learning-based method which is efficient and flexible. In the second module we add an attention layer in the deep CNN to enhance the importance of discriminative features and obtain more accurate results. Through extensive experiments our model achieves good performance on several public challenging datasets.
132,Wang2018f, BackgroundFor practical straight-line scanning in photoacoustic imaging (PAI) serious artifacts caused by missing data will occur. Traditional total variation (TV)-based algorithms fail to obtain satisfactory results with an over-smoothed and blurred geometric structure. Therefore it is important to develop a new algorithm to improve the quality of practical straight-line reconstructed images.MethodsIn this paper a combined nonlocal patch and TV-based regularization model for PAI reconstruction is proposed to solve these problems. A modified adaptive nonlocal weight function is adopted to provide more reliable estimations for the similarities between patches. Similar patches are searched for throughout the entire image; thus this model realizes adaptive search for the neighborhood of the patch. The optimization problem is simplified to a common iterative PAI reconstruction problem.Results and conclusionThe proposed algorithm is validated by a series of numerical simulations and an in vitro experiment for straight-line scanning. The results of patch-TV are compared to those of two mainstream TV-based algorithms as well as the iterative algorithm only with patch-based regularization. Moreover the peak signal-to-noise ratio the noise robustness and the convergence and calculation speeds are compared and discussed. The results show that the proposed patch-TV yields significant improvement over the other three algorithms qualitatively and quantitatively. These simulations and experiment indicate that the patch-TV algorithm successfully solves the problems of PAI reconstruction and is highly effective in practical PAI applications.
133,Brereton2018, The contribution of chemometrics to important stages throughout the entire analytical process such as experimental design sampling and explorative data analysis including data pretreatment and fusion was described in the first part of the tutorial “Chemometrics in analytical chemistry.” This is the second part of a tutorial article on chemometrics which is devoted to the supervised modeling of multivariate chemical data i.e. to the building of calibration and discrimination models their quantitative validation and their successful applications in different scientific fields. This tutorial provides an overview of the popularity of chemometrics in analytical chemistry.
134,Zhao2018b, Eye state recognition is widely used in many fields such as driver drowsiness recognition facial expression classification and human computer interface technology. This study proposes a novel framework based on the deep learning method to classify eye states in still facial images. The proposed method combines a deep neural network and a deep convolutional neural network to construct a deep integrated neural network for characterizing useful information in the eye region by use of the joint optimization method. A transfer learning strategy is applied to extract effective abstract eye features and improve the classification capability of the proposed model on small sample datasets. Experimental results on the Closed Eyes in the Wild (CEW) and Zhejiang University Eyeblink datasets show that the proposed approach outperforms other state-of-the-art methods. In addition the effects of transfer learning methods with different pretraining datasets on classification accuracy are investigated with the CEW dataset. A driver drowsiness recognition dataset is constructed and used in an experiment to evaluate the effectiveness of the proposed method in driving environments. Experimental results demonstrate that the proposed method performs more stably and robustly than do other methods.
135,Wotawa2018, For safety critical systems like cars trains or airplanes quality assurance methods and techniques are crucial for preventing situations that may harm people. The case of automated driving represents the next level of safety critical systems where additional challenges arise. This includes the question of how to assure that artificial intelligence and machine learning based systems fulfill safety criticality requirements under all potential conditions and situations that may emerge during operation. In this paper we first review simulation-based verification and validation methods for such systems and afterwards discuss necessary requirements and future research challenges that have to be solved in order to bring automated driving into practice without compromising safety requirements.ZusammenfassungF r sicherheitskritische Systeme wie Autos Z ge oder Flugzeuge sind Methoden und Techniken zur Qualit tssicherung entscheidend um den Schutz von Leib und Leben zu garantieren. Automatisiertes Fahren stellt eine neue Herausforderung f r die Entwicklung und den Test von sicherheitskritischen Systemen dar. Dazu geh rt die Frage wie sichergestellt werden kann dass Systeme die auf k nstliche Intelligenz und maschinellen Lernen aufbauen alle sicherheitskritischen Anforderungen die w hrend des Betriebs auftreten k nnen erf llen. In diesem Beitrag stellen die Autoren zun chst simulationsbasierte Verifikations- und Validierungsmethoden f r solche Systeme vor und diskutieren anschlie end zuk nftige Herausforderungen die gel st werden m ssen um automatisiertes Fahren in die Praxis umzusetzen ohne die Sicherheitsanforderungen zu verletzten.
136,Souza2018, Human gait analysis is considered a new biometric tool for the ability to obtain metrics from the body at a distance. Biometric identifiers have properties that can technologically measure and analyze the characteristics of the human body and can be used as a form of identification and access control for security applications. Recognition through proper interpretation of gait parameters has become a relevant pattern classification problem. This work aims to develop an image processing system with the use of the Microsoft Kinect sensor which is capable to extract movement patterns for gait analysis and to present a comparative study of different pattern recognition methods for human identification. The image processing system developed in C# allowed the acquisition of three-dimensional data from several volunteers and made it possible to identify the human skeleton and automatically extract the kinetic and kinematic parameters of the body. For data analysis different classification methods were compared. Among them the algorithms that presented better performance were probabilistic neural networks deep neural networks and k-nearest neighbors with nearly 99% correct recognition rate. The obtained results demonstrate the efficiency of gait analysis as a biometric method. They also show the viability of gait parameter extraction using the Kinect sensor and the good performance of pattern recognition methods applied to the acquired gait kinetic and kinematic parameters.
137,Ma2018d, Sentiment analysis has emerged as one of the most popular natural language processing (NLP) tasks in recent years. A classic setting of the task mainly involves classifying the overall sentiment polarity of the inputs. However it is based on the assumption that the sentiment expressed in a sentence is unified and consistent which does not hold in the reality. As a fine-grained alternative of the task analyzing the sentiment towards a specific target and aspect has drawn much attention from the community for its more practical assumption that sentiment is dependent on a particular set of aspects and entities. Recently deep neural models have achieved great successes on sentiment analysis. As a functional simulation of the behavior of human brains and one of the most successful deep neural models for sequential data long short-term memory (LSTM) networks are excellent in learning implicit knowledge from data. However it is impossible for LSTM to acquire explicit knowledge such as commonsense facts from the training data for accomplishing their specific tasks. On the other hand emerging knowledge bases have brought a variety of knowledge resources to our attention and it has been acknowledged that incorporating the background knowledge is an important add-on for many NLP tasks. In this paper we propose a knowledge-rich solution to targeted aspect-based sentiment analysis with a specific focus on leveraging commonsense knowledge in the deep neural sequential model. To explicitly model the inference of the dependent sentiment we augment the LSTM with a stacked attention mechanism consisting of attention models for the target level and sentence level respectively. In order to explicitly integrate the explicit knowledge with implicit knowledge we propose an extension of LSTM termed Sentic LSTM. The extended LSTM cell includes a separate output gate that interpolates the token-level memory and the concept-level input. In addition we propose an extension of Sentic LSTM by creating a hybrid of the LSTM and a recurrent additive network that simulates sentic patterns. In this paper we are mainly concerned with a joint task combining the target-dependent aspect detection and targeted aspect-based polarity classification. The performance of proposed methods on this joint task is evaluated on two benchmark datasets. The experiment shows that the combination of proposed attention architecture and knowledge-embedded LSTM could outperform state-of-the-art methods in two targeted aspect sentiment tasks. We present a knowledge-rich solution for the task of targeted aspect-based sentiment analysis. Our model can effectively incorporate the commonsense knowledge into the deep neural network and be trained in an end-to-end manner. We show that the two-step attentive neural architecture as well as the proposed Sentic LSTM and H-Sentic-LSTM can achieve an improved performance on resolving the aspect categories and sentiment polarity for a targeted entity in its context over state-of-the-art systems.
138,Broedner2018, Looking back on the development of computer technology particularly in the context of manufacturing we can distinguish three big waves of technological exuberance with a wave length of roughly 30 years: In the first wave during the 1950s mainframe computers at that time were conceptualized as 
139,Nowak-Sliwinska2018, The formation of new blood vessels or angiogenesis is a complex process that plays important roles in growth and development tissue and organ regeneration as well as numerous pathological conditions. Angiogenesis undergoes multiple discrete steps that can be individually evaluated and quantified by a large number of bioassays. These independent assessments hold advantages but also have limitations. This article describes in vivo ex vivo and in vitro bioassays that are available for the evaluation of angiogenesis and highlights critical aspects that are relevant for their execution and proper interpretation. As such this collaborative work is the first edition of consensus guidelines on angiogenesis bioassays to serve for current and future reference.
140,Song2018a, Visual big data is an essential and significant research topic due to its diverse applications. In this paper a new visual detection method for transportation is proposed based on probabilistic latent semantic analysis with visual data. We detect the distinctiveness by integrating three steps as follows: first representing the co-ocurrence matrix of images which were vectorized using the bag of visual words (BoVW) framework; then calculating the histograms of the visual words of each class; and finally applying the test images as the visual words. A multilayer perceptron (MLP) is used as the classification method in our system. The visual words are extracted by sampling the patches from the current image. A new topology of the neural network for the BoVW model is proposed and management of the learning rate by reducing at specific iterations is exploited. The Probabilistic latent semantic analysis (PLSA) is compared to the MLP using the Caltech 256 datasets. The classes used include cars motorbikes and horses. The results of the experiment show that the MLP outperforms current methods in predicting transportation objects and properly approximates the transportation detection function with extracted local features. It shows that the proposed method yields about 4.4% higher accuracy than the conventional PLSA for all classes.
141,Colen2018, SummaryWe present the first reported work that explores the potential of radiomics to predict patients who are at risk for developing immunotherapy-induced pneumonitis. Despite promising results with immunotherapies immune-related adverse events (irAEs) are challenging. Although less common pneumonitis is a potentially fatal irAE. Thus early detection is critical for improving treatment outcomes; an urgent need to identify biomarkers that predict patients at risk for pneumonitis exists. Radiomics an emerging field is the automated extraction of high fidelity high-dimensional imaging features from standard medical images and allows for comprehensive visualization and characterization of the tissue of interest and corresponding microenvironment. In this pilot study we sought to determine whether radiomics has the potential to predict development of pneumonitis. We performed radiomic analyses using baseline chest computed tomography images of patients who did (N , 2) and did not (N ," 30) develop immunotherapy-induced pneumonitis. We extracted 1860 radiomic features in each patient. Maximum relevance and minimum redundancy feature selection method, anomaly detection algorithm, and leave-one-out cross-validation identified radiomic features that were significantly different and predicted subsequent immunotherapy-induced pneumonitis (accuracy, 100% [p "," 0.0033]). This study suggests that radiomic features can classify and predict those patients at baseline who will subsequently develop immunotherapy-induced pneumonitis, further enabling risk-stratification that will ultimately lead to better treatment outcomes.,"
142,Du2018, Driver state analysis is considered as a potential application of computer vision. Facial images contain important information that enable recognition of the states of a driver. Unfortunately the information hidden in facial images is imperfect and varies with the external environments. Modeling the relationship between the face information and driver’s state plays an essential role in driver fatigue detection. In this work facial sequences are aligned and normalized following which a few fixed observation areas related to the fatigue expressions are extracted. Some discriminative features are extracted to represent facial states from these areas. A single image does not contain enough information to reflect fatigue expressions hence a sequence of face images are exploited for fatigue detection using a sliding window. Thus both static and sequential information are used to represent the states of a driver. An algorithm is designed to evaluate the quality of the extracted candidate features. Each area only contains partial information for state recognition and merely provides a single view of the evidence for driver state recognition. We built base models with the information extracted from some specific facial areas and integrated these to recognize the states of the driver. Experimental results show that these base models can offer complementary information for accurately identifying the facial status and the integrated model shows good performance in driver state analysis.
143,Zhang2018b, With increasing power density in modern integrated circuits thermal issues are becoming a critical problem in System-on-a-Chip (SoC) testing. In this paper we develop the thermal-aware test scheduling methods using Voltage/Frequency Scaling (VFS) and Test Partition (TP) to reduce the expensive Test Application Time (TAT). First we develop a quick temperature estimation method in test scheduling to ensure the test temperature within the given range. Second we propose a thermal-aware test scheduling method based on the mixed-integer linear programming model (MILP) (called STP-M) that applies VFS and TP to search the optimum scheduling and further reduce the TAT. Third we develop a heuristic method based on Rectangular Strip Packing (called H-RSP) to quickly access the quasi-optimal scheduling. The experimental results on ITC’02 benchmarks showed that the STP-M obtains the most optimized result for every benchmark and saved 15.5% and 8.0% TAT on average compared with the existing works while H-RSP takes less than 10 seconds to access the quasi-optimal scheduling that is close to that of STP-M.
144,Ramalhinho2018, PurposeLaparoscopic ultrasound (LUS) enhances the safety of laparoscopic liver resection by enabling real-time imaging of internal structures such as vessels. However LUS probes can be difficult to use and many tumours are iso-echoic and hence are not visible. Registration of LUS to a pre-operative CT or MR scan has been proposed as a method of image guidance. However the field of view of the probe is very small compared to the whole liver making the registration task challenging and dependent on a very accurate initialisation.MethodsWe propose the use of a subject-specific planning framework that provides information on which anatomical liver regions it is possible to acquire vascular data that is unique enough for a globally optimal initial registration. Vessel-based rigid registration on different areas of the pre-operative CT vascular tree is used in order to evaluate predicted accuracy and reliability.ResultsThe planning framework is tested on one porcine subject where we have taken 5 independent sweeps of LUS data from different sections of the liver. Target registration error of vessel branching points was used to measure accuracy. Global registration based on vessel centrelines is applied to the 5 datasets. In 3 out of 5 cases registration is successful and in agreement with the planning. Further tests with a CT scan under abdominal insufflation show that the framework can provide valuable information in all of the 5 cases.ConclusionsWe have introduced a planning framework that can guide the surgeon on how much LUS data to collect in order to provide a reliable globally unique registration without the need for an initial manual alignment. This could potentially improve the usability of these methods in clinic.
145,Alirr2018, PurposeSegmentation of liver tumours is an important part of the 3D visualisation of the liver anatomy for surgical planning. The spatial relationship between tumours and other structures inside the liver forms the basis of preoperative surgical risk assessment. However the automatic segmentation of liver tumours from abdominal CT scans is riddled with challenges. Tumours located at the border of the liver impose a big challenge as the surrounding tissues could have similar intensities.MethodsIn this work we introduce a fully automated liver tumour segmentation approach in contrast-enhanced CT datasets. The method is a multi-stage technique which starts with contrast enhancement of the tumours using anisotropic filtering followed by adaptive thresholding to extract the initial mask of the tumours from an identified liver region of interest. Localised level set-based active contours are used to extend the mask to the tumour boundaries.ResultsThe proposed method is validated on the IRCAD database with pathologies that offer highly variable and complex liver tumours. The results are compared quantitatively to the ground truth which is delineated by experts. We achieved an average dice similarity coefficient of 75% over all patients with liver tumours in the database with overall absolute relative volume difference of 11%. This is comparable to other recent works which include semiautomated methods although they were validated on different datasets.ConclusionsThe proposed approach aims to segment tumours inside the liver envelope automatically with a level of accuracy adequate for its use as a tool for surgical planning using abdominal CT images. The approach will be validated on larger datasets in the future.
146,Reily2018, Activity prediction is an essential task in practical human-centered robotics applications such as security assisted living etc. which is targeted at inferring ongoing human activities based on incomplete observations. To address this challenging problem we introduce a novel bio-inspired predictive orientation decomposition (BIPOD) approach to construct representations of people from 3D skeleton trajectories. BIPOD is invariant to scales and viewpoints runs in real-time on basic computer systems and is able to recognize and predict activities in an online fashion. Our approach is inspired by biological research in human anatomy. To capture spatio-temporal information of human motions we spatially decompose 3D human skeleton trajectories and project them onto three anatomical planes (i.e. coronal transverse and sagittal planes); then we describe short-term time information of joint motions and encode high-order temporal dependencies. By using Extended Kalman Filters to estimate future skeleton trajectories we endow our BIPOD representation with the critical capabilities to reduce noisy skeleton observation data and predict the ongoing activities. Experiments on benchmark datasets have shown that our BIPOD representation significantly outperforms previous methods for real-time human activity classification and prediction from 3D skeleton trajectories. Empirical studies using TurtleBot2 and Baxter humanoid robots have also validated that our BIPOD method obtains promising performance in terms of both accuracy and efficiency making BIPOD a fast simple yet powerful representation for low-latency online activity prediction in human robot interaction applications.
147,Newell2018, PurposePedicle screw malplacement leading to neurological symptoms vascular injury and premature implant loosening is not uncommon and difficult to reliably detect intraoperatively with current techniques. We propose a new intraoperative post-placement pedicle screw position assessment system that can therefore allow surgeons to correct breaches during the procedure. Our objectives were to assess the accuracy and robustness of this proposed screw location system and to compare its performance to that of 2D planar radiography.MethodsThe proposed system uses two intraoperative X-ray shots acquired with a standard fluoroscopic C-arm and processed using 2D/3D registration methods to provide a 3D visualization of the vertebra and screw superimposed on one another. Point digitization and CT imaging of the residual screw tunnel were used to assess accuracy in five synthetic lumbar vertebral models (10 screws in total). Additionally the accuracy was evaluated with and without correcting for image distortion and for various screw lengths screw materials breach directions and vertebral levels.ResultsThe proposed method is capable of localizing the implanted screws with less than 2 mm of translational error (RMSE: 0.7 and 0.8 mm for the screw head and tip respectively) and less than $$2.3^\circ $$2.3  angular error (RMSE: $$1.3^\circ $$1.3 ) with minimal change to the errors if image distortion is not corrected. Breaches and their anatomical locations were all correctly visualized and identified for a variety of screw lengths screw materials breach locations and vertebral levels demonstrating the robustness of this approach. In contrast one breach one non-breach and the anatomical location of three screws were misclassified with 2D X-ray.ConclusionWe have demonstrated an accurate and low-radiation technique for localizing pedicle screws post-implantation that requires only two X-rays. This intraoperative feedback of screw location and direction may allow the surgeon to correct malplaced screws intraoperatively thereby reducing postoperative complications and reoperation rates.
148,Kok2018, Dense crowd density estimation is one of the fundamental tasks in crowd analysis. While tremendous progress has been made to understand crowd scenes along with the rise of Convolutional Neural Networks (CNNs) research work on dense crowd density estimation is still an ongoing process. In this paper we propose a novel approach to learn discriminative crowd features from granules that conforms to the outline between crowd and background (i.e. non-crowd) regions for density estimation. It shows that by studying the inner statistics of granules for density estimation this approach is adaptive to arbitrary distribution of crowd (i.e. scene independent). Multiple features fusion is proposed to learn discriminative crowd features from granules. This is to be used as description of the crowd where a direct mapping between the features and crowd density is learned. Extensive experiments on public benchmark datasets demonstrate the effectiveness of our novel approach for scene independent dense crowd density estimation.
149,Vo2018, Recent advancements in systematic analysis of high resolution whole slide images have increase efficiency of diagnosis prognosis and prediction of cancer and important diseases. Due to the enormous sizes and dimensions of whole slide images the analysis requires extensive computing resources which are not commonly available. Images have to be tiled for processing due to computer memory limitations which lead to inaccurate results due to the ignorance of boundary crossing objects. Thus we propose a generic and highly scalable cloud-based image analysis framework for whole slide images. The framework enables parallelized integration of image analysis steps such as segmentation and aggregation of micro-structures in a single pipeline and generation of final objects manageable by databases. The core concept relies on the abstraction of objects in whole slide images as different classes of spatial geometries which in turn can be handled as text based records in MapReduce. The framework applies an overlapping partitioning scheme on images and provides parallelization of tiling and image segmentation based on MapReduce architecture. It further provides robust object normalization graceful handling of boundary objects with an efficient spatial indexing based matching method to generate accurate results. Our experiments on Amazon EMR show that MaReIA is highly scalable generic and extremely cost effective by benchmark tests.
150,Kotseruba2018, In this paper we present a broad overview of the last 40 years of research on cognitive architectures. To date the number of existing architectures has reached several hundred but most of the existing surveys do not reflect this growth and instead focus on a handful of well-established architectures. In this survey we aim to provide a more inclusive and high-level overview of the research on cognitive architectures. Our final set of 84 architectures includes 49 that are still actively developed and borrow from a diverse set of disciplines spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities such as perception attention mechanisms action selection memory learning reasoning and metareasoning. In order to assess the breadth of practical applications of cognitive architectures we present information on over 900 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight the overall trends in the development of the field. In addition to summarizing the current state-of-the-art in the cognitive architecture research this survey describes a variety of methods and ideas that have been tried and their relative success in modeling human cognitive abilities as well as which aspects of cognitive behavior need more research with respect to their mechanistic counterparts and thus can further inform how cognitive science might progress.
151,Kwon2018b, Today as the number of drones has suddenly increased the problems of collision between drones in low altitude flight and Traffic control in drone flight have been highlighted. The collision between drones has a potential danger that can be extended to a very serious accident which is directly linked to the second or third accident rather than the first accident. More detailed and thorough air traffic management is essential in order to minimize the collisions between drones and predict or address the collision before the collision occurs. However the current unmanned traffic management technology is a basic research level and it is urgent to establish the concept. Therefore the purpose of this study is to design and compare the airspace maps for drones which is a core technical element in construction of drone Traffic control system. As a result of this study based on the building height values in the conventional navigation map a drone obstacle map for each airspace area is displayed. Also the airspace classification is analyzed and the design definition and method for the uncontrolled drones are suggested.
152,Vivaldini2018, Eucalyptus represents one of the main sources of raw material in Brazil and each year substantial losses estimated at $400 million occur due to diseases. The active monitoring of eucalyptus crops can help getting accurate information about contaminated areas in order to improve response time. Unmanned aerial vehicles (UAVs) provide low-cost data acquisition and fast scanning of large areas however the success of the data acquisition process depends on an efficient planning of the flight route particularly due to traditionally small autonomy times. This paper proposes a single framework for efficient visual data acquisition using UAVs that combines perception environment representation and route planning. A probabilistic model of the surveyed environment containing diseased eucalyptus soil and healthy trees is incrementally built using images acquired by the vehicle in combination with GPS and inertial information for positioning. This incomplete map is then used in the estimation of the next point to be explored according to a certain objective function aiming to maximize the amount of information collected within a certain traveled distance. Experimental results show that the proposed approach compares favorably to other traditionally used route planning methods.
153,Wang2018g, Most existing person re-identification (re-id) methods are unsuitable for real-world deployment due to two reasons: Unscalability to large population size and Inadaptability over time. In this work we present a unified solution to address both problems. Specifically we propose to construct an identity regression space (IRS) based on embedding different training person identities (classes) and formulate re-id as a regression problem solved by identity regression in the IRS. The IRS approach is characterised by a closed-form solution with high learning efficiency and an inherent incremental learning capability with human-in-the-loop. Extensive experiments on four benchmarking datasets (VIPeR CUHK01 CUHK03 and Market-1501) show that the IRS model not only outperforms state-of-the-art re-id methods but also is more scalable to large re-id population size by rapidly updating model and actively selecting informative samples with reduced human labelling effort.
154,Pitsilis2018, This paper addresses the important problem of discerning hateful content in social media. We propose a detection scheme that is an ensemble of Recurrent Neural Network (RNN) classifiers and it incorporates various features associated with user-related information such as the users’ tendency towards racism or sexism. This data is fed as input to the above classifiers along with the word frequency vectors derived from the textual content. We evaluate our approach on a publicly available corpus of 16k tweets and the results demonstrate its effectiveness in comparison to existing state-of-the-art solutions. More specifically our scheme can successfully distinguish racism and sexism messages from normal text and achieve higher classification quality than current state-of-the-art algorithms.
155,Oztemel2018, Manufacturing industry profoundly impact economic and societal progress. As being a commonly accepted term for research centers and universities the Industry 4.0 initiative has received a splendid attention of the business and research community. Although the idea is not new and was on the agenda of academic research in many years with different perceptions the term “Industry 4.0” is just launched and well accepted to some extend not only in academic life but also in the industrial society as well. While academic research focuses on understanding and defining the concept and trying to develop related systems business models and respective methodologies industry on the other hand focuses its attention on the change of industrial machine suits and intelligent products as well as potential customers on this progress. It is therefore important for the companies to primarily understand the features and content of the Industry 4.0 for potential transformation from machine dominant manufacturing to digital manufacturing. In order to achieve a successful transformation they should clearly review their positions and respective potentials against basic requirements set forward for Industry 4.0 standard. This will allow them to generate a well-defined road map. There has been several approaches and discussions going on along this line a several road maps are already proposed. Some of those are reviewed in this paper. However the literature clearly indicates the lack of respective assessment methodologies. Since the implementation and applications of related theorems and definitions outlined for the 4th industrial revolution is not mature enough for most of the reel life implementations a systematic approach for making respective assessments and evaluations seems to be urgently required for those who are intending to speed this transformation up. It is now main responsibility of the research community to developed technological infrastructure with physical systems management models business models as well as some well-defined Industry 4.0 scenarios in order to make the life for the practitioners easy. It is estimated by the experts that the Industry 4.0 and related progress along this line will have an enormous effect on social life. As outlined in the introduction some social transformation is also expected. It is assumed that the robots will be more dominant in manufacturing implanted technologies cooperating and coordinating machines self-decision-making systems autonom problem solvers learning machines 3D printing etc. will dominate the production process. Wearable internet big data analysis sensor based life smart city implementations or similar applications will be the main concern of the community. This social transformation will naturally trigger the manufacturing society to improve their manufacturing suits to cope with the customer requirements and sustain competitive advantage. A summary of the potential progress along this line is reviewed in introduction of the paper. It is so obvious that the future manufacturing systems will have a different vision composed of products intelligence communications and information network. This will bring about new business models to be dominant in industrial life. Another important issue to take into account is that the time span of this so-called revolution will be so short triggering a continues transformation process to yield some new industrial areas to emerge. This clearly puts a big pressure on manufacturers to learn understand design and implement the transformation process. Since the main motivation for finding the best way to follow this transformation a comprehensive literature review will generate a remarkable support. This paper presents such a review for highlighting the progress and aims to help improve the awareness on the best experiences. It is intended to provide a clear idea for those wishing to generate a road map for digitizing the respective manufacturing suits. By presenting this review it is also intended to provide a hands-on library of Industry 4.0 to both academics as well as industrial practitioners. The top 100 headings abstracts and key words (i.e. a total of 619 publications of any kind) for each search term were independently analyzed in order to ensure the reliability of the review process. Note that this exhaustive literature review provides a concrete definition of Industry 4.0 and defines its six design principles such as interoperability virtualization local real-time talent service orientation and modularity. It seems that these principles have taken the attention of the scientists to carry out more variety of research on the subject and to develop implementable and appropriate scenarios. A comprehensive taxonomy of Industry 4.0 can also be developed through analyzing the results of this review.
156,Caster2018, Introduction and ObjectiveSocial media has been proposed as a possibly useful data source for pharmacovigilance signal detection. This study primarily aimed to evaluate the performance of established statistical signal detection algorithms in Twitter/Facebook for a broad range of drugs and adverse events.MethodsPerformance was assessed using a reference set by Harpaz et al. consisting of 62 US Food and Drug Administration labelling changes and an internal WEB-RADR reference set consisting of 200 validated safety signals. In total 75 drugs were studied. Twitter/Facebook posts were retrieved for the period March 2012 to March 2015 and drugs/events were extracted from the posts. We retrieved 4.3 million and 2.0 million posts for the WEB-RADR and Harpaz drugs respectively. Individual case reports were extracted from VigiBase for the same period. Disproportionality algorithms based on the Information Component or the Proportional Reporting Ratio and crude post/report counting were applied in Twitter/Facebook and VigiBase. Receiver operating characteristic curves were generated and the relative timing of alerting was analysed.ResultsAcross all algorithms the area under the receiver operating characteristic curve for Twitter/Facebook varied between 0.47 and 0.53 for the WEB-RADR reference set and between 0.48 and 0.53 for the Harpaz reference set. For VigiBase the ranges were 0.64 0.69 and 0.55 0.67 respectively. In Twitter/Facebook at best 31 (16%) and four (6%) positive controls were detected prior to their index dates in the WEB-RADR and Harpaz references respectively. In VigiBase the corresponding numbers were 66 (33%) and 17 (27%).ConclusionsOur results clearly suggest that broad-ranging statistical signal detection in Twitter and Facebook using currently available methods for adverse event recognition performs poorly and cannot be recommended at the expense of other pharmacovigilance activities.
157,Joshi2018, Machine vision inspection systems are often used for part classification applications to confirm that correct parts are available in manufacturing or assembly operations. Support vector machines (SVMs) and artificial neural networks (ANNs) are popular choices for classifiers. These supervised classifiers perform well when developed for specific applications and trained with known class images. Their drawback is that they cannot be easily applied to different applications without extensive retuning. Moreover for the same application they do not perform well if there are unknown class images. This paper proposes a novel solution to the above limitations of SVMs and ANNs with the development of a hybrid approach that combines supervised and semi-supervised layers. To illustrate its performance the system is applied to three different small part identification and sorting applications: (1) solid plastic gears (2) clear plastic wire connectors and (3) metallic Indian coins. The ability of the system to work with different applications with minimal tuning and user inputs illustrates its flexibility. The robustness of the system is demonstrated by its ability to reject unknown class images. Four hybrid classification methods were developed and tested: (1) SSVM USVM (2) USVM SSVM (3) USVM SANN and (4) SANN USVM. It was found that SANN USVM gave the best results with an accuracy of over 95% for all three applications. A software package known as FlexMVS for flexible machine vision system was written to illustrate the hybrid approach that enabled easy execution of the image conditioning feature extraction and classification steps. The image library and database used in this study is available at http://my.me.queensu.ca/People/Surgenor/Laboratory/Database.html.
158,Wang2018h, Ship capsizes in the extreme weather condition are difficult to predict even though they cause a great number of casualties. Numerical methods have been developed to analyze the capsize phenomena and to predict the possible capsizes. The visual simulation can augment the physical evaluation of ship motion and validation of the experimental results. However most of the existing visual simulations merely focus on the normal ship motion in general sea state and have less fascinating visual effect (VFX) compared with those in movie industry. In this paper we propose a visual simulation method to generate stormy wave surface to simulate wave properties such as splash and wake using particle simulation to render the wave and its relevant stormy features and finally to compose the visual effects together. Wave simulation is based on the modified Tessendorf grid method of ocean wave and SPH particles are adopted to simulate the wave properties. Lastly the integrated rendering is to produce the scene of ship motion in storm condition.
159,He2018a, In this editorial we first summarize the 2nd International Workshop on Semantics-Powered Data Analytics (SEPDA 2017) held on November 13 2017 in Kansas City Missouri U.S.A. and then briefly introduce 13 research articles included in this supplement issue covering topics such as Semantic Integration Deep Learning Knowledge Base Construction and Natural Language Processing.
160,Fan2018a, BackgroundDespite widespread use the safety of dietary supplements is open to doubt due to the fact that they can interact with prescribed medications leading to dangerous clinical outcomes. Electronic health records (EHRs) provide a potential way for active pharmacovigilance on dietary supplements since a fair amount of dietary supplement information especially those on use status can be found in clinical notes. Extracting such information is extremely significant for subsequent supplement safety research.MethodsIn this study we collected 2500 sentences for 25 commonly used dietary supplements and annotated into four classes: Continuing (C) Discontinued (D) Started (S) and Unclassified (U). Both rule-based and machine learning-based classifiers were developed on the same training set and evaluated using the hold-out test set. The performances of the two classifiers were also compared.ResultsThe rule-based classifier achieved F-measure of 0.90 0.85 0.90 and 0.86 in C D S and U status respectively. The optimal machine learning-based classifier (Maximum Entropy) achieved F-measure of 0.90 0.92 0.91 and 0.88 in C D S and U status respectively. The comparison result shows that the machine learning-based classifier has a better performance which is more efficient and scalable especially when the sample size doubles.ConclusionsMachine learning-based classifier outperforms rule-based classifier in categorization of the use status of dietary supplements in clinical notes. Future work includes applying deep learning methods and developing a hybrid system to approach use status classification task.
161,Li2018e, BackgroundExtracting relationships between chemicals and diseases from unstructured literature have attracted plenty of attention since the relationships are very useful for a large number of biomedical applications such as drug repositioning and pharmacovigilance. A number of machine learning methods have been proposed for chemical-induced disease (CID) extraction due to some publicly available annotated corpora. Most of them suffer from time-consuming feature engineering except deep learning methods. In this paper we propose a novel document-level deep learning method called recurrent piecewise convolutional neural networks (RPCNN) for CID extraction.ResultsExperimental results on a benchmark dataset the CDR (Chemical-induced Disease Relation) dataset of the BioCreative V challenge for CID extraction show that the highest precision recall and F-score of our RPCNN-based CID extraction system are 65.24 77.21 and 70.77% which is competitive with other state-of-the-art systems.ConclusionsA novel deep learning method is proposed for document-level CID extraction where domain knowledge piecewise strategy attention mechanism and multi-instance learning are combined together. The effectiveness of the method is proved by experiments conducted on a benchmark dataset.
162,Chen2018a, Thermal imaging is a nondestructive testing method for monitoring internal material changes that are indicated by changes in an object’s surface temperature. In this study field observation using thermographs was applied to monitor and analyze the breach process of large-scale earth dams. The earth dam test site was Landao Creek in Nantou County Taiwan. Four field tests were performed to monitor and analyze a single earth dam and two successive earth dams. Ponding first occurred at the lowland of the riverbed upstream and base seepage occurred at the base of the dam downstream; overtopping failure soon followed. Earth-dam failure mode is affected by the topographic characteristics of the riverbed; specifically lowland areas are prone to ponding upstream which causes piping and seepage downstream as well as subsequent breaches. Ponding and piping cause water seepage and soil wetting which are reflected in lower surface temperatures recorded on thermographs. Thermographs can monitor changes in surface temperature to evaluate the potential failure modes of dams. Dam surfaces with large temperature variations may be potential failure areas. If confirmed this fact may prove useful for failure-mode prediction. This paper proposes a monitoring index to reflect the temperature changes in a given period and this study verified it empirically.
163,Pektas2018, Compromised computer systems on the Internet namely botnets receive commands and share information with their central malicious systems while executing frequent and common network activities. Former botnet detection methods such as blacklists and botnet’s signature matching cannot timely and reliably discover evolving botnet variants. Analysis of botnet network communication flows can be used to discover behavior of botnets toward detection. A rich dataset constituted by both botnet and normal network traffic flow summaries can be used for training and testing purposes. Furthermore neural networks along emerging parallelization computing tools and processors may improve classification statistical metric results in an efficient manner. A neural network built by a higher number of layers and its architecture enhances classification accuracy. In this paper we present a combination of convolutional and recurrent neural network to identify botnets. To validate the effectiveness of the proposed method we test and benchmark the proposed method with two publicly available datasets which are CTU-13 and ISOT involving both botnet and normal data traffic. We evaluate statistical metric results by tuning the neural network architecture and compare the results with respect to baseline classifiers. Our experiment results show that the presented deep network learning-based botnet detection method is reached at 99.3% level in accuracy and 99.1% in F-measure respectively.
164,Licorish2018, Technology is being increasingly integrated into teaching environments in view of enhancing students’ engagement and motivation. In particular game-based student response systems have been found to foster students’ engagement enhance classroom dynamics and improve overall students’ learning experience. This article presents outcomes of research that examined students’ experience using a game-based student response system Kahoot! in an Information Systems Strategy and Governance course at a research-intensive teaching university in New Zealand. We conducted semi-structured interviews with students to learn about the extent to which Kahoot! influence classroom dynamics motivation and students’ learning process. Key findings revealed that Kahoot! enriched the quality of student learning in the classroom with the highest influence reported on classroom dynamics engagement motivation and improved learning experience. Our findings also suggest that the use of educational games in the classroom is likely to minimise distractions thereby improving the quality of teaching and learning beyond what is provided in conventional classrooms. Other factors that contributed to students’ enhanced learning included the creation and integration of appropriate content in Kahoot! providing students with timely feedback and game-play (gamification) strategies.
165,Guo2018a, Drowsiness and fatigue of the drivers are amongst the significant causes of the car accidents. Every year the number of deaths and fatalities are tremendously increasing due to multifaceted issues and henceforth requires an intelligent processing system for accident avoidance. In relevant with this an effective driver drowsiness detection system is proposed. The main challenges are robustness of the algorithm towards variation of the human face and real-time processing capability. The first challenge pertaining to the facial variation has been handled well using conventional image processing and hand-craft features of computer vision algorithms. Yet variations such as facial expression lighting condition intra-class variation and pose variation are additional issues that conventional method failed to address. Deep learning is an alternative solution which provides a better performance by learning features automatically. Thus this paper proposed a new concept for handling the real-time driver drowsiness detection using the hybrid of convolutional neural network (CNN) and long short-term memory (LSTM). The performance of the system has been tested using the public drowsy driver dataset from ACCV 2016 competition. The results show that it can outperform the former schemes in the literature.
166,Huang2018c, The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot’s behavior in novel situations. And since a robot’s behavior is often a direct result of its underlying objective function our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior in order to then show those behaviors that are maximally informative. We introduce two factors to define candidate models of human inference and show that certain models indeed produce example robot behaviors that better enable users to anticipate what it will do in novel situations. Our results also reveal that choosing the appropriate model is key and suggest that our candidate models do not fully capture how humans extrapolate from examples of robot behavior. We leverage these findings to propose a stronger model of human learning in this setting and conclude by analyzing the impact of different ways in which the assumed model of human learning may be incorrect.
167,Hu2018a, With the explosive growth of personal photos an effective classification tool is becoming an urgent need to organize our progressive image collections. Facing the dynamically growing collections we present a new method to categorize images effectively by integrating image clustering incremental updating and user feedback together in an online framework. Considering the user burden and the user-specific preference during image classification we propose several strategies to learn a customized classification model progressively for each user. Firstly we use a multi-view learning method to learn the preferred classification perspective of the user. Secondly we cluster similar images into groups according to user’s preference so that images in a group can be categorized simultaneously with high efficiency. Thirdly we propose a multi-centroid nearest class mean classifier to online learn the user’s preferred category granularity and use it to classify the image groups. Unlike offline systems where pre-labeling and batch training often take hours or even days to perform our approach is fully online. It can learn the classification model and classify newly acquired images alternately in no time. The sufficient experimental results and a user study demonstrate the effectiveness of the proposed method.
168,Wang2018i, To maintain national socio-economic development and maritime rights and interests it is necessary to obtain the space location information of various ships. Therefore it is important to detect the locations of ships accurately and rapidly. At present ship detection is mainly carried out by combining satellite remote sensing imaging with constant false alarm rate (CFAR) detection. However with the rapid development of satellite remote sensing technology remote sensing data have gradually begun to show the characteristics of “big data”; additionally the accuracy and speed of ship detection can be improved by analysing big data such as by deep learning. Thus a ship detection algorithm that combines CFAR and CNN is proposed based on the CFAR global detection algorithm and image recognition with the CNN model. Compared with the multi-level CFAR algorithm that is based on multithreading the algorithm in this paper is more suitable for application to ship detection systems.
169,Garcia-Zapirain2018, A 3D convolution neural network (CNN) of deep learning architecture is supplied with essential visual features to accurately classify and segment granulation necrotic eschar and slough tissues in pressure ulcer color images. After finding a region of interest (ROI) the features are extracted from both the original and convolved with a pre-selected Gaussian kernel 3D HSI images combined with first-order models of current and prior visual appearance. The models approximate empirical marginal probability distributions of voxel-wise signals with linear combinations of discrete Gaussians (LCDG). The framework was trained and tested on 193 color pressure ulcer images. The classification accuracy and robustness were evaluated using the Dice similarity coefficient (DSC) the percentage area distance (PAD) and the area under the ROC curve (AUC). The obtained preliminary DSC of 92% PAD of 13% and AUC of 95% are promising.
170,XXX,A 3D convolution neural network (CNN) of deep learning architecture is supplied with essential visual features to accurately classify and segment granulation necrotic eschar and slough tissues in pressure ulcer color images. After finding a region of interest (ROI) the features are extracted from both the original and convolved with a pre-selected Gaussian kernel 3D HSI images combined with first-order models of current and prior visual appearance. The models approximate empirical marginal probability distributions of voxel-wise signals with linear combinations of discrete Gaussians (LCDG). The framework was trained and tested on 193 color pressure ulcer images. The classification accuracy and robustness were evaluated using the Dice similarity coefficient (DSC) the percentage area distance (PAD) and the area under the ROC curve (AUC). The obtained preliminary DSC of 92% PAD of 13% and AUC of 95% are promising.
171,Govindasamy2018, This article proposes three software reliability models based on a hybrid approach combining NHPP models Weibull model and exponential model. The software failure is first categorised into three categories namely pure software failures hardware-induced software failures and user-induced software failures. Based on the failure behaviour NHPP models were adapted for pure software failures Weibull model for hardware-induced failures and exponential model for user-induced software failures. The failure intensity function mean value function and reliability function were determined. The proposed models are validated using big data analysis. From the data collected during the testing phase the optimal values of parameters were estimated using maximum likelihood estimation and genetic algorithm. The expected number of failures and the cumulative number of failures were calculated and comparison was made between the observed values to show the performance of the proposed models. A comparison criterion was also proposed to confirm the estimation accuracy. Finally a t test was conducted to test the significance of the difference between the observed and estimated values. Experimental results confirm the better estimation accuracy of the proposed models.
172,Floyd2018, Learning by observation agents learn to perform a behaviour by watching an expert perform that behaviour. The ability of the agents to learn correctly is therefore related to the quality and coverage of the observations. This article presents two novel approaches for observation acquisition mixed-initiative observation acquisition and delayed observation acquisition that allow learning agents to identify problems they are having difficulty solving and ask the expert for assistance solving them. The observation approaches are presented in the context of a case-based learning by observation agent and empirically compared to traditional passive observation in the domain of Tetris. Our results show that not only do the mixed-initiative and delayed observation acquisition approaches result in observations that cannot be obtained in a passive manner but they also improve the learning performance of an agent.
173,EhteshamiBejnordi2018, The breast stromal microenvironment is a pivotal factor in breast cancer development growth and metastases. Although pathologists often detect morphologic changes in stroma by light microscopy visual classification of such changes is subjective and non-quantitative limiting its diagnostic utility. To gain insights into stromal changes associated with breast cancer we applied automated machine learning techniques to digital images of 2387 hematoxylin and eosin stained tissue sections of benign and malignant image-guided breast biopsies performed to investigate mammographic abnormalities among 882 patients ages 40 65 years that were enrolled in the Breast Radiology Evaluation and Study of Tissues (BREAST) Stamp Project. Using deep convolutional neural networks we trained an algorithm to discriminate between stroma surrounding invasive cancer and stroma from benign biopsies. In test sets (928 whole-slide images from 330 patients) this algorithm could distinguish biopsies diagnosed as invasive cancer from benign biopsies solely based on the stromal characteristics (area under the receiver operator characteristics curve 0.962). Furthermore, without being trained specifically using ductal carcinoma in situ as an outcome, the algorithm detected tumor-associated stroma in greater amounts and at larger distances from grade 3 versus grade 1 ductal carcinoma in situ. Collectively, these results suggest that algorithms based on deep convolutional neural networks that evaluate only stroma may prove useful to classify breast biopsies and aid in understanding and evaluating the biology of breast lesions.
174,Suarez-Paniagua2018, BackgroundDeep Neural Networks (DNN) in particular Convolutional Neural Networks (CNN) has recently achieved state-of-art results for the task of Drug-Drug Interaction (DDI) extraction. Most CNN architectures incorporate a pooling layer to reduce the dimensionality of the convolution layer output preserving relevant features and removing irrelevant details. All the previous CNN based systems for DDI extraction used max-pooling layers.ResultsIn this paper we evaluate the performance of various pooling methods (in particular max-pooling average-pooling and attentive pooling) as well as their combination for the task of DDI extraction. Our experiments show that max-pooling exhibits a higher performance in F1-score (64.56%) than attentive pooling (59.92%) and than average-pooling (58.35%).ConclusionsMax-pooling outperforms the others alternatives because is the only one which is invariant to the special pad tokens that are appending to the shorter sentences known as padding. Actually the combination of max-pooling and attentive pooling does not improve the performance as compared with the single max-pooling technique.
175,Gupta2018, BackgroundSocial media is a useful platform to share health-related information due to its vast reach. This makes it a good candidate for public-health monitoring tasks specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media particularly from Twitter. Medical information extraction from social media is challenging mainly due to short and highly informal nature of text as compared to more technical and formal medical reports.MethodsCurrent methods in ADR mention extraction rely on supervised learning methods which suffer from labeled data scarcity problem. The state-of-the-art method uses deep neural networks specifically a class of Recurrent Neural Network (RNN) which is Long-Short-Term-Memory network (LSTM). Deep neural networks due to their large number of free parameters rely heavily on large annotated corpora for learning the end task. But in the real-world it is hard to get large labeled data mainly due to the heavy cost associated with the manual annotation.ResultsTo this end we propose a novel semi-supervised learning based RNN model which can leverage unlabeled data also present in abundance on social media. Through experiments we demonstrate the effectiveness of our method achieving state-of-the-art performance in ADR mention extraction.ConclusionIn this study we tackle the problem of labeled data scarcity for Adverse Drug Reaction mention extraction from social media and propose a novel semi-supervised learning based method which can leverage large unlabeled corpus available in abundance on the web. Through empirical study we demonstrate that our proposed method outperforms fully supervised learning based baseline which relies on large manually annotated corpus for a good performance.
176,Jiang2018, BackgroundAs Twitter has become an active data source for health surveillance research it is important that efficient and effective methods are developed to identify tweets related to personal health experience. Conventional classification algorithms rely on features engineered by human domain experts and engineering such features is a challenging task and requires much human intelligence. The resultant features may not be optimal for the classification problem and can make it challenging for conventional classifiers to correctly predict personal experience tweets (PETs) due to the various ways to express and/or describe personal experience in tweets. In this study we developed a method that combines word embedding and long short-term memory (LSTM) model without the need to engineer any specific features. Through word embedding tweet texts were represented as dense vectors which in turn were fed to the LSTM neural network as sequences.ResultsStatistical analyses of the results of 10-fold cross-validations of our method and conventional methods indicate that there exist significant differences (p<0.01) in performance measures of accuracy precision recall F1-score and ROC/AUC demonstrating that our approach outperforms the conventional methods in identifying PETs.ConclusionWe presented an efficient and effective method of identifying health-related personal experience tweets by combining word embedding and an LSTM neural network. It is conceivable that our method can help accelerate and scale up analyzing textual data of social media for health surveillance purposes because of no need for the laborious and costly process of engineering features.
177,Elton2018, We present a proof of concept that machine learning techniques can be used to predict the properties of CNOHF energetic molecules from their molecular structures. We focus on a small but diverse dataset consisting of 109 molecular structures spread across ten compound classes. Up until now candidate molecules for energetic materials have been screened using predictions from expensive quantum simulations and thermochemical codes. We present a comprehensive comparison of machine learning models and several molecular featurization methods - sum over bonds custom descriptors Coulomb matrices Bag of Bonds and fingerprints. The best featurization was sum over bonds (bond counting) and the best model was kernel ridge regression. Despite having a small data set we obtain acceptable errors and Pearson correlations for the prediction of detonation pressure detonation velocity explosive energy heat of formation density and other properties out of sample. By including another dataset with  300 additional molecules in our training we show how the error can be pushed lower although the convergence with number of molecules is slow. Our work paves the way for future applications of machine learning in this domain including automated lead generation and interpreting machine learning models to obtain novel chemical insights.
178,Zou2018, On account of transportation management a predictive model of the traffic flow is built up that would precisely predict the traffic flow reduce longer travel delays. In prediction model of traffic flow based on traditional neural network the parameters of prediction model need to be tuned through iterative processing and these methods easily get stuck in local minimum. The paper presents a novel prediction model based on back propagation bidirectional extreme learning machine (BP-BELM). Parameters of BP-BELM are not tuned by experience. Compared with back propagation neural network radial basis function support vector machine and other improved incremental ELM the combined simulations and comparisons demonstrate that BP-BELM is used in predicting the traffic flow for its suitability and effectivity.
179,Liu2018c, We present a numerical analysis on injection-induced crack propagation and coalescence in brittle rock. The DEM network coupling model in PFC is modified to capture the evolution of fracture geometry. An improved fluid flow model for fractured porous media is proposed and coupled with a bond-based DEM model to simulate the interactions among cracks induced by injecting fluid in two nearby flaws at identical injection rates. The material parameters are calibrated based on the macro-properties of Lac du Bonnet granite and KGD solution. A grain-based model which generates larger grains from assembles of particles bonded together is calibrated to identify the microscopic mechanical and hydraulic parameters of Lac du Bonnet granite such that the DEM model yields a ratio between the compressive and tensile strength consistent with experiments. The simulations of fluid injection reveal that the initial flaw direction plays a crucial role in crack interaction and coalescence pattern. When two initial flaws are aligned cracks generally propagate faster. Some geometrical measures from graph theory are used to analyze the geometry and connectivity of the crack network. The results reveal that initial flaws in the same direction may lead to a well-connected crack network with higher global efficiency.
180,Zhang2018c, Barely acceptable block I/O performance prevents virtualization from being widely used in the High- Performance Computing field. Although the virtio paravirtual framework brings great I/O performance improvement there is a sharp performance degradation when accessing high-performance NAND-flash-based devices in the virtual machine due to their data parallel design. The primary cause of this fact is the deficiency of block I/O parallelism in hypervisor such as KVM and Xen. In this paper we propose a novel design of block I/O layer for virtualization named VBMq. VBMq is based on virtio paravirtual I/O model aiming to solve the block I/O parallelism issue in virtualization. It uses multiple dedicated I/O threads to handle I/O requests in parallel. In the meanwhile we use polling mechanism to alleviate overheads caused by the frequent context switches of the VM’s notification to and from its hypervisor. Each dedicated I/O thread is assigned to a non-overlapping core to improve performance by avoiding unnecessary scheduling. In addition we configure CPU affinity to optimize I/O completion for each request. The CPU affinity setting is very helpful to reduce CPU cache miss rate and increase CPU efficiency. The prototype system is based on Linux 4.1 kernel and QEMU 2.3.1. Our measurements show that the proposed method scales graciously in the multi-core environment and provides performance which is 39.6x better than the baseline Received September 26 2016; accepted June 23 2017 E-mail: diming.zhang@gmail.com at most and approaches bare-metal performance.
181,Ambrogio2018, Neural-network training can be slow and energy intensive owing to the need to transfer the weight data for the network between conventional digital memory chips and processor chips. Analogue non-volatile memory can accelerate the neural-network training algorithm known as backpropagation by performing parallelized multiply accumulate operations in the analogue domain at the location of the weight data. However the classification accuracies of such in situ training using non-volatile-memory hardware have generally been less than those of software-based training owing to insufficient dynamic range and excessive weight-update asymmetry. Here we demonstrate mixed hardware software neural-network implementations that involve up to 204900 synapses and that combine long-term storage in phase-change memory near-linear updates of volatile capacitors and weight-data transfer with ‘polarity inversion’ to cancel out inherent device-to-device variations. We achieve generalization accuracies (on previously unseen data) equivalent to those of software-based training on various commonly used machine-learning test datasets (MNIST MNIST-backrand CIFAR-10 and CIFAR-100). The computational energy efficiency of 28065 billion operations per second per watt and throughput per area of 3.6 trillion operations per second per square millimetre that we calculate for our implementation exceed those of today’s graphical processing units by two orders of magnitude. This work provides a path towards hardware accelerators that are both fast and energy efficient particularly on fully connected neural-network layers.Analogue-memory-based neural-network training using non-volatile-memory hardware augmented by circuit simulations achieves the same accuracy as software-based training but with much improved energy efficiency and speed.
182,Zhu2018b, Person re-identification (re-id) plays an important role in video surveillance and forensics applications. In many cases person re-id should be conducted between video clips i.e. given a query pedestrian video from one camera the re-id system should retrieve the video clips containing the same person from other cameras. However person re-id between videos which we call video-based person re-id has not been well studied. In this paper we propose a visual-appearance-level and spatial-temporal-level dictionary learning (VSDL) approach for video-based person re-id. Specifically we first employ two kinds of models to represent each walking cycle in the video i.e. visual-appearance features of all frames within the walking cycle and a spatial-temporal feature vector. By separately learning a visual-appearance-level dictionary and a spatial-temporal-level dictionary from two kinds of representations each walking cycle can be represented as a coding coefficient. To enhance the discriminative ability of the obtained coding coefficients we design a representation coefficient discriminant term for VSDL. Experiments on the public iLIDS-VID and PRID 2011 datasets demonstrate the effectiveness of VSDL.
183,Christensen2018, We present a comparative quantitative assessment focused on measuring ‘stances towards inquiry’ among middle-school students who have received design education. Our assessments are based on results of a written survey questionnaire and a statistical analysis using ‘The Design Literacy assessment tool’. Our analysis suggest that participating students have internalized basic knowledge about design but lack adaptive aspects of design literacy specifically the capability to take a designerly stance towards inquiry when confronted with ‘wicked problems’. We suggest that due to societal developments students are becoming more ‘design literate’ but that they generally develop routine expertise in these first practical encounters with design processes whereas the more complex adaptive capabilities demand more education of both students and teachers.
184,Abdi2018, Improving traffic safety is one of the important goals of Intelligent Transportation Systems (ITS). In vehicle-based safety systems it is more desirable to prevent an accident than to reduce severity of injuries. Critical traffic problems such as accidents and traffic congestion require the development of new transportation systems. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. One of the solutions to prevent accidents is to provide information on the surrounding environment of the driver. Augmented Reality Head-Up Display (AR-HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper we propose a fast deep-learning-based object detection approaches for identifying and recognizing road obstacles types as well as interpreting and predicting complex traffic situations. A single convolutional neural network predicts region of interest and class probabilities directly from full images in one evaluation. We also investigated potential costs and benefits of using dynamic conformal AR cues in improving driving safety. A new AR-HUD approach to create real-time interactive traffic animations was introduced in terms of types of obstacle rules for placement and visibility and projection of these on an in-vehicle display. The novelty of our approach is that both global and local context information are integrated into a unified framework to distinguish the ambiguous detection outcomes enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads.
185,Chen2018b, Activity recognition is an important step towards monitoring and evaluating the functional health of an individual and it potentially promotes human-centric ubiquitous applications in smart homes particularly for senior healthcare. The nature of human activity characterized by a high degree of complexity and uncertainty however poses a great challenge to the design of good feature representations and the optimization of classifiers towards building a robust model for human activity recognition. In this study we propose to exploit deep learning techniques to automatically learn high-level features from the binary sensor data under the assumption that there exist discriminative latent patterns inherent in the simple low-level features. Specifically we extract high-level features with a stacked autoencoder that has a deep and hierarchy architecture and combine feature learning and classifier construction into a unified framework to obtain a jointly optimized activity recognizer. Besides we investigate two different original feature representations of the sensor data for latent feature learning. To evaluate the performance of the proposed method we conduct extensive experiments on three publicly available smart home datasets and compare it with a range of shallow models in terms of time-slice accuracy and class accuracy. Experimental results show that our proposed model achieves better recognition rates and generalizes better across different original feature representations indicating its applicability to the real-world activity recognition.
186,Comfort2018, IntroductionThere is increasing interest in social digital media (SDM) as a data source for pharmacovigilance activities; however SDM is considered a low information content data source for safety data. Given that pharmacovigilance itself operates in a high-noise lower-validity environment without objective ‘gold standards’ beyond process definitions the introduction of large volumes of SDM into the pharmacovigilance workflow has the potential to exacerbate issues with limited manual resources to perform adverse event identification and processing. Recent advances in medical informatics have resulted in methods for developing programs which can assist human experts in the detection of valid individual case safety reports (ICSRs) within SDM.ObjectiveIn this study we developed rule-based and machine learning (ML) models for classifying ICSRs from SDM and compared their performance with that of human pharmacovigilance experts.MethodsWe used a random sampling from a collection of 311189 SDM posts that mentioned Roche products and brands in combination with common medical and scientific terms sourced from Twitter Tumblr Facebook and a spectrum of news media blogs to develop and evaluate three iterations of an automated ICSR classifier. The ICSR classifier models consisted of sub-components to annotate the relevant ICSR elements and a component to make the final decision on the validity of the ICSR. Agreement with human pharmacovigilance experts was chosen as the preferred performance metric and was evaluated by calculating the Gwet AC1 statistic (gKappa). The best performing model was tested against the Roche global pharmacovigilance expert using a blind dataset and put through a time test of the full 311189-post dataset.ResultsDuring this effort the initial strict rule-based approach to ICSR classification resulted in a model with an accuracy of 65% and a gKappa of 46%. Adding an ML-based adverse event annotator improved the accuracy to 74% and gKappa to 60%. This was further improved by the addition of an additional ML ICSR detector. On a blind test set of 2500 posts the final model demonstrated a gKappa of 78% and an accuracy of 83%. In the time test it took the final model 48 h to complete a task that would have taken an estimated 44000 h for human experts to perform.ConclusionThe results of this study indicate that an effective and scalable solution to the challenge of ICSR detection in SDM includes a workflow using an automated ML classifier to identify likely ICSRs for further human SME review.
187,James2018, The International Maritime Industry (IMI) is a global web of shipping connecting all Continents and bringing together mariners from a multitude of national ethnic and linguistic backgrounds. Wherever humans interact they communicate in some form or another and the ability to communicate competently is the cornerstone of safety at sea. This paper identifies the growth of English as the lingua franca of the sea as a framework for the later discussion on current teaching practices. We consider that Maritime English (ME) fits into the category of a special-purpose form of English in that it has generated modified or adopted from other languages many terms and phrases that are only used in the IMI. This paper considers safety-to-practice as a factor in curriculum design teaching and assessment as the basis for ME competence. Further we propose opportunities to move industry focus forward through collaboration with stakeholders and discussion about teaching practices. This paper identifies a gap in maritime teaching practices. This paper further explores the use of authentic teaching as a way forward to improve maritime communication.
188,Xing2018, Natural language processing (NLP) or the pragmatic research perspective of computational linguistics has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result articles that leverage NLP techniques to predict financial markets are fast accumulating gradually establishing the research field of natural language based financial forecasting (NLFF) or from the application perspective stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF and bring about discussions across many different disciplines.
189,Xue2018, Realizing autonomy is a hot research topic for automatic vehicles in recent years. For a long time most of the efforts to this goal concentrate on understanding the scenes surrounding the ego-vehicle (autonomous vehicle itself). By completing low-level vision tasks such as detection tracking and segmentation of the surrounding traffic participants e.g. pedestrian cyclists and vehicles the scenes can be interpreted. However for an autonomous vehicle low-level vision tasks are largely insufficient to give help to comprehensive scene understanding. What are and how about the past the on-going and the future of the scene participants  This deep question actually steers the vehicles towards truly full automation just like human beings. Based on this thoughtfulness this paper attempts to investigate the interpretation of traffic scene in autonomous driving from an event reasoning view. To reach this goal we study the most relevant literatures and the state-of-the-arts on scene representation event detection and intention prediction in autonomous driving. In addition we also discuss the open challenges and problems in this field and endeavor to provide possible solutions.
190,Huang2018d, The sea surface vessel/ship classification is a challenging problem with enormous implications to the world’s global supply chain and militaries. The problem is similar to other well-studied problems in object recognition such as face recognition. However it is more complex since ships’ appearance is easily affected by external factors such as lighting or weather conditions viewing geometry and sea state. The large within-class variations in some vessels also make ship classification more complicated and challenging. In this paper we propose an effective multiple features learning (MFL) framework for ship classification which contains three types of features: Gabor-based multi-scale completed local binary patterns (MS-CLBP) patch-based MS-CLBP and Fisher vector and combination of Bag of visual words (BOVW) and spatial pyramid matching (SPM). After multiple feature learning feature-level fusion and decision-level fusion are both investigated for final classification. In the proposed framework typical support vector machine (SVM) classifier is employed to provide posterior-probability estimation. Experimental results on remote sensing ship image datasets demonstrate that the proposed approach shows a consistent improvement on performance when compared to some state-of-the-art methods.
191,Amraee2018, This paper presents a new method for detecting abnormal events in the surveillance systems. This method does not employ object detection or tracking and thus it does not fail in the crowded scenes. In the first step of proposed method the appropriate cell size is determined by calculating the prevalent size of the connected components. Then the redundant information is eliminated and the important regions are extracted from training data. This preprocessing significantly reduces the volume of training data in the learning phase. Next using the HOG descriptors and a multivariate Gaussian model the appearance anomalies i.e. the abnormality in terms of physical characteristics is detected. Besides that a simple algorithm is provided to detect the abnormal motion using the average optical flow of the cells. Experimental results on the UCSD-PED2 datasets show that the proposed method can reliably detect abnormal events in video sequences outperforming the current state-of-the-art methods.
192,Luo2018, It is a challenging task to recognize smoke from visual scenes due to large variations in the feature of color texture shapes etc. The current detection algorithms are mainly based on single feature or fusion of multiple static features of smoke which leads to low detection accuracy. To solve this problem this paper proposes a smoke detection algorithm based on the motion characteristics of smoke and the convolutional neural networks (CNN). Firstly a moving object detection algorithm based on background dynamic update and dark channel priori is proposed to detect the suspected smoke regions. Then the features of suspected region is extracted automatically by CNN on that the smoke identification is performed. Compared to previous work our algorithm improves the detection accuracy which can reach 99% in the testing sets. For the problem that the region of smoke is relatively small in the early stage of smoke generation the strategy of implicit enlarging the suspected regions is proposed which improves the timeliness of smoke detection. In addition a fine-tuning method is proposed to solve the problem of scarce of data in the training network. Also the algorithm has good smoke detection performance by testing under various video scenes.
193,Masino2018, Motivated by limitations of adverse drug reaction (ADR) detection in clinical trials and passive post-market drug safety surveillance systems a number of researchers have examined social media data as a potential ADR information source. Twitter is a particularly attractive platform because it has a large diverse user community. Two challenges faced in applying Twitter data are that ADR descriptions are infrequent relative to the overall number of user posts and human review of all posts is impractical. To address these challenges we framed the ADR detection problem as a binary classification task where our objective was to develop a computational method that can classify user posts known as tweets relative to the presence of an ADR description. We developed a convolutional neural network model (ConvNet) that processes tweets as represented by word vectors created using unsupervised learning on large datasets. The ConvNet model achieved an F1-score of 0.46 and sensitivity of 0.78 for tweet ADR classification on the test dataset compared to 0.37 F1-score and 0.33 sensitivity obtained by two baseline support vector machine (SVM) models that incorporated word embedding n-gram and lexicon features. We attribute the superior ConvNet model performance to its ability to process arbitrary length inputs which allows it to evaluate every word embedding in a given tweet and make better use of their semantic content as compared to the SVM models which require a fixed length aggregated embedding input. The results presented demonstrate the feasibility of detection of infrequent ADR mentions in large-scale media data.
194,Cha2018, Harsh noises come from air-conditioning units are chronic complaining issues to their users. Individual perceptions of noise levels have been generally quantified by means of subjective evaluation such as a jury test. This article proposes a classification approach to acoustic noise signals using a wavelet spectrum analysis. We derive energy spectrums of noise signals using a discrete wavelet transform at pre-specified window length. The energy spectrums are a linear form and represented by a Hurst parameter as an informative summary of long-range dependent signal data. The Hurst parameter controls the self-similarity scaling as well as the degree of long-range dependence. We estimate the Hurst parameter through the least squares regression of sample energy against a resolution level in the wavelet spectral domain. In the context of multi-class classification problem the classification of noise signals is performed by a nonlinear support vector machine (SVM) for parameter estimates of linear energy profiles containing the Hurst parameter. In an application example of air-conditioner noise signals empirical results show that the proposed method offers the higher level of accuracy in acoustic noise sound classification.
195,Petit2018, Most deaths occurring due to a surgical intervention happen postoperatively rather than during surgery. The current standard of care in many hospitals cannot fully cope with detecting and addressing post-surgical deterioration in time. For millions of patients this deterioration is left unnoticed leading to increased mortality and morbidity. Postoperative deterioration detection currently relies on general scores that are not fully able to cater for the complex post-operative physiology of surgical patients. In the last decade however advanced risk and warning scoring techniques have started to show encouraging results in terms of using the large amount of data available peri-operatively to improve postoperative deterioration detection. Relevant literature has been carefully surveyed to provide a summary of the most promising approaches as well as how they have been deployed in the perioperative domain. This work also aims to highlight the opportunities that lie in personalizing the models developed for patient deterioration for these particular post-surgical patients and make the output more actionable. The integration of pre- and intra-operative data e.g. comorbidities vitals lab data and information about the procedure performed in post-operative early warning algorithms would lead to more contextualized personalized and adaptive patient modelling. This combined with careful integration in the clinical workflow would result in improved clinical decision support and better post-surgical care outcomes.
196,Royakkers2018, In this paper we discuss the social and ethical issues that arise as a result of digitization based on six dominant technologies: Internet of Things robotics biometrics persuasive technology virtual & augmented reality and digital platforms. We highlight the many developments in the digitizing society that appear to be at odds with six recurring themes revealing from our analysis of the scientific literature on the dominant technologies: privacy autonomy security human dignity justice and balance of power. This study shows that the new wave of digitization is putting pressure on these public values. In order to effectively shape the digital society in a socially and ethically responsible way stakeholders need to have a clear understanding of what such issues might be. Supervision has been developed the most in the areas of privacy and data protection. For other ethical issues concerning digitization such as discrimination autonomy human dignity and unequal balance of power the supervision is not as well organized.
197,Chen2018c, Machinery fault diagnosis has progressed over the past decades with the evolution of machineries in terms of complexity and scale. High-value machineries require condition monitoring and fault diagnosis to guarantee their designed functions and performance throughout their lifetime. Research on machinery Fault diagnostics has grown rapidly in recent years. This paper attempts to summarize and review the recent R&D trends in the basic research field of machinery fault diagnosis in terms of four main aspects: Fault mechanism sensor technique and signal acquisition signal processing and intelligent diagnostics. The review discusses the special contributions of Chinese scholars to machinery fault diagnostics. On the basis of the review of basic theory of machinery fault diagnosis and its practical applications in engineering the paper concludes with a brief discussion on the future trends and challenges in machinery fault diagnosis.
198,Lee2018, This paper studies stability and inverse optimality for a class of linear consensus protocols applied to the identical linear time-invariant multi-agent systems where communications among the agents are described by a fixed digraph containing a spanning tree whose scaled Laplacian is diagonalizable. The concept of the scaled Laplacian normal Laplacian multiplied by a positive diagonal matrix allows a more general graph topology to be handled. First we show that partial stability and even more inverse optimality hold if either the scaling factors in each protocol are sufficiently large depending on graph properties or the system matrix A satisfies the Lyapunov inequality. Then duality principles between the agents’ and the consensus error dynamics are presented which provide additional properties regarding consensus-related stability and inverse optimality. And the results are characterized in terms of the related symmetric Laplacian and its algebraic connectivity when the given graph is scaled undirected. Finally through formation control simulation of a multi-agent mobile robot system for Г-scaled undirected and Г-scaled directed graphs we have verified the theory of this paper on the partial stability and the inverse optimality conditions.
199,Wen2018, A multiuser multiple-input-single-output simultaneous wireless information and power transfer downlink system with imperfect channel is considered. This paper maximizes the worst-case harvested energy of all users and maintains the signal-to-interference and noise ratio requirement. The unknown channel estimate error is assumed within a certain range. Under the assumption beamforming vector optimization is formulated as a semi-definite programming problem transformed by S-Procedure method. Using semidefinite relaxation technique an optimal beamforming vector is obtained for the multi-antenna transmitter. The robustness and effectiveness of our proposed algorithm are provided by the simulation results compared with the non-robust design.
200,Hariharan2018, PurposeClinical procedures that make use of fluoroscopy may expose patients as well as the clinical staff (throughout their career) to non-negligible doses of radiation. The potential consequences of such exposures fall under two categories namely stochastic (mostly cancer) and deterministic risks (skin injury). According to the “as low as reasonably achievable” principle the radiation dose can be lowered only if the necessary image quality can be maintained.MethodsOur work improves upon the existing patch-based denoising algorithms by utilizing a more sophisticated noise model to exploit non-local self-similarity better and this in turn improves the performance of low-rank approximation. The novelty of the proposed approach lies in its properly designed and parameterized noise model and the elimination of initial estimates. This reduces the computational cost significantly.ResultsThe algorithm has been evaluated on 500 clinical images (7 patients 20 sequences 3 clinical sites) taken at ultra-low dose levels i.e. 50% of the standard low dose level during electrophysiology procedures. An average improvement in the contrast-to-noise ratio (CNR) by a factor of around 3.5 has been found. This is associated with an image quality achieved at around 12 (square of 3.5) times the ultra-low dose level. Qualitative evaluation by X-ray image quality experts suggests that the method produces denoised images that comply with the required image quality criteria.ConclusionThe results are consistent with the number of patches used and they demonstrate that it is possible to use motion estimation techniques and “recycle” photons from previous frames to improve the image quality of the current frame. Our results are comparable in terms of CNR to Video Block Matching 3D a state-of-the-art denoising method. But qualitative analysis by experts confirms that the denoised ultra-low dose X-ray images obtained using our method are more realistic with respect to appearance.
201,Robu2018, PurposeImage-guidance systems have the potential to aid in laparoscopic interventions by providing sub-surface structure information and tumour localisation. The registration of a preoperative 3D image with the intraoperative laparoscopic video feed is an important component of image guidance which should be fast robust and cause minimal disruption to the surgical procedure. Most methods for rigid and non-rigid registration require a good initial alignment. However in most research systems for abdominal surgery the user has to manually rotate and translate the models which is usually difficult to perform quickly and intuitively.MethodsWe propose a fast global method for the initial rigid alignment between a 3D mesh derived from a preoperative CT of the liver and a surface reconstruction of the intraoperative scene. We formulate the shape matching problem as a quadratic assignment problem which minimises the dissimilarity between feature descriptors while enforcing geometrical consistency between all the feature points. We incorporate a novel constraint based on the liver contours which deals specifically with the challenges introduced by laparoscopic data.ResultsWe validate our proposed method on synthetic data on a liver phantom and on retrospective clinical data acquired during a laparoscopic liver resection. We show robustness over reduced partial size and increasing levels of deformation. Our results on the phantom and on the real data show good initial alignment which can successfully converge to the correct position using fine alignment techniques. Furthermore since we can pre-process the CT scan before surgery the proposed method runs faster than current algorithms.ConclusionThe proposed shape matching method can provide a fast global initial registration which can be further refined by fine alignment methods. This approach will lead to a more usable and intuitive image-guidance system for laparoscopic liver surgery.
202,Breda2018, Youth transitioning into university face numerous life challenges particularly in South Africa with its high levels of poverty and inequality. This article recognising the vulnerability of many students sets out to identify the resilience processes that facilitate the resilient outcomes of life satisfaction and academic progress. Using a sample of 232 psychosocially vulnerable undergraduate students a quantitative survey was conducted and analysed using multivariate procedures. Results indicate that 27% of the variance in life satisfaction was accounted for by 19 resilience variables with community relationships and family financial security being individually significant and that 18% of the variance in academic progress was accounted for with learning orientation being individually significant. Family relationships also emerged as important for both outcomes. The findings suggest that during times of adversity South African students draw in particular on relational resources in their home communities and that academic progress is protected from deterioration by vulnerable students’ love for learning. Practice implications for universities are proposed that go beyond reactive therapeutic services towards creating a supportive academic community.
203,Choudhury2018, Pedestrians are the most interesting as well as vulnerable entity from both safety and security perspective in the field of video surveillance. In this article we present a framework to detect pedestrians across a stationary camera view. Our propositions thrust upon developing a motion segmentation module and a feature extraction module for human localization. In the first stage a background subtraction method is proposed to collect the initial set of moving objects in the processed frame. A shape descriptor is then presented to encode the pattern of human body in terms of silhouette orientation histogram. Moreover the principle of Golden ratio is employed to formulate a part-based detector to alleviate the problem with occlusion. Both the above modules are first validated separately and then as a unified unit using various statistical measures. The proposed background subtraction module is simulated on twenty video clips taken from three benchmark datasets. The efficacy of our shape descriptor is validated on various image-windows taken from three publicly available datasets. The unified framework including both the modules are tested on two standard surveillance datasets. All the experimental results are uploaded at: https://sites.google.com/site/PedestriansInMotion.
204,Jin2018, To protect the contents of images in the mobile internet era during image storage and transmission image encryption has achieved a tremendous success during the last decades. Traditional color image encryption method often use the RGB color space. We have the observation that in non-RGB color spaces the luminance channels often contain more information for content recognition than the chroma channels do. Thus in this paper we propose to use high level encryption schemes in more informative channels and low level encryption schemes in less informative channels. The 2D Arnold’s cat map followed by the 3D Lu chaotic map are conducted in the luminance channel. The less complicated DNA coding and 1D logistic map based encryption scheme is leveraged in the chroma channels. We use this strategies in 4 typical non-RGB color spaces i.e. YCbCr YIQ HSV L*a*b*. We evaluate and compare the performances and the time consumptions of the methods in the 4 Non-RGB color spaces. The experimental results reveal that the encryption methods in Non-RGB color spaces can achieve similar results as the method that conducts the same encryption level in each channel of the RBG color space including the resistance to several attacks such as brute-force attack statistic attack correlation attack while consuming less time. The method in YCbCr color space performances the best in the time consumption.
205,Hui2018, The importance of digital citizenship has been well recognized and integrated in standardized school curriculum. However there are very few empirical studies that report on the success of these new initiatives. Our teaching experience suggest that students are able to perform well on exams that assess proper online conduct but they still fail to follow digital citizenship guidelines in practice. In this paper we present a study to investigate students’ attitudes and opinions on various digital citizenship concepts via a self-reported questionnaire that is designed to gain insights on what the students would actually do in real life. Our results show that among the nine digital citizenship elements students have the most appreciation for access communication literacy and security. On the other hand elements such as digital etiquette and health and wellness were trivialized and undervalued. Furthermore we found some students were unable to come to a consensus on what is right and wrong in certain scenarios pertaining to digital law. As the Internet continues to gain prominence in our daily lives these findings lead to important questions of how learning modules and how the overall education system need to change so to ensure the growth of good digital citizens in the future generation.
206,Asadi-Aghbolaghi2018, One of the most challenging tasks in computer vision is human action recognition. The recent development of depth sensors has created new opportunities in this field of research. In this paper a novel supervised spatio-temporal kernel descriptor (SSTKDes) is proposed from RGB-depth videos to establish a discriminative and compact feature representation of actions. To enhance the descriptive and discriminative ability of the descriptor extracted primary kernel-based features are transformed into a new space by exploiting a supervised training strategy; i.e. large margin nearest neighbor (LMNN). The LMNN highly reduces the error of a nearest neighbor classifier by minimizing the intra-class variations and maximizing the inter-class distances. Subsequently the efficient match kernel (EMK) is used to abstract the mid-level kernel features for a more efficient classification. The proposed approach is evaluated on five public benchmark datasets. The experimental evaluations demonstrate that the proposed method achieves superior performance to the state-of-the-art methods.
207,Streitz2018, This paper presents different manifestations and problems of the ‘smart-everything’ paradigm provides a critical reflection of its implications and proposes a human-centered design approach resulting in the provision of ‘people-oriented empowering smartness’. The approach is characterized by design goals like “keeping the human in the loop and in control” and the proposal that “smart spaces make people smarter”. The critical reflection implies to ‘redefine’ the ‘smart-everything’ paradigm. One could also say this is a proposal in the spirit of humanized computing. While the approach has general applicability the examples are mainly taken from the domain of employing information technology in current and future urban environments where one can observe an increasing hype indicated by the label ‘smart cities’. The paper argues that a citizen-centered design approach for future cities is needed for going beyond technology-driven ubiquitous instrumentations and installations of cities. To illustrate the situation the paper addresses several general problem sets concerning artificial intelligence and algorithmic automation as well as privacy issues. There are two trade-offs to be considered: (a) between human control and automation and (b) between privacy and smartness. People are not asked anymore beforehand for their permission to collect and process their personal data. People do not have the choice to decide and make the trade-off decision between smartness and privacy themselves but are confronted with serious privacy infringements. To remedy the situation a ‘privacy by design’ respectively ‘privacy by default’ approach is proposed. The combination of redefining the ‘smart-everything’ paradigm in terms of empowering people employing privacy by design and enforcing an overall citizen-centered design approach is guided by the goal of reconciling people and technology creating and maintaining a balance of decision-making and control entities. It should convince and incite all stakeholders “to move beyond ‘smart-only’ cities” and transform them into Humane Sociable and Cooperative Hybrid Cities.
208,Li2018f, IntroductionNon-recurrent congestion caused by traffic incident is difficult to predict but should be dealt with in a timely and effective manner to reduce its influence on road capacity reduction and enormous travel time loss. Influence factor analysis and reasonable prediction of traffic incident duration are important in traffic incident management to predict incident impacts and aid in the implementation of appropriate traffic operation strategies. The objective of this study is to conduct a thorough review and discusses the research evolution mainly including the different phases of incident duration data resources and the various methods that are applied in the traffic incident duration influence factor analysis and duration time prediction.MethodsIn order to achieve the goal of this study we presented a systematic review of traffic incident duration time estimation and prediction methods developed based on various data resource methodologies etc.Resultsbased on the previous studies we analyse (i) Data resources and characteristics: different traffic incident time phases data set size incident types duration time distribution available data resources significant influence factors and unobserved heterogeneity and randomness (ii) traffic incident duration analysis methods mainly including hazard-based duration model and regression and statistical tests (iii) traffic incident duration prediction methods and evaluation of prediction accuracy.ConclusionsAfter a comprehensive review of literature this study identifies and analyses future challenges and what can be achieved in the future to estimate and predict the traffic incident duration time.
209,Kaljahi2018, The concept of smart cities has quickly evolved to improve the quality of life and provide public safety. Smart cities mitigate harmful environmental impacts and offences and bring energy-efficiency cost saving and mechanisms for better use of resources based on ubiquitous monitoring systems. However existing visual ubiquitous monitoring systems have only been developed for a specific purpose. As a result they cannot be used for different scenarios. To overcome this challenge this paper presents a new ubiquitous visual surveillance mechanism based on classification of scene images. The proposed mechanism supports different applications including Soil Flood Air Plant growth and Garbage monitoring. To classify the scene images of the monitoring systems we introduce a new technique which combines edge strength and sharpness to detect focused edge components for Canny and Sobel edges of the input images. For each focused edge component a patch that merges nearest neighbor components in Canny and Sobel edge images is defined. For each patch the contribution of the pixels in a cluster given by k-means clustering on edge strength and sharpness is estimated in terms of the percentage of pixels. The same percentage values are considered as a feature vector for classification with the help of a Support Vector Machine (SVM) classifier. Experimental results show that the proposed technique outperforms the state-of-the-art scene categorization methods. Our experimental results demonstrate that the SVM classifier performs better than rule and template-based methods.
210,Zhang2018d, The widely used American Society of Anesthesiologists Physical Status (ASA PS) classification is subjective requires manual clinician review to score and has limited granularity. Our objective was to develop a system that automatically generates an ASA PS with finer granularity by creating a continuous ASA PS score. Supervised machine learning methods were used to create a model that predicts a patient’s ASA PS on a continuous scale using the patient’s home medications and comorbidities. Three different types of predictive models were trained: regression models ordinal models and classification models. The performance and agreement of each model to anesthesiologists were compared by calculating the mean squared error (MSE) rounded MSE and Cohen’s Kappa on a holdout set. To assess model performance on continuous ASA PS model rankings were compared to two anesthesiologists on a subset of ASA PS 3 case pairs. The random forest regression model achieved the best MSE and rounded MSE. A model consisting of three random forest classifiers (split model) achieved the best Cohen’s Kappa. The model’s agreement with our anesthesiologists on the ASA PS 3 case pairs yielded fair to moderate Kappa values. The results suggest that the random forest split classification model can predict ASA PS with agreement similar to that of anesthesiologists reported in literature and produce a continuous score in which agreement in accurately judging granularity is fair to moderate.
211,ArulMary2018, In cloud computing enormous information storage is one of the great challenging tasks in term of reliable storage of sensitive data and quality of storage service. Among different cloud security issues the data disaster recovery is the most critical issue. The motive of recovery technique is to help the user to collect data from any backup server when server lost his data and unable to provide data to the user. To achieve this purpose many types of research develop different techniques. Therefore in this paper we propose a data disaster recovery process using Oppositional Group search optimizer (OGSO) algorithm which is mainly avoid the disaster in the cloud. The proposed data recovery process consists of four modules such as (1) file uploading module (2) replica generation module (3) data backup module and (4) disaster recovery module. At first we split the data into a number of files and upload the file to the corresponding virtual machine using OGSO algorithm. After that we generate the replica based on each file bandwidth. The replica is mainly used for data backup strategy. Finally the user query based files are backup and retrieve based on replicas. The experimental results show that the proposed OGSO based data disaster recovery process is better than other approaches.
212,Nystroem2018, BackgroundGetting research into policy and practice in healthcare is a recognised world-wide concern. As an attempt to bridge the gap between research and practice research funders are requesting more interdisciplinary and collaborative research while actual experiences of such processes have been less studied. Accordingly the purpose of this study was to gain more knowledge on the interdisciplinary collaborative and partnership research process by investigating researchers’ experiences of and approaches to the process based on their participation in an inventive national research programme. The programme aimed to boost collaborative and partnership research and build learning structures while improving ways to lead manage and develop practices in Swedish health and social services.MethodsInterviews conducted with project leaders and/or lead researchers and documentation from 20 projects were analysed using directed and conventional content analysis.ResultsCollaborative approaches were achieved by design e.g. action research or by involving practitioners from several levels of the healthcare system in various parts of the research process. The use of dual roles as researcher/clinician or practitioner/PhD student or the use of education designed especially for practitioners or ‘student researchers’ were other approaches. The collaborative process constituted the area for the main lessons learned as well as the main problems. Difficulties concerned handling complexity and conflicts between different expectations and demands in the practitioner’s and researcher’s contexts and dealing with human resource issues and group interactions when forming collaborative and interdisciplinary research teams. The handling of such challenges required time resources knowledge interactive learning and skilled project management.ConclusionsCollaborative approaches are important in the study of complex phenomena. Results from this study show that allocated time arenas for interactions and skills in project management and communication are needed during research collaboration to ensure support and build trust and understanding with involved practitioners at several levels in the healthcare system. For researchers dealing with this complexity takes time and energy from the scientific process. For practitioners this puts demands on understanding a research process and how it fits with on-going organisational agendas and activities and allocating time. Some of the identified factors may be overlooked by funders and involved stakeholders when designing performing and evaluating interdisciplinary collaborative and partnership research.
213,Sattarian2018, Internet of Things (IoT) is turning into an essential part of daily life and numerous IoT-based scenarios will be seen in future of modern cities ranging from small indoor situations to huge outdoor environments. In this era navigation continues to be a crucial element in both outdoor and indoor environments and many solutions have been provided in both cases. On the other side recent smart objects have produced a substantial amount of various data which demands sophisticated data mining solutions to cope with them. This paper presents a detailed review of previous studies on using data mining techniques in indoor navigation systems for the loT scenarios. We aim to understand what type of navigation problems exist in different IoT scenarios with a focus on indoor environments and later on we investigate how data mining solutions can provide solutions on those challenges.
214,Zeng2018, Human action recognition typically requires a large amount of training samples which is often expensive and time-consuming to create. In this paper we present a novel approach for enhancing human actions with a limited number of samples via structural average curves analysis. Our approach first learns average sequences from each pair of video samples for every action class and then gather them with original video samples together to form a new training set. Action modeling and recognition are proposed to be performed with the resulting new set. Our technique was evaluated on four benchmarking datasets. Our classification results are superior to those obtained with the original training sets which suggests that the proposed method can potentially be integrated with other approaches to further improve their recognition performances.
215,Yang2018b, Despite the abundance of large-scale molecular and drug-response data the insights gained about the mechanisms underlying treatment efficacy in cancer has been in general limited. Machine learning algorithms applied to those datasets most often are used to provide predictions without interpretation or reveal single drug-gene association and fail to derive robust insights. We propose to use Macau a bayesian multitask multi-relational algorithm to generalize from individual drugs and genes and explore the interactions between the drug targets and signaling pathways’ activation. A typical insight would be: “Activation of pathway Y will confer sensitivity to any drug targeting protein X”. We applied our methodology to the Genomics of Drug Sensitivity in Cancer (GDSC) screening using gene expression of 990 cancer cell lines activity scores of 11 signaling pathways derived from the tool PROGENy as cell line input and 228 nominal targets for 265 drugs as drug input. These interactions can guide a tissue-specific combination treatment strategy for example suggesting to modulate a certain pathway to maximize the drug response for a given tissue. We confirmed in literature drug combination strategies derived from our result for brain skin and stomach tissues. Such an analysis of interactions across tissues might help target discovery drug repurposing and patient stratification strategies.
216,Kim2018b, With the development of big data computing technology most documents in various areas including politics economics society culture life and public health have been digitalized. The structure of conventional documents differs according to their authors or the organization that generated them. Therefore policies and studies related to their efficient digitalization and use exist. Text mining is the technology used to classify cluster extract search and analyze data to find patterns or features in a set of unstructured or structured documents written in natural language. In this paper a method for extracting associative feature information using text mining from health big data is proposed. Using health documents as raw data health big data are created by means of the Web. The useful information contained in health documents is extracted through text mining. Health documents as raw data are collected through Web scraping and then saved in a file server. The collected raw data of health documents are sentence type and thus morphological analysis is applied to create a corpus. The file server executes stop word removal tagging and the analysis of polysemous words in a preprocessing procedure to create a candidate corpus. TF-C-IDF is applied to the candidate corpus to evaluate the importance of words in a set of documents. The words classified as of high importance by TF-C-IDF are included in a set of keywords and the transactions of each document are created. Using an Apriori mining algorithm the association rules of keywords in the created transaction are analyzed and associative keywords are generated. TF-C-IDF weights and associative keywords are extracted from health big data as associative features. The proposed method is a base technology for creating added value in the healthcare industry in the era of the 4th industrial revolution. Its evaluation in terms of F-measure and efficiency showed its performance to be high. The method is expected to contribute to healthcare big data management and information search.
217,Wang2018j, With the advancing of industrialization and the advent of the information age intelligent robots play an increasingly important role in intelligent manufacturing intelligent transportation system Internet of things medical health and intelligent services. Based on working experiences and reviews on intelligent robot studies both in China and abroad the authors summarized researches on key and leading technologies related to human-robot collaboration driverless technology emotion recognition brain-computer interface bionic software robot and cloud platform big data network etc. The development trend of intelligent robot was discussed and reflections on and suggestions to intelligent robot development in China were proposed. The review is not only meant to overview leading technologies of intelligent robot all over the world but also provide related theories methods and technical guidance to the technological and industrial development of intelligent robot in China.
218,Bevelander2018, BackgroundYouth are an important target group for social network interventions because they are particularly susceptible to the adaptation of healthy and unhealthy habits and behaviors of others. They are surrounded by ‘social influence agents’ (i.e. role models such as family friends and peers) that co-determine their dietary intake and physical activity. However there is a lack of systematic and comprehensive research on the implementation of a social network approach in health campaigns. The MyMovez research project aims to fill this gap by developing a method for effective social network campaign implementation. This protocol paper describes the design and methods of Phase I of the MyMovez project aiming to unravel youth’s social network structures in combination with individual psychosocial and environmental factors related to energy intake and expenditure. In addition the Wearable Lab is developed to enable an attractive and state-of-the-art way of collecting data and online campaign implementation via social networks.MethodsPhase I of the MyMovez project consists of a large-scale cross-sequential cohort study (N 953; 8-12 and 12-15 y/o). In five waves during a 3-year period (2016-2018), data are collected about youth’s social network exposure, media consumption, socialization experiences, psychological determinants of behavior, physical environment, dietary intake (snacking and drinking behavior) and physical activity using the Wearable Lab. The Wearable Lab exists of a smartphone-based research application (app) connected to an activity tracking bracelet, that is developed throughout the duration of the project. It generates peer- and self-reported (e.g., sociometric data and surveys) and experience sampling data, social network beacon data, real-time physical activity data (i.e., steps and cycling), location information, photos and chat conversation data from the app’s social media platform Social Buzz.DiscussionThe MyMovez project - Phase I is an innovative cross-sequential research project that investigates how social influences co-determine youth’s energy intake and expenditure. This project utilizes advanced research technologies (Wearable Lab) that provide unique opportunities to better understand the underlying processes that impact youths’ health-related behaviors. The project is theoretically and methodologically pioneering and produces a unique and useful method for successfully implementing and improving health campaigns.
219,Wang2018k, The main task of medical image registration is to match different modal images or the same modal images of different periods to provide the doctor with richer diagnostic information. Image registration has been widely used in image diagnostics image-guided surgical planning and real-time interventional surgical navigation. The deformation model is a key part of the image registration process and can drive the image deformation to achieve a perfect match of the same organization in the two images. In practical application it is important to establish a reasonable registration deformation model according to the research object which directly affects the registration results. This paper presents a review of the deformation models used in medical image registration. The deformation model is summarized with respect to four aspects: the elastic image model the viscoelastic image model the optical flow model and the prior knowledge model. We primarily summarize the deformation models with good registration results in recent years and analyze their adaptability and existing defects. The purpose of this paper is to provide a reference for the selection of a deformation model.
220,Hu2018b, Fire is one of the most dangerous disasters threatening human life and property globally. In order to reduce fire losses researches on video analysis for early smoke detection have become particularly significant. However it is still a challenging task to extract stable features for smoke recognition largely due to its variations in color shapes and texture. Classical convolutional neural networks can automatically learn feature representations of appearance from a single frame but fail to capture motion information between frames. For addressing this issue in this paper we propose a spatial-temporal based convolutional neural network for video smoke detection and for real-time detection propose an enhanced architecture which utilizes a multitask learning strategy to jointly recognize smoke and estimate optical flow capturing intra-frame appearance features and inter-frame motion features simultaneously. The effectiveness and efficiency of our proposed method is validated by experiments carried out on our self-created dataset which achieves 97.0% detection rate and 3.5% false alarm rate with processing time of 5ms per frame obviously outperforming existing methods.
221,Park2018, Detecting when something unusual has happened could help assistive robots operate more safely and effectively around people. However the variability associated with people and objects in human environments can make anomaly detection difficult. We previously introduced an algorithm that uses a hidden Markov model (HMM) with a log-likelihood detection threshold that varies based on execution progress. We now present an improved version of our previous algorithm (HMM-D) and introduce a new algorithm based on Gaussian process regression (HMM-GP). We also present a new and more thorough evaluation of 8 anomaly detection algorithms with force sound and kinematic signals collected from a robot closing microwave doors latching a toolbox scooping yogurt and feeding yogurt to able-bodied participants. Overall HMM-GP had the highest performance in terms of area under the curve for these real-world tasks and multiple modalities improved performance with some anomalies being better detected with particular modalities. With synthetic anomalies HMM-D exhibited shorter detection delays and outperformed HMM-GP with high-magnitude anomalies. In general higher-magnitude synthetic anomalies tended to be detected more rapidly.
222,ArguelloCasteleiro2018, BackgroundAutomatic identification of term variants or acceptable alternative free-text terms for gene and protein names from the millions of biomedical publications is a challenging task. Ontologies such as the Cardiovascular Disease Ontology (CVDO) capture domain knowledge in a computational form and can provide context for gene/protein names as written in the literature. This study investigates: 1) if word embeddings from Deep Learning algorithms can provide a list of term variants for a given gene/protein of interest; and 2) if biological knowledge from the CVDO can improve such a list without modifying the word embeddings created.MethodsWe have manually annotated 105 gene/protein names from 25 PubMed titles/abstracts and mapped them to 79 unique UniProtKB entries corresponding to gene and protein classes from the CVDO. Using more than 14 M PubMed articles (titles and available abstracts) word embeddings were generated with CBOW and Skip-gram. We setup two experiments for a synonym detection task each with four raters and 3672 pairs of terms (target term and candidate term) from the word embeddings created. For Experiment I the target terms for 64 UniProtKB entries were those that appear in the titles/abstracts; Experiment II involves 63 UniProtKB entries and the target terms are a combination of terms from PubMed titles/abstracts with terms (i.e. increased context) from the CVDO protein class expressions and labels.ResultsIn Experiment I Skip-gram finds term variants (full and/or partial) for 89% of the 64 UniProtKB entries while CBOW finds term variants for 67%. In Experiment II (with the aid of the CVDO) Skip-gram finds term variants for 95% of the 63 UniProtKB entries while CBOW finds term variants for 78%. Combining the results of both experiments Skip-gram finds term variants for 97% of the 79 UniProtKB entries while CBOW finds term variants for 81%.ConclusionsThis study shows performance improvements for both CBOW and Skip-gram on a gene/protein synonym detection task by adding knowledge formalised in the CVDO and without modifying the word embeddings created. Hence the CVDO supplies context that is effective in inducing term variability for both CBOW and Skip-gram while reducing ambiguity. Skip-gram outperforms CBOW and finds more pertinent term variants for gene/protein names annotated from the scientific literature.
223,Li2018g, To meet the urgent requirement of reliable artificial intelligence applications we discuss the tight link between artificial intelligence and intelligence test in this paper. We highlight the role of tasks in intelligence test for all kinds of artificial intelligence. We explain the necessity and difficulty of describing tasks for intelligence test checking all the tasks that may encounter in intelligence test designing simulation-based test and setting appropriate test performance evaluation indices. As an example we present how to design reliable intelligence test for intelligent vehicles. Finally we discuss the future research directions of intelligence test.
224,Park2018a, In this paper we propose an anomaly detection system of machines using a hybrid learning mechanism that combines two kinds of machine learning approaches namely unsupervised and non-parametric learning. To do so we used vibration data which is known to be suitable for anomaly detection in machines during operation. Furthermore in order to take into account various characteristics of abnormal data such as scarcity and diversity we propose a novel method that can detect anomalous behaviors using normal patterns instead of abnormal patterns from the machines. That is we first perform a machine learning of the normal patterns of the machines during operation and if any of the operation patterns deviates from the normal pattern we identify that pattern as abnormal. A key characteristic of our system is that it does not use any prior information such as predefined data labels or data distributions to learn the normal operation patterns. To demonstrate the superiority of our system we constructed a test bed consisting of a washing machine and a 3-axis accelerometer. We also demonstrated that our system can improve the accuracy of anomaly detection for the machines compared to other approaches.
225,Sailunaz2018, Emotion recognition has emerged as an important research area which may reveal some valuable input to a variety of purposes. People express their emotions directly or indirectly through their speech facial expressions gestures or writings. Many different sources of information such as speech text and visual can be used to analyze emotions. Nowadays writings take many forms of social media posts micro-blogs news articles etc. and the content of these posts can be useful resource for text mining to discover and unhide various aspects including emotions. Extracting emotions behind these postings is an immense and complicated task. To tackle this problem researchers from diverse fields are trying to find an efficient way to more precisely detect human emotions from various sources including text and speech. In this sense different word-based and sentence-based techniques machine learning natural language processing methods etc. have been used to achieve better accuracy. Analyzing emotions can be helpful in many different domains. One such domain is human computer interaction. With the help of emotion recognition computers can make better decisions to help users. With the increase in popularity of robotic research emotion recognition will also help making human robot interaction more natural. This survey covers existing emotion detection research efforts emotion models emotion datasets emotion detection techniques their features limitations and some possible future directions. We focus on reviewing research efforts analyzing emotions based on text and speech. We investigated different feature sets that have been used in existing methodologies. We summarize basic achievements in the field and highlight possible extensions for better outcome.
226,Nunes2018, Legislation on the testing of self-driving cars does not address liability and safety concerns warn Ashley Nunes Bryan Reimer and Joseph F. Coughlin.Legislation on the testing of self-driving cars does not address liability and safety concerns warn Ashley Nunes Bryan Reimer and Joseph F. Coughlin.
227,Kong2018, The Industry 4.0 program and corresponding international initiatives continue to transform the industrial workforce and their work. The service-oriented customer-centric and demand-driven production is pushing forward the progress of industrial automation. Even though it does not mean that human can be fully replaced by machines/robots. There is an increasing awareness that human presence is not only one type of manufacturing capability but also contributes to the overall system’s fault tolerant. How to achieve the seamless integration between human and machines/robots and harness human’s full potential is a critical issue for the success of Industry 4.0. In this research a human-centric empowering technology: industrial wearable system is proposed. The aim of this system is to establish a human cyber physical symbiosis to support real time trusting and dynamic interaction among operators machines and production systems. In order to design a substantial framework three world-leading R&D groups in this field are investigated. Five design considerations have been identified from real-life pilot projects. The future trends and research opportunities also show great promise of industrial wearable system in the next generation of manufacturing.
228,Komiske2018, A bstractWe introduce the energy flow polynomials: a complete set of jet substructure observables which form a discrete linear basis for all infrared- and collinear-safe observables. Energy flow polynomials are multiparticle energy correlators with specific angular structures that are a direct consequence of infrared and collinear safety. We establish a powerful graph-theoretic representation of the energy flow polynomials which allows us to design efficient algorithms for their computation. Many common jet observables are exact linear combinations of energy flow polynomials and we demonstrate the linear spanning nature of the energy flow basis by performing regression for several common jet observables. Using linear classification with energy flow polynomials we achieve excellent performance on three representative jet tagging problems: quark/gluon discrimination boosted W tagging and boosted top tagging. The energy flow basis provides a systematic framework for complete investigations of jet substructure using linear methods.
229,Lee2018a, The purposes of this study are to evaluate the feasibility of protocol determination with a convolutional neural networks (CNN) classifier based on short-text classification and to evaluate the agreements by comparing protocols determined by CNN with those determined by musculoskeletal radiologists. Following institutional review board approval the database of a hospital information system (HIS) was queried for lists of MRI examinations referring department patient age and patient gender. These were exported to a local workstation for analyses: 5258 and 1018 consecutive musculoskeletal MRI examinations were used for the training and test datasets respectively. The subjects for pre-processing were routine or tumor protocols and the contents were word combinations of the referring department region contrast media (or not) gender and age. The CNN Embedded vector classifier was used with Word2Vec Google news vectors. The test set was tested with each classification model and results were output as routine or tumor protocols. The CNN determinations were evaluated using the receiver operating characteristic (ROC) curves. The accuracies were evaluated by a radiologist-confirmed protocol as the reference protocols. The optimal cut-off values for protocol determination between routine protocols and tumor protocols was 0.5067 with a sensitivity of 92.10% a specificity of 95.76% and an area under curve (AUC) of 0.977. The overall accuracy was 94.2% for the ConvNet model. All MRI protocols were correct in the pelvic bone upper arm wrist and lower leg MRIs. Deep-learning-based convolutional neural networks were clinically utilized to determine musculoskeletal MRI protocols. CNN-based text learning and applications could be extended to other radiologic tasks besides image interpretations improving the work performance of the radiologist.
230,Xu2018a, The material attributes of foreign object debris (FOD) are the most crucial factors to understand the level of damage sustained by an aircraft. However the prevalent FOD detection systems lack an effective method for automatic material recognition. This paper proposes a novel FOD material recognition approach based on both transfer learning and a mainstream deep convolutional neural network (D-CNN) model. To this end we create an FOD image dataset consisting of images from the runways of Shanghai Hongqiao International Airport and the campus of our research institute. We optimize the architecture of the D-CNN by considering the characteristics of the material distribution of the FOD. The results show that the proposed approach can improve the accuracy of material recognition by 39.6% over the state-of-the-art method. The work here will help enhance the intelligence capability of future FOD detection systems and encourage other practical applications of material recognition technology.
231,Gao2018, With the continuous development of the intelligent vehicle vehicle security events occur frequently therefore the vehicle information security is particularly important. In this paper the in-vehicle security measures are analyzed especially the current situation of in-vehicle intrusion detection system which are mainly aimed at specific vehicles and are not enough to meet the need of vehicle security. Then a new in-vehicle intrusion detection mechanism is proposed based on deep learning and the set of experience knowledge structure (SOEKS) which is a knowledge representation structure. Utilizing SOEKS and information entropy to increase the versatility of intrusion detection for different vehicle. In practice the more precise model for specific vehicle can formed by training a large amount of specific vehicle data through deep learning. It is demonstrated with experimental results that the proposed approach is able to have 98% accuracy and detect a wide range of in-vehicle attacks.
232,Hussein2018, Deep learning techniques have shown success in learning from raw high-dimensional data in various applications. While deep reinforcement learning is recently gaining popularity as a method to train intelligent agents utilizing deep learning in imitation learning has been scarcely explored.
233,Zhang2018e, This paper described manage sewer in-line storage control for the city of Drammen Norway. The purpose of the control is to use the free space of the pipes to reduce overflow at the wastewater treatment plant (WWTP). This study combined the powerful sides of the hydraulic model and neural networks. A detailed hydraulic model was developed to identify which part of the sewer system have more free space. Subsequently the effectiveness of the proposed control solution was tested. Simulation results showed that intentionally control sewer with free space could significantly reduce overflow at the WWTP. At last in order to enhance better decision making and give enough response time for the proposed control solution Recurrent Neural Network (RNN) was employed to forecast flow. Three RNN architectures namely Elman NARX (nonlinear autoregressive network with exogenous inputs) and a novel architecture of neural networks LSTM (Long Short-Term Memory) were compared. The LSTM exhibits the superior capability for time series prediction.
234,2018b, Special Program and Abstract issue of the 14th Annual Congress of the European Cardiac Arrhythmia Society (ECAS)
235,Xu2018b, As crop diseases bring huge losses every year in both developed and developing countries determining how to precisely predict crop disease severity to facilitate agricultural emergency management is really a worldwide problem. Previous studies have introduced machine learning (ML) techniques into crop disease prediction and achieved better experimental results. However the architectures of these ML models are unsuitable to model time series data. Moreover the dependences among observations over time and across space have not been taken into account in model construction. By applying data-mining techniques to dynamic spatial panels of remote sensing data and considering features of bioclimatic topographic and soil conditions as a supplement we propose a novel crop disease prediction framework for agricultural emergency management based on ensemble learning techniques and spatio-temporal recurrent neural network (STRNN) which is an extension of recurrent neural network (RNN) in time and space. Empirical experiments are conducted on a specific dataset which is built based on reported cases of wheat yellow rust outbreaks in the Longnan city. Experimental results indicate that our proposed method outperforms all baseline models in crop disease severity prediction. The managerial implication of our work is that by applying the proposed methodology some preparedness measures can be implemented in advance to prevent or mitigate the possible disasters according to predicted results. Notable economic and ecological benefits can be achieved by optimizing the frequency and timing of application of fungicide pesticides and other preventative measures.
236,Hao2018, Plasticity in our brain offers us promising ability to learn and know the world. Although great successes have been achieved in many fields few bio-inspired machine learning methods have mimicked this ability. Consequently when meeting large-scale or time-varying data these bio-inspired methods are infeasible due to the reasons that they lack plasticity and need all training data loaded into memory. Furthermore even the popular deep convolutional neural network (CNN) models have relatively fixed structures and cannot process time varying data well. Through incremental methodologies this paper aims at exploring an end-to-end lifelong learning framework to achieve plasticities of both the feature and classifier constructions. The proposed model mainly comprises of three parts: Gabor filters followed by max pooling layer offering shift and scale tolerance to input samples incremental unsupervised feature extraction and incremental SVM trying to achieve plasticities of both the feature learning and classifier construction. Different from CNN plasticity in our model has no back propogation (BP) process and does not need huge parameters. Our incremental models including IncPCANet and IncKmeansNet have achieved better results than PCANet and KmeansNet on minist and Caltech101 datasets respectively. Meanwhile IncPCANet and IncKmeansNet show promising plasticity of feature extraction and classifier construction when the distribution of data changes. Lots of experiments have validated the performance of our model and verified a physiological hypothesis that plasticity exists in high level layer better than that in low level layer.
238,Ruiz-Garcia2018, We have recently seen significant advancements in the development of robotic machines that are designed to assist people with their daily lives. Socially assistive robots are now able to perform a number of tasks autonomously and without human supervision. However if these robots are to be accepted by human users there is a need to focus on the form of human robot interaction that is seen as acceptable by such users. In this paper we extend our previous work originally presented in Ruiz-Garcia et al. (in: Engineering applications of neural networks: 17th international conference EANN 2016 Aberdeen UK September 2 5 2016 proceedings pp 79 93 2016. https://doi.org/10.1007/978-3-319-44188-7_6) to provide emotion recognition from human facial expressions for application on a real-time robot. We expand on previous work by presenting a new hybrid deep learning emotion recognition model and preliminary results using this model on real-time emotion recognition performed by our humanoid robot. The hybrid emotion recognition model combines a Deep Convolutional Neural Network (CNN) for self-learnt feature extraction and a Support Vector Machine (SVM) for emotion classification. Compared to more complex approaches that use more layers in the convolutional model this hybrid deep learning model produces state-of-the-art classification rate of $$96.26\%$$96.26% when tested on the Karolinska Directed Emotional Faces dataset (Lundqvist et al. in The Karolinska Directed Emotional Faces KDEF 1998) and offers similar performance on unseen data when tested on the Extended Cohn Kanade dataset (Lucey et al. in: Proceedings of the third international workshop on CVPR for human communicative behaviour analysis (CVPR4HB 2010) San Francisco USA pp 94 101 2010). This architecture also takes advantage of batch normalisation (Ioffe and Szegedy in Batch normalization: accelerating deep network training by reducing internal covariate shift. http://arxiv.org/abs/1502.03167 2015) for fast learning from a smaller number of training samples. A comparison between Gabor filters and CNN for feature extraction and between SVM and multilayer perceptron for classification is also provided.
239,Duan2018a, A rolling bearing is an essential component of a rotating mechanical transmission system. Its performance and quality directly affects the life and reliability of machinery. Bearings’ performance and reliability need high requirements because of a more complex and poor working conditions of bearings. A bearing with high reliability reduces equipment operation accidents and equipment maintenance costs and achieves condition-based maintenance. First in this paper the development of technology of the main individual physical condition monitoring and fault diagnosis of rolling bearings are introduced then the fault diagnosis technology of multi-sensors information fusion is introduced and finally the advantages disadvantages and trends developed in the future of the detection main individual physics technology and multi-sensors information fusion technology are summarized. This paper is expected to provide the necessary basis for the follow-up study of the fault diagnosis of rolling bearings and a foundational knowledge for researchers about rolling bearings.
240,Alshareet2018, Guided by the eagerness to fulfill business objectives quality assurance has become one of the highlighted topics in software engineering. With the rise of globalization and free markets software users are becoming increasingly powerful with their ability to buy or reject computer software. While there is agreement over achieving quality there is debate over the definition of quality. To illustrate literature shows inconsistencies between a software development team definition to quality and a user definition to quality. Recently there is a tendency amongst researchers to appreciate the need for studying quality from a user prospective. Following a systematic approach this research attempts to develop a QiUPS an expert system for predicting quality in use in early software development phases. With the scariness of research data in this field the research generates a dataset from the documentation of Information Communication and E-learning Technology Centre software projects. The research methodology followed a comparative approach as it statistically compared four different classification algorithms (CAs) in terms of accuracy in classifying the research dataset. After that the research results led the researchers to compare the performance of artificial neural networks with convolutional neural networks in three empirical experiments which is rarely researched. Finally the research incorporated the best CA with ISO 25010 in order to develop the novel QiUPS. The research results are consistent and contributive to this rarely researched area.
241,Wang2018l, With the proliferation of sensor-equipped portable mobile devices Mobile CrowdSensing (MCS) using smart devices provides unprecedented opportunities for collecting enormous surrounding data. In MCS applications a crucial issue is how to recruit appropriate participants from a pool of available users to accomplish released tasks satisfying both resource efficiency and sensing quality. In order to meet these two optimization goals simultaneously in this paper we present a novel MCS task allocation framework by aligning existing task sequence with users’ moving regularity as much as possible. Based on the process of mobility repetitive pattern discovery the original task allocation problem is converted into a pattern matching issue and the involved optimization goals are transformed into pattern matching length and support degree indicators. To determine a trade-off between these two competitive metrics we propose greedy-based optimal assignment scheme search approaches namely MLP MDP IU1 and IU2 algorithm with respect to matching length-preferred support degree-preferred and integrated utility respectively. Comprehensive experiments on realworld open data set and synthetic data set clearly validate the effectiveness of our proposed framework on MCS task optimal allocation.
242,Roy2018, Web reliability is gaining importance with time due to the exponential increase in the popularity of different social community networks mailing systems and other online applications. Hence to enhance the reliability of any existing web system the web administrators must have the knowledge of various web errors present in the system influences of various workload characteristics on the manifestation of several web errors and the relations among different workload characteristics. But in reality often it may not be possible to institute a generalized correspondence among several workload characteristics. Moreover the issues like the prediction and estimation of the cumulative occurrences of the source content failures and the corresponding time between failures of a web system become less highlighted by the reliability research community. Hence in this work the authors have presented a well-defined procedure (a forecasting framework) for the web admins to analyze and enhance the reliability of the web sites under their supervision. Initially it takes the HTTP access and the error logs to extract all the necessary information related to the workloads web errors and corresponding time between failures. Next we have performed the principal component analysis correlation analysis and the change point analysis to select the number of independent variables. Next we have developed various time series based forecasting models for foretelling the cumulative occurrences of the source content failures and the corresponding time between failures. In the current work the multivariate models also include various uncorrelated workloads the exogeneous and the endogenous noises for forecasting the web errors and the corresponding time between failures. The proposed methodology has been validated with usage statistics collected from the web sites belong of two highly renowned Indian academic institutions.
243,Cath2018, In October 2016 the White House the European Parliament and the UK House of Commons each issued a report outlining their visions on how to prepare society for the widespread use of artificial intelligence (AI). In this article we provide a comparative assessment of these three reports in order to facilitate the design of policies favourable to the development of a ‘good AI society’. To do so we examine how each report addresses the following three topics: (a) the development of a ‘good AI society’; (b) the role and responsibility of the government the private sector and the research community (including academia) in pursuing such a development; and (c) where the recommendations to support such a development may be in need of improvement. Our analysis concludes that the reports address adequately various ethical social and economic topics but come short of providing an overarching political vision and long-term strategy for the development of a ‘good AI society’. In order to contribute to fill this gap in the conclusion we suggest a two-pronged approach.
244,Lee2018b, Drug-resistant microorganism infections cause serious disease and can lead to mortality and morbidity. In particular Staphylococcus aureus induces pyrogenic and toxigenic infections and drug-resistance occurs rapidly. Multidrug-resistant S. aureus such as methicillin-resistant S. aureus and methicillin-sensitive S. aureus can also cause immunodeficiency and immune deficiency syndrome from lipoteichoic acid. However antimicrobial peptides such as KW4 have strong antimicrobial activity low cytotoxicity and high neutralization activity against endotoxin substances from Gram-negative bacteria. The objective of this study was to use a synthetic KW4 antimicrobial peptide to evaluate the inhibition of drug-resistance development antimicrobial activity and neutralizing activity in S. aureus Gram-positive bacteria. The KW4 peptide showed strong antimicrobial activity against drug-resistant S. aureus strains and significantly increased the anti-neutralizing activity of lipoteichoic acid in S. aureus 1630 drug-resistant bacteria. In addition S. aureus ATCC 29213 did not develop resistance to KW4 as with other antibiotic drugs. These results suggest that the KW4 peptide is an effective antibiotic and anti-neutralizing agent against multidrug-resistant S. aureus strains.
245,Liong2018, Micro-expressions can occur when a person attempts to conceal and suppress his true feelings and emotions both deliberately or unconsciously.In recent years facial micro-expression analysis has received tremendous attention in the field of psychology media and computer vision. However due to its subtlety and brief duration development of automated micro-expression detection and recognition system are still great challenges in the field of computer vision. In this paper we present a novel hybrid facial region extraction framework that combines heuristic and automatic approaches to better recognize spontaneous micro-expressions. Salient facial regions are statistically determined based on the occurrence frequency of facial action units instead of holistic utilization of the entire facial area. The regions were automatically selected according to the facial landmark coordinates. We tested on two recent publicly available datasets that provided sufficient samples while also fulfilling the criteria of being elicited spontaneously. To further confirm the reliability of the proposed method two distinct feature extractors were employed to describe micro-expression information. Results show consistent and promising performance in all scenarios considered. The best result achieved is an improvement of approximately 10.5% in CASME II and an increment of nearly 10% in SMIC. We also report F-measure precision and recall performance metrics that are most suited for the imbalanced nature of spontaneous micro-expression datasets.
246,Coole2018, Within the security stratum is the jurisdictional practice of physical security; however physical security is not currently a recognised professional undertaking. Professional standing is a social recognition designated by the public and legal arenas. Yet the physical security practitioner is instrumental in addressing security concerns. Therefore this study extends a recent article by Coole et al. (Secur J 1 29 2016) to articulate educational learning objectives within a formal knowledge structure. The study applied a cultural domain analysis using security expert focus groups to develop a physical security knowledge system. Findings indicated that knowledge subjects should be aligned in order to diagnose the security problem inference an optimal resolution and identify treatment strategies. The study articulated the overarching physical security learning objectives using a top-down structure and underpinned by subject learning objectives. The adoption of standardised learning objectives based on clear content requisites would represent a significant step forward in the professionalisation of the physical security professional.
247,Chang2018a, With the rapid development of wearable technology wearable medical devices have gradually garnered a significant amount of research interest. Motion reconstruction can accurately reproduce the posture of the user at the time of the accident which provides medical personnel with necessary reference information. However because of the vast range of human body activities motion reconstruction needs high-frequency sampling data to avoid the occurrence of errors. Moreover the fact that movements resulting from an accident can be irregular the difficulties arising from unexpected training samples. This study attempts to establish a real-time human body inferential motion reconstruction system on fall accident. The data of human motion is recorded by using tri-axis accelerometers and tri-axis gyroscopes. The angles and tracks of the human limbs computed and the next action occurrence point deduced using long short-term memory. Then the postural trajectory is corrected using feedback inference of gravity data from the end of a fall accident. Through the correction mechanism of bidirectional feedback the error diffusion caused can reduce efficiency. In this study using a parameter adjustment strategy under data sampling rate of 0.01 the average normal-m reconstruction rate as well as the fall-motion reconstruction rate can be determined. The overall posture is reproduced through the 3D video to ambulance personnel as a reference.
248,Neveol2018, BackgroundNatural language processing applied to clinical text or aimed at a clinical outcome has been thriving in recent years. This paper offers the first broad overview of clinical Natural Language Processing (NLP) for languages other than English. Recent studies are summarized to offer insights and outline opportunities in this area.Main BodyWe envision three groups of intended readers: (1) NLP researchers leveraging experience gained in other languages (2) NLP researchers faced with establishing clinical text processing in a language other than English and (3) clinical informatics researchers and practitioners looking for resources in their languages in order to apply NLP techniques and tools to clinical practice and/or investigation. We review work in clinical NLP in languages other than English. We classify these studies into three groups: (i) studies describing the development of new NLP systems or components de novo (ii) studies describing the adaptation of NLP architectures developed for English to another language and (iii) studies focusing on a particular clinical application.ConclusionWe show the advantages and drawbacks of each method and highlight the appropriate application context. Finally we identify major challenges and opportunities that will affect the impact of NLP on clinical practice and public health studies in a context that encompasses English as well as other languages.
249,Peng2018b, During the transportation of aquatic products living states of the aquatic products and carbon emission of the carriers are respectively the concerns of the carrier enterprises and the government. Drivers adds virtual living states of aquatic products to the real driving environments through augmented reality technology to help themselves control oxygen contents and temperatures and choose distribution routes. Furthermore the effectiveness of carbon emission policy is investigated by simulating the impact of the policy on the distribution operations. In the simulation the carriers take the minimization of overall costs of the distribution operations as the objective. In order to meet the requirements of carbon emission policies factors of customer demands road lengths and road congestion are used to determine the distribution. The simulation is implemented based on genetic algorithm: total costs are used as fitness values and distribution sequences are encoded into chromosomes. And the carbon emission price is introduced as a operating parameter. Through running the simulation we obtained the overall costs carbon emission amounts and carbon emission costs. And a linear equation are fitted on the output data passed the reliability test. Based on the linear equation it is concluded that the increase of the carbon emission price is helpful in reducing carbon emission amount but it would greatly increase the operation costs of the carriers.
250,Sathish2018, Person re-identification is a challenging problem in computer vision. Lots of research interest is observed in this area over the past few years. A model for complete person re-identification can prove useful in this direction. Use of convolutional neural networks for pedestrian detection can improve the accuracy of detection to a larger extent. Deriving a descriptor which is invariant to the changes in the illumination background and the pose can make the difference in the re-identification process. The predominant part of our work focuses on building a robust descriptor which can tackle such challenges. We have concentrated on building a descriptor by employing appearance-based features extracted both at local and global levels. Further the dimensionality of the descriptor is reduced using kernel PCA. Distance metric learning algorithms are used to evaluate the descriptor on three major benchmark datasets. We propose a complete person re-identification system which involves both pedestrian detection and person re-identification. Major contributions of this work are to detect pedestrians from surveillance videos using CNN-based learning and to generate a kernel-PCA-based spatial descriptor and evaluate the descriptor using known distance metric learning methods on benchmark datasets.
251,Kumar2018, Nowadays the video surveillance systems may be omnipresent but essential for supervision everywhere e.g. ATM airport railway station and other crowded situations. In the multi-view video systems various cameras are producing a huge amount of video content around the clock which makes it difficult for fast browsing retrieval and analysis. Accessing and managing such huge data in real time becomes a real challenging task because of inter-view dependencies illumination changes and the bearing of many inactive frames. The work highlights an accurate and efficient technique to detect and summarize the event in multi-view surveillance videos using boosting a machine learning algorithm as a solution to the above issues. Interview dependencies across multiple views of the video are captured via weak learning classifiers in boosting algorithm. The light changes and still frames are tackled with moving an object in the frame by Deep learning framework. It helps to reach the correct decision for the active frame and inactive frame without any prior information about the number of issues in a video. Target as well as subjective ratings clearly indicate the potency of our proposed DELTA model where it successfully reduces the video data while keeping the important information as events.
252,Ren2018, The M-Distinct is an excellent model that supports the anonymization of a fully dynamic set of data. This study aimed to explore and analyze the M-Distinct model. First of all sensitive values in the QI-Group have certain randomness which caused the M-Distinct model prone to be property attacked. However the (M CUS)-Distinct model was proposed and required additional records in anonymity in the process. Therefore its QI-Group-sensitive attribute value must belong to the same set of CUS to reduce the probability of property attacks. Secondly the M-Distinct model involved time and cost. The proposed (M CUS)-Distinct model creation phase generated disjoint barrel queues to ensure that each record could be stored in the record distribution phase which reduced the time complexity of the algorithm. Finally the experiment based on real data sets showed that the (M CUS)-Distinct model was superior to the M-Distinct model in terms of data security faked records processing and execution time.
253,Gupta2018b, The analysis and identification of different attributes of produce such as taxonomy vendor and organic nature is vital to verifying product authenticity in a distribution network. Though a variety of analysis techniques have been studied in the past we present a novel data-centric approach to classifying produce attributes. We employed visible and near infrared (NIR) spectroscopy on over 75000 samples across several fruit and vegetable varieties. This yielded 0.90 0.98 and 0.98 0.99 classification accuracies for taxonomy and farmer classes respectively. The most significant factors in the visible spectrum were variations in the produce color due to chlorophyll and anthocyanins. In the infrared spectrum we observed that the varying water and sugar content levels were critical to obtaining high classification accuracies. High quality spectral data along with an optimal tuning of hyperparameters in the support vector machine (SVM) was also key to achieving high classification accuracies. In addition to demonstrating exceptional accuracies on test data we explored insights behind the classifications and identified the highest performing approaches using cross validation. We presented data collection guidelines experimental design parameters and machine learning optimization parameters for the replication of studies involving large sample sizes.
254,Tripathi2018, Interest in automatic crowd behaviour analysis has grown considerably in the last few years. Crowd behaviour analysis has become an integral part all over the world for ensuring peaceful event organizations and minimum casualties in the places of public and religious interests. Traditionally the area of crowd analysis was computed using handcrafted features. However the real-world images and videos consist of nonlinearity that must be used efficiently for gaining accuracies in the results. As in many other computer vision areas deep learning-based methods have taken giant strides for obtaining state-of-the-art performance in crowd behaviour analysis. This paper presents a comprehensive survey of current convolution neural network (CNN)-based methods for crowd behaviour analysis. We have also surveyed popular software tools for CNN in the recent years. This survey presents detailed attributes of CNN with special emphasis on optimization methods that have been utilized in CNN-based methods. It also reviews fundamental and innovative methodologies both conventional and latest methods of CNN reported in the last few years. We introduce a taxonomy that summarizes important aspects of the CNN for approaching crowd behaviour analysis. Details of the proposed architectures crowd analysis needs and their respective datasets are reviewed. In addition we summarize and discuss the main works proposed so far with particular interest on CNNs on how they treat the temporal dimension of data their highlighting features and opportunities and challenges for future research. To the best of our knowledge this is a unique survey for crowd behaviour analysis using the CNN. We hope that this survey would become a reference in this ever-evolving field of research.
255,Pyrkov2018, Age-related physiological changes in humans are linearly associated with age. Naturally linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. In this work we used one-week long physical activity records from a 2003 2006 National Health and Nutrition Examination Survey (NHANES) to compare three increasingly accurate biological age models: the unsupervised Principal Components Analysis (PCA) score a multivariate linear regression and a state-of-the-art deep convolutional neural network (CNN). We found that the supervised approaches produce better chronological age estimations at the expense of a loss of the association between the aging acceleration and all-cause mortality. Consequently we turned to the NHANES death register directly and introduced a novel way to train parametric proportional hazards models suitable for out-of-the-box implementation with any modern machine learning software. As a demonstration we produced a separate deep CNN for mortality risks prediction that outperformed any of the biological age or a simple linear proportional hazards model. Altogether our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.
256,Gao2018a, Many real world pattern classification problems involve the process and analysis of multiple variables in temporal domain. This type of problem is referred to as Multivariate Time Series (MTS) problem. It remains a challenging problem due to the nature of time series data: high dimensionality large data size and updating continuously. In this paper we use three types of physiological signals from the driver to predict lane changes before the event actually occurs. These are the electrocardiogram (ECG) galvanic skin response (GSR) and respiration rate (RR) and were determined in prior studies to best reflect a driver’s response to the driving environment. A novel Group-wise Convolutional Neural Network MTS-GCNN model is proposed for MTS pattern classification. In our MTS-GCNN model we present a new structure learning algorithm in training stage. The algorithm exploits the covariance structure over multiple time series to partition input volume into groups then learns the MTS-GCNN structure explicitly by clustering input sequences with spectral clustering. Different from other feature-based classification approaches our MTS-GCNN can select and extract the suitable internal structure to generate temporal and spatial features automatically by using convolution and down-sample operations. The experimental results showed that in comparison to other state-of-the-art models our MTS-GCNN performs significantly better in terms of prediction accuracy.
257,Sukrat2018, The rapid growth of social networking usage has initiated a new business model called C2C s-commerce which has opened a novel opportunity for SNS users to conduct commercial activities among members. Novice vendors (low-maturity merchants) perform business using a trial-and-error method. Through learning by doing capability enhancement in online business arrangements to become a mature vendor is difficult and takes time. Having a recommendation system can effectively and systematically support inexperienced vendors to conduct online business and enable vendors to become high-maturity merchants. This study proposes architecture of a C2C s-commerce recommendation system presented in conjunction with its input process and output. The architecture was devised to infer from information collected from case study interviews observation and secondary research methods. Artificial intelligence technologies and big data were considered to design the proposed framework in order to generate efficient recommendations for capability enhancement in online business arrangements.
258,Krittanawong2018, Heart failure with preserved ejection fraction (HFpEF) a complex and debilitating syndrome is commonly seen in elderly populations. Exacerbation of HFpEF is among the most common reasons for hospital admission in the USA. The high rate of morbidity and mortality from this condition underscores the fact that HFpEF is heterogeneous complex and poorly characterized. Randomized controlled trials have been very successful at identifying treatments for HF with reduced ejection fraction (HFrEF) but effective treatment options for HFpEF are lacking. Here we discuss (1) the pathophysiology of HFpEF (2) a standardized diagnostic and therapeutic approach (3) a comparison of the management of recent guidelines and (4) challenges and future directions for HFpEF management. The authors believe that it is important to identify new subtypes of HFpEF to better classify genotypes and phenotypes of HFpEF and to develop novel targeted therapies. It is our hypothesis that big data analytics will shine new light on unique HFpEF phenotypes that better respond to treatment modalities.
259,Datta2018, A bstractPrevious studies have demonstrated the utility and applicability of machine learning techniques to jet physics. In this paper we construct new observables for the discrimination of jets from different originating particles exclusively from information identified by the machine. The approach we propose is to first organize information in the jet by resolved phase space and determine the effective N -body phase space at which discrimination power saturates. This then allows for the construction of a discrimination observable from the N -body phase space coordinates. A general form of this observable can be expressed with numerous parameters that are chosen so that the observable maximizes the signal vs. background likelihood. Here we illustrate this technique applied to discrimination of H→bb $$ H\to b\overlineb $$ decays from massive g→bb $$ g\to b\overlineb $$ splittings. We show that for a simple parametrization we can construct an observable that has discrimination power comparable to or better than widely-used observables motivated from theory considerations. For the case of jets on which modified mass-drop tagger grooming is applied the observable that the machine learns is essentially the angle of the dominant gluon emission off of the bb $$ b\overlineb $$ pair.
260,Curiel-Ramirez2018, Road traffic accidents are a leading cause of deaths globally and represent the main cause of death among 15 29 years olds (World Health Organization in Global status report on road safety 2015 World Health Organization Geneva 2015). Efforts have been made to develop and improve advanced driver assistance systems as to provide better quality of driving and reduce the number of traffic accidents. However most of the current solutions focus on developing systems for fully autonomous cars. This study investigates a modular architecture that could potentially assist drivers in situations such as traffic jams autonomous parking and detection of obstacles such as pedestrians. The aim is to develop a low-cost driver assistance system that could be acquired and easily incorporated by non-technical users. It is becoming increasingly feasible to build a powerful and low-cost system because of the low cost of big-sized cameras electronics and processing cards. As a proof of the concept a hardware and software solution had been developed for autonomous steering-wheel actuation that only utilizes a stereoscopic camera sensor.Graphical Abstract
261,Cui2018, For the fault tree analysis a basic event probability is often complicated. The probability is not constant and even can be represented by function. In order to analyze the system reliability and related characteristics we represent the probabilities of the basic events by functions. The variables of the function are n influencing factors on the basic events. We extend the top event probability from the constant value to n + 1-dimensional space considering n influencing factors and the probability is n + 1th dimension. Further research the n + 1-dimensional space with related mathematical methods and then transform the system probability analysis into the problem of mathematic. The above ideas are the space fault tree (SFT). In SFT component fault probability distribution replace basic event probability and system fault probability distribution replace top event probability. In this paper we research the electrical system fault probability distribution and explain the related construction process. The main factors influencing the system are working temperature c and working time t. This paper constructs the three-dimensional fault probability distribution of the components and the system and the probability importance and criticality importance of the components. With partial derivation of the system fault probability distribution by the c and t we study the change trend of the fault probability. The optimal replacement schemes of components and the scheme considering the cost are obtained. The results show SFT is feasible and reasonable to analyze the fault probability of system under multi-factor influence and suitable for deep learning of the characteristics of the system reliability change.
262,Keel2018, The purpose of this study is to evaluate the feasibility and patient acceptability of a novel artificial intelligence (AI)-based diabetic retinopathy (DR) screening model within endocrinology outpatient settings. Adults with diabetes were recruited from two urban endocrinology outpatient clinics and single-field non-mydriatic fundus photographs were taken and graded for referable DR (   pre-proliferative DR). Each participant underwent; (1) automated screening model; where a deep learning algorithm (DLA) provided real-time reporting of results; and (2) manual model where retinal images were transferred to a retinal grading centre and manual grading outcomes were distributed to the patient within 2 weeks of assessment. Participants completed a questionnaire on the day of examination and 1-month following assessment to determine overall satisfaction and the preferred model of care. In total 96 participants were screened for DR and the mean assessment time for automated screening was 6.9 minutes. Ninety-six percent of participants reported that they were either satisfied or very satisfied with the automated screening model and 78% reported that they preferred the automated model over manual. The sensitivity and specificity of the DLA for correct referral was 92.3% and 93.7% respectively. AI-based DR screening in endocrinology outpatient settings appears to be feasible and well accepted by patients.
263,Gharajeh2018, There are ingenious characteristics in humanistic behaviors so that they can be utilized by the most developers to design smart and complex systems. This paper proposes a novel knowledge and learning based method called behavior-based decision making BBDM in control and system engineering. It is an expert decision support system containing the learning ability to work based on humanistic behavioral reasoning. The knowledge base is built by the system based on various behavioral styles (e.g. safe) associated to other systems and humans. BBDM uses the knowledge-based information to make appropriate decisions when any desired behavioral style is requested from the system. It specifies a success rate for any desired style based on the obtained knowledge base with the aid of a behavioral inference system. This procedure can be used to select a proper system or human to accomplish a requested job. All operations of the BBDM method are performed by a proposed behavioral decision system called BDS which consists of three main units: decomposition behavioral inference and composition. The decomposition unit splits any behavioral style into several optional features (e.g. safety). The behavioral aggregation sub-unit aggregates all behavioral styles obtained by the system to define the total behavior. The behavioral inference unit produces a success set for any desired behavioral style. Finally the composition unit converts success set to success rate to specify the success probability of the desired style. Simulation results show that the proposed method has a high efficiency compared to some of the existing decision-making methods.Graphical Abstract
264,Cui2018a, Some fault data in an actual system operation has the strong discretization and big data characteristics. Meanwhile the external factors affect the system reliability and the change of factors may lead to the change of system reliability. At present the methods in safety system engineering lack the ability to process the multi-factor influence and fault big data simultaneously. But these are the general problems that the actual system reliability analysis must face to be resolved. In order to solve the problems on the basis of Discrete Space Fault Tree (DSFT) the Fuzzy Structured Element method is introduced to construct the Fuzzy Structured Element Discrete Space Fault Tree (EDSFT). The method can analyze the multi-factor influence on system reliability with DSFT and use Fuzzy Structured Element (E) to denote the discrete characteristics of fault big data. The results of EDSFT with E can preserve the characteristics of the original fault data distribution and lay the foundation for the analysis of fault big data. The research is particularly suitable for the analysis of system reliability under the fault big data and multi-factor influence.
265,Tutubalina2018, Pharmacovigilance and generally applications of natural language processing models to healthcare have attracted growing attention over the recent years. In particular drug reactions can be extracted from user reviews posted on the Web and automated processing of this information represents a novel and exciting approach to personalized medicine and wide-scale drug tests. In medical applications demographic information regarding the authors of these reviews such as age and gender is of primary importance; however existing studies usually either assume that this information is available or overlook the issue entirely. In this work we propose and compare several approaches to automated mining of demographic information from user-generated texts. We compare modern natural language processing techniques including extensions of topic models and convolutional neural networks (CNN). We apply single-task and multi-task learning approaches to this problem. Based on a real-world dataset mined from a health-related web site we conclude that while CNNs perform best in terms of predicting demographic information by jointly learning different user attributes topic models provide additional information and reflect gender-specific and age-specific symptom profiles that may be of interest for a researcher.
266,Abdi2018a, Improving traffic safety is one of the important goals of intelligent transportation systems. Traffic signs play a very vital role in safe driving and in avoiding accidents by informing the driver about the speed limits or possible dangers such as icy roads imminent road works or pedestrian crossings. In-vehicle contextual Augmented reality (AR) has the potential to provide novel visual feedbacks to drivers for an enhanced driving experience. In this paper we propose a new AR traffic sign recognition system (AR-TSR) to improve driving safety and enhance the driver’s experience based on the Haar cascade and the bag-of-visual-words approach using spatial information to improve accuracy and an overview of studies related to the driver’s perception and the effectiveness of the AR in improving driving safety. In the first step the region of interest (ROI) is extracted using a scanning window with a Haar cascade detector and an AdaBoost classifier to reduce the computational region in the hypothesis-generation step. Second we proposed a new computationally efficient method to model global spatial distribution of visual words by taking into consideration the spatial relationships of its visual words. Finally a multiclass sign classifier takes the positive ROIs and assigns a 3D traffic sign for each one using a linear SVM. Experimental results show that the suggested method could reach comparable performance of the state-of-the-art approaches with less computational complexity and shorter training time and the AR-TSR more strongly impacts the allocation of visual attention during the decision-making phase.Graphical Abstract
267,Wu2018b, The adverse drug event (ADE) is an unexpected and harmful consequence of drug usege. Identifying the association between the use of drugs and adverse events from biomedical literature can contribute a lot to drug safety supervision. Such identification can not only assist drug safety monitoring but also correct known dependencies among events. In this paperwe propose a novel approach based on graph algorithm to detect adverse drug events(GA-ADE). In our approach we first construct a graph using candidate ADE extracted from biomedical literature. We then propose a method to select important vertices from the graph as core vertices and design a Personal Rank algorithm using the core vertices for clustering to build subgraphs. Lastly the correlation between the drug and the event is calculated based on the subgraphs. Experiments show that our approach is feasible.
268,Hammoudi2018, The emergence of Internet of Things (IoT) is empowered by the availability of the high volume of smart sensors Radio Frequency Identification a suitable communication technologies and protocols. In the near future the Internet will be full of heterogeneous connected devices. In recent years the IoT has drawn significant attention as it can solve difficult problems. However the heterogeneity of devices and the large scale networks expose the IoT to many challenges that must be addressed; otherwise the systems performance will deteriorate. As an attempt to identify these challenges this paper comprehensibly cites the main IoT concepts the serious IoT challenges and the quality of services presented in the recent literature. It also investigates the corresponding main research directions and the proposed solutions. This paper can increase the knowledge of the reader since it is the first IoT survey that presents load balancing algorithms utilized in solving the extreme data storage challenge.
269,Loukas2018, BackgroundIn addition to its therapeutic benefits minimally invasive surgery offers the potential for video recording of the operation. The videos may be archived and used later for reasons such as cognitive training skills assessment and workflow analysis. Methods from the major field of video content analysis and representation are increasingly applied in the surgical domain. In this paper we review recent developments and analyze future directions in the field of content-based video analysis of surgical operations.MethodsThe review was obtained from PubMed and Google Scholar search on combinations of the following keywords: ‘surgery’ ‘video’ ‘phase’ ‘task’ ‘skills’ ‘event’ ‘shot’ ‘analysis’ ‘retrieval’ ‘detection’ ‘classification’ and ‘recognition’. The collected articles were categorized and reviewed based on the technical goal sought type of surgery performed and structure of the operation.ResultsA total of 81 articles were included. The publication activity is constantly increasing; more than 50% of these articles were published in the last 3 years. Significant research has been performed for video task detection and retrieval in eye surgery. In endoscopic surgery the research activity is more diverse: gesture/task classification skills assessment tool type recognition shot/event detection and retrieval. Recent works employ deep neural networks for phase and tool recognition as well as shot detection.ConclusionsContent-based video analysis of surgical operations is a rapidly expanding field. Several future prospects for research exist including inter alia shot boundary detection keyframe extraction video summarization pattern discovery and video annotation. The development of publicly available benchmark datasets to evaluate and compare task-specific algorithms is essential.
270,Stoekle2018, The practice and development of modern medicine requires large amounts of data particularly in the domain of cancer. The future of personalized medicine lies neither with “genomic medicine” nor with “precision medicine” but with “data medicine” (DM) (big data data mining). The establishment of this DM has required far-reaching changes to establish four essential elements connecting patients and doctors: biobanks databases bioinformatic platforms and genomic platforms. The “transformation” of scientific research areas such as genetics bioinformatics and biostatistics into clinical specialties has generated a new vision of care. Molecular tumor boards (MTB) are one response to these changes and are now providing better access to next-generation sequencing (NGS) and new cancer treatments to patients with inoperable or metastatic cancers and those for whom the usual treatment has failed. However MTB face a crucial ethical challenge: maintaining and improving the trust of patients clinicians researchers and industry in academic medical centers supported by private or public funding rather than providing genetic data directly to private companies. We believe that in this era of DM appropriate modern digital communication networks will be required to maintain this trust and to improve the organization and effectiveness of the system. There is therefore a need to reconsider the form and content of informed consent (IC) documents at all academic medical centers and to introduce dynamic and electronic informed consent (e-IC).
271,Hu2018c, Recent real medical datasets show that the number of outpatients in China has sharply increased since 2013 when the Chinese health insurance reform started. This situation leads to increased waiting time for the outpatients; in particular the normal operation of a hospital will be congested at rush hour. The existence of this problem in outpatient departments causes a reduction in doctors’ diagnostic time and a high working strength is required to address this issue. In this paper a simultaneous model based on machine learning is proposed for aiding outpatient doctors in performing diagnoses. We use Support Vector Machine (SVM) and Neural Networks (NN) to classify hyperlipemia using the clinical features extracted from a real medical dataset. The results with an accuracy of 90 % indicate that our Simultaneously Aided Diagnosis Model (SADM) applied to aid diagnosis for outpatient doctors and achieves the objective of increasing efficiency and reducing working strength.
272,Perveen2018, Prevention and diagnosis of NAFLD is an ongoing area of interest in the healthcare community. Screening is complicated by the fact that the accuracy of noninvasive testing lacks specificity and sensitivity to make and stage the diagnosis. Currently no non-invasive ATP III criteria based prediction method is available to diagnose NAFLD risk. Firstly the objective of this research is to develop machine learning based method in order to identify individuals at an increased risk of developing NAFLD using risk factors of ATP III clinical criteria updated in 2005 for Metabolic Syndrome (MetS). Secondly to validate the relative ability of quantitative score defined by Italian Association for the Study of the Liver (IASF) and guideline explicitly defined for the Canadian population based on triglyceride thresholds to predict NAFLD risk. We proposed a Decision Tree based method to evaluate the risk of developing NAFLD and its progression in the Canadian population using Electronic Medical Records (EMRs) by exploring novel risk factors for NAFLD. Our results show proposed method could potentially help physicians make more informed choices about their management of patients with NAFLD. Employing the proposed application in ordinary medical checkup is expected to lessen healthcare expenditures compared with administering additional complicated test.
273,Zheng2018, Within a marine vehicle electric propulsion system three-phase voltage source PWM rectifiers instantaneously produce larger impulse current and overshoot of DC voltage during the startup and load transients and may lead to system instability; a fuzzy PI compound control scheme is proposed to replace the traditional PI controller in the voltage outer loop. In order to circumvent the defects of the traditional PI controller the voltage outer loop is designed by combining a fuzzy controller and a PI controller. Moreover fuzzy control rules are employed to online adapt parameters of the PI controller and thereby enhancing the system robustness and contributing to faster dynamic response and smaller overshoot. Simulation results demonstrate that the proposed control scheme can effectively suppress the impulse current and overshoot of DC voltage during the startup and load transients and improve the anti-disturbance ability of the rectifier and the operation reliability of the marine vehicle electric propulsion system.
274,Hu2018d, Like outdoors indoor security is also a critical problem and human action recognition in indoor area is still a hot topic. Most studies on human action recognition ignored the semantic information of a scene whereas indoors contains varieties of semantics. Meanwhile the depth sensor with color and depth data is more suitable for extracting the semantics context in human actions. Hence this paper proposed an indoor action recognition method using Kinect based on the semantics of a scene. First we proposed a trajectory clustering algorithm for a three-dimensional (3D) scene by combining the different characteristics of people such as the spatial location movement direction and speed. Based on the clustering results and scene context it concludes a region of interest (ROI) extraction method for indoors and dynamic time warping (DTW) is used to study the abnormal action sequences. Finally the color and depth-data-based 3D motion history image (3D MHI) features and the semantics context of the scene were combined to recognize human action. In the experiment two datasets were tested and the results demonstrate that our semantics-based method performs better than other methods.
276,Chen2018d, The process of current urban and accelerating the number of motor vehicles increased rapidly resulting in road traffic pressure is increasing we need to analyze large data traffic in the city to guide urban road planning and improve the level of city management and city operation rules found from traffic data in complex. However traffic data are characterized by large amount and high dimension which makes the analysis process difficult. In this paper the composition characteristics and application of large data in traffic field are introduced. Mining multi-source heterogeneous data traffic generated by the depth of the traffic data to establish a comprehensive analysis platform and project evaluation subsystem the formation of integrated traffic system model for multi field multi-level application requirements. In this paper we propose a visualization model based on self-organizing feature map neural networks with graph theory. This paper analyzes the traffic data of the whole life cycle combing the traffic data collection analysis discovery the level of application and uses big data techniques to guide the city traffic planning construction management operation and decision support.
277,Sun2018b, Anomaly detection in social media refers to the detection of users’ abnormal opinions sentiment patterns or special temporal aspects of such patterns. Social media platforms such as Sina Weibo or Twitter provide a Big-data platform for information retrieval which include user feedbacks opinions and information on most issues. This paper proposes a hybrid neural network model called Convolutional Neural Network-Long-Short Term Memory(CNN-LSTM) we successfully applies the model to sentiment analysis on a microblog Big-data platform and obtains significant improvements that enhance the generalization ability. Based on the sentiment of a single post in Weibo this study also adopted the multivariate Gaussian model and the power law distribution to analyze the users’ emotion and detect abnormal emotion on microblog the multivariate Gaussian method automatically captures the correlation between different features of the emotions and saves a certain amount of time through the batch calculation of the joint probability density of data sets. Through the measure of a joint probability density value and validation of the corpus from social network anomaly detection accuracy of an individual user is 83.49% and that for a different month is 87.84%. The results of the distribution test show that individual user’s neutral happy and sad emotions obey the normal distribution but the surprised and angry emotions do not. In addition the group-based emotions on microblogs obey the power law distribution but individual emotions do not.
278,Heravi2018, One of the most significant issues in machine learning is system identification with many applications e.g. channel estimation (CE) in digital communications. Introducing a new correntropy-based method this paper deals with the comparison between mean square error (MSE) and information theoretic measures in non-Gaussian noise channel estimation by analyzing the MSE minimum error entropy (MEE) and correntropy algorithms in several channel models utilizing neural networks. The first contribution of this paper is introducing a new correntropy-based conjugate gradient (CCG) method and applying it in the CE problem which this new algorithm converges faster than standard maximum correntropy criterion algorithm. Aiming at this contribution the better convergence rate is discussed analytically and it is proved that the CCG could converge to the optimal solution quadratically. Next the performance of an extended MSE algorithm is compared with information theoretic criteria; in addition a comparison between MEE and correntropy-based algorithm is presented. The Monte Carlo results illustrate that correntropy and MEE outperform MSE algorithm in low-SNR communications especially in the presence of impulsive noise. Then we apply the trained neural networks in the receiver as an equalizer to obtain the intended performance for different SNR values.
279,Kraus2018, Recent snapshots of the European progress on big data in health care and precision medicine reveal diverse perceptions of experts and the public leading to the impression that algorithmic issues have the largest share among the challenges all health systems are faced with. Yet from a comparison of different countries it is evident that the adaption and integration of heterogeneous data sources have a major impact on the advancement of precision medicine. Legal regulations for implementation and operation of healthcare networking are actively discussed in the public and gradually implemented in several countries. Based on a unified documentation they are a perfect precondition for integrating distributed healthcare data to a big data platform with a reliable fact representation. Now basic and clinical scientists have to be motivated to share their work with these data platforms. In this work we aim to provide an overview on the common issues in big healthcare data applications and address the challenges for the involved scientific clinical and administrative partners. We propose a possible strategy for a comprehensive data integration by iterating data harmonization semantic enrichment and data analysis processes.
280,Liu2018d, Under the circumstance of congestion the low-efficiency operation of security system is one of the major causes of frequent security incidents over the span of recent years. At the same time identifying the potential bottleneck which disrupts the audience is an effective way to explore new solutions of security issues and use mathematical models to develop the most efficient and effective security screening system. In this paper the security system of Luzhniki stadium in 2018 is instance model. The queuing theory is used to construct the model (MMS) while according to the Poisson distribution and network science to analyze bottlenecks models and results. Both practical and cultural factors are considered in the model optimization process. In this way the target loop checkpoint system is designed and the convolution neural network algorithm is added to the system to classify and dispose the captured images. The purpose is to improve security efficiency by increasing the flow of people in best use of space.
281,Wibmer2018, Hybrid imaging plays a central role in the diagnosis and management of a wide range of malignancies at all stages. In this article we review the most pertinent historical developments emerging clinical applications of novel radiotracers and imaging technologies and potential implications for training and practice. This includes an overview of novel tracers for prostate breast and neuroendocrine tumors assessment of tumor heterogeneity the concept of image-guided ‘biologically relevant dosing’ and theranostic applications. Recent technological advancements including time-of-flight PET PET/MRI and ‘one-minute whole-body PET’ are also covered. Finally we discuss how these rapidly evolving applications might affect current training curricula and how imaging-derived big data could be harnessed to the benefit of our patients.
282,Ramakrishna2018, Materials informatics employs techniques tools and theories drawn from the emerging fields of data science internet computer science and engineering and digital technologies to the materials science and engineering to accelerate materials products and manufacturing innovations. Manufacturing is transforming into shorter design cycles mass customization on-demand production and sustainable products. Additive manufacturing or 3D printing is a popular example of such a trend. However the success of this manufacturing transformation is critically dependent on the availability of suitable materials and of data on invertible processing structure property performance life cycle linkages of materials. Experience suggests that the material development cycle i.e. the time to develop and deploy new material generally exceeds the product design and development cycle. Hence there is a need to accelerate materials innovation in order to keep up with product and manufacturing innovations. This is a major challenge considering the hundreds of thousands of materials and processes and the huge amount of data on microstructure composition properties and functional environmental and economic performance of materials. Moreover the data sharing culture among the materials community is sparse. Materials informatics is key to the necessary transformation in product design and manufacturing. Through the association of material and information sciences the emerging field of materials informatics proposes to computationally mine and analyze large ensembles of experimental and modeling datasets efficiently and cost effectively and to deliver core materials knowledge in user-friendly ways to the designers of materials and products and to the manufacturers. This paper reviews the various developments in materials informatics and how it facilitates materials innovation by way of specific examples.
283,Kho2018, BackgroundWhile emergency airway management training is conventionally conducted via face-to-face learning (F2FL) workshops there are inherent cost time place and manpower limitations in running such workshops. Blended learning (BL) refers to the systematic integration of online and face-to-face learning aimed to facilitate complex thinking skills and flexible participation at a reduced financial time and manpower cost. This study was conducted to evaluate its effectiveness in emergency airway management training.MethodsA single-center prospective randomised controlled trial involving 30 doctors from Sarawak General Hospital Malaysia was conducted from September 2016 to February 2017 to compare the effectiveness of BL versus F2FL for emergency airway management training. Participants in the BL arm were given a period of 12 days to go through the online materials in a learning management system while those in the F2FL arm attended a-day of face-to-face lectures (8 h). Participants from both arms then attended a day of hands-on session consisting of simulation skills training with airway manikins. Pre- and post-tests in knowledge and practical skills were administered. E-learning experience and the perception towards BL among participants in the BL arm were also assessed.ResultsSignificant improvements in post-test scores as compared to pre-test scores were noted for participants in both BL and F2FL arms for knowledge practical and total scores. The degree of increment between the BL group and the F2FL arms for all categories were not significantly different (total scores: 35 marks inter-quartile range (IQR) 15.0   41.0 vs. 31 marks IQR 24.0   41.0 p ," 0.690; theory scores: 18 marks, IQR 9   24 vs. 19 marks, IQR 15   20, p "," 0.992; practical scores: 11 marks, IQR 5 -18 vs. 10 marks, IQR 9   20, p "," 0.461 respectively). The overall perception towards BL was positive.ConclusionsBlended learning is as effective as face-to-face learning for emergency airway management training of junior doctors, suggesting that blended learning may be a feasible alternative to face-to-face learning for such skill training in emergency departments.Trial registrationMalaysian National Medical Research NMRR-16-696-30190. Registered 28 April 2016.,"
284,Nguyen2018b, Antimicrobial resistant infections are a serious public health threat worldwide. Whole genome sequencing approaches to rapidly identify pathogens and predict antibiotic resistance phenotypes are becoming more feasible and may offer a way to reduce clinical test turnaround times compared to conventional culture-based methods and in turn improve patient outcomes. In this study we use whole genome sequence data from 1668 clinical isolates of Klebsiella pneumoniae to develop a XGBoost-based machine learning model that accurately predicts minimum inhibitory concentrations (MICs) for 20 antibiotics. The overall accuracy of the model within ±1 two-fold dilution factor is 92%. Individual accuracies are  90% for 15/20 antibiotics. We show that the MICs predicted by the model correlate with known antimicrobial resistance genes. Importantly the genome-wide approach described in this study offers a way to predict MICs for isolates without knowledge of the underlying gene content. This study shows that machine learning can be used to build a complete in silico MIC prediction panel for K. pneumoniae and provides a framework for building MIC prediction models for other pathogenic bacteria.
285,Okafor2018, In an effort to tackle fragmented care in the US healthcare delivery system we explored the use of learning collaborative (LC) to advance integration of behavioral health and primary care as one of the potential solutions to a holistic approach to the delivery of quality healthcare to individuals with physical and mental illness. How a diverse group of primary care and behavioral health providers formed a Community of Practice (CoP) with a common purpose and shared vision to advance integrated care using a LC approach is described. An account of their learning experience key components of their quality improvement practice changes clinical processes and improved outcomes are explained. This paper aims at describing the history creative design processes roles of the CoP and impact of the LC on the advancement of integrated care practice and quality improvements for further exploration and replications.
286,Chen2018e, This paper presents a motion-based superpixel-level background estimation algorithm that aims to be competitively accurate while requiring less computation time for background modelling and updating. Superpixels are chosen for their spatial and colour coherency and can be grouped together to better define the shapes of objects in an image. RGB mean and colour covariance matrices are used as the discriminative features for comparing superpixels to their background model samples. The background model initialisation and update procedures are inspired by existing approaches with the key aim of minimising computational complexity and therefore processing time. Experiments carried out with a widely used dataset show that SuperBE can achieve a high level of accuracy and is competitive against other state-of-the-art background estimation algorithms. The main contribution of this paper is the computationally efficient use of superpixels in background estimation while maintaining high accuracy reaching 135 fps on 320 × 240 resolution images.
287,Fang2018, Nowadays lecture-recording systems play a vital role in collecting spoken discourse for e-learning. However in view of the growing development of e-learning the lack of content is becoming a problem. This research presents a smart lecture-recording (SLR) system that can record orations at the same level of quality as a human team but with a reduced degree of human involvement. The proposed SLR system is composed of two subsystems referred to as virtual cameraman (VC) and virtual director (VD) respectively. All camera man components of VC subsystem are automatic and can take actions that include target and event detection tracking and view searching. The videos taken by these three components are forwarded to the VD subsystem in which the representative shot is chosen for recording or direct broadcasting. We refer to this function of the VD subsystem as shot selection that is based on the content analysis. The capability of shot selection is pre-trained through a machine-learning process characterized by the counter-propagation neural (CPN) network. However the CPN network yielded poor results when the input data were heterogeneous data. To increases the accuracy of shot selection we applied multiple kernel learning (MKL) techniques into CPN network called MK-CPN to transform all the heterogeneous data from different content analysis methods into unified space. A series of experiments for real lecture has been conducted. The results showed that the proposed SLR system can provide oration records close to some extend to those taken by real human teams. We believe that the proposed system may not be limited to live speeches if it can be configured with appropriate training materials.
288,Zhang2018f, Chinese text classification problem was studied based on domain ontology graph (DOG) of semi-supervised conceptual clustering to solve the problem that English word disambiguation method cannot be applied to Chinese text classification. Structure model of domain ontology graph text classification algorithm in HowNet dictionary and KLSeeker ontology and so on were used to realize accurate classification of Chinese text and display effectiveness of algorithm. Chinese text classification model in domain ontology graph based on conceptual clustering was developed from the angle of decreasing human participation in ontology construction as much as possible in the paper. Aimed at application domain of Chinese web text the algorithm can generate DOG of knowledge conceptualization automatically. At the same time document ontology graph (DocOG) was defined to represent contents of individual text document. DocOG extracting target realized text classification based on ontology by matching of single document ontology and domain ontology. Finally example calculation analysis and actual data test set experiment were given in experimental stage. The result shows that proposed Chinese text classification method has higher classification accuracy and reflects effectiveness of design.
289,Wang2018m, Bearing fault diagnosis under variable conditions has become a research hotspot recently. To solve this problem this paper presents a new classifier: multiclass self-adaptive support vector classifier (MSa-SVC). Firstly self-adaptive SVC is created by combination of SVC and information geometry. Then several binary Sa-SVCs are constructed as a multiclass classifier for fault diagnosis. The proposed MSa-SVC in conjunction with complementary ensemble empirical mode decomposition (CEEMD) and singular value decomposition (SVD) is utilized for bearing fault diagnosis: (1) each signal is processed into singular features by CEEMD SVD. (2) MSa-SVC is used for fault clustering under variable conditions. Finally the proposed method was applied on bearing fault diagnosis in practice. The results show that this method provides an efficient approach for bearing fault diagnosis under variable conditions.
290,Nalepa2018, Support vector machines (SVMs) are a supervised classifier successfully applied in a plethora of real-life applications. However they suffer from the important shortcomings of their high time and memory training complexities which depend on the training set size. This issue is especially challenging nowadays since the amount of data generated every second becomes tremendously large in many domains. This review provides an extensive survey on existing methods for selecting SVM training data from large datasets. We divide the state-of-the-art techniques into several categories. They help understand the underlying ideas behind these algorithms which may be useful in designing new methods to deal with this important problem. The review is complemented with the discussion on the future research pathways which can make SVMs easier to exploit in practice.
291,Song2018b, This paper presents CRSPF-SLAM a critical rays self-adaptive particle filtering occupancy grid based SLAM system that can operate efficiently with different kinds of odometer in real time in small and large indoor and outdoor environments for various platforms. Its basic idea is to eliminate the accumulated error of odometer through scan to map matching based on particle filtering. Through some improvements for the original particle filtering method the lidar system becomes more robust to conduct accurate localization and mapping. Specifically in our proposed method particle filter based on Monte-Carlo algorithm is designed to be out-of-step to the odometer; During the scan matching process the influence of some critical rays selected through a ray-selection algorithm is enhanced and that of the unreliable rays is weaken or removed; The current optimal match value is regarded as the feedback to reset the particle number and the filtering range; Once the optimal pose and scan are obtained the previous error scan stored in the map will be removed. It is also introduced in the paper that the method can work effectively with dead reckoning visual odometry and IMU respectively. And we have tried to use it on different types of platforms   an indoor service robot a self-driving car and an off-road vehicle. The experiments in a variety of challenging environments such as bumpy and characterless area are conducted and analyzed.
292,Lee2018c, This study aims to develop an autonomous algorithm to control the safety systems of nuclear power plant (NPP) by using the deep learning that is one of machine learning methods. The autonomous algorithm has two main goals. First it achieves a high level of automation for nine safety functions of NPP. Second the algorithm controls the nine safety functions in an integrated way. The function-based hierarchical framework is suggested to represent the multi-level structure that models NPP safety systems with the levels of goal function and system. The function-based hierarchical framework is used to model the NPP for the application of the multi-system deep learning network. Multi-system deep learning network is applied to develop the algorithm for autonomous control. This approach enables the systematic analysis of power plant system and development of the database for the deep learning network.
293,Du2018a, With the development of the Internet of Things more and more edge devices (such as smart-phones tablets wearable devices embedded devices gateway equipment and etc.) generate huge amounts of rich sensor data every day. With them some deep learning based recognition applications provide users with various recognition services on edge devices. However a fundamental problem these applications meet is how to perform deep learning algorithms effectively and promptly on a resource-constrained platform. Some researchers have proposed completing all computation tasks on the cloud side then returning the results back to edge devices but such procedure is always time-consuming because of data transmission. In this case training deep learning models on cloud side and executing the trained model directly on edge devices for inference is a better choice. Meanwhile the deep learning based mobile applications also need to satisfy the requirements of low latency low storage and low consumption. To fulfill above objectives we aim to propose a new deep learning compression algorithm. We conduct comprehensive experiments to compare the proposed light-weight model with other standard state-of-the-art compression algorithms in terms of inference accuracy process delay CPU load energy cost and storage coverage based on an audio recognition system.
294,Zheng2018a, This chapter offers a discussion on the relations between knowledge transfer in human deep learning and machine learning. A review of cognitive theories and models related to knowledge transfer in human deep learning was made in reference to the cognitive structure of surface and deep processes in learning. This is followed by a review of the characteristics of machine learning and their unique features in terms of supporting cognitive processes in knowledge transfer. Discussions on how knowledge in human cognitive processes may assist the design and implementation of machine learning are made. A framework was proposed to advance the practice of machine learning focusing on transfer of knowledge in human deep learning with respect to the relations between human cognitive processes and machine learning.
295,Vieira2018, Artificial neural networks are not new; they have been around for about 50 years and got some practical recognition after the mid-1980s with the introduction of a method (backpropagation) that allowed for the training of multiple-layer neural networks. However the true birth of deep learning may be traced to the year 2006 when Geoffrey Hinton [GR06] presented an algorithm to efficiently train deep neural networks in an unsupervised way in other words data without labels. They were called deep belief networks (DBNs) and consisted of staked restrictive Boltzmann machines (RBMs) with each one placed on the top of another. DBNs differ from previous networks since they are generative models capable of learning the statistical properties of data being presented without any supervision.
296,Aranjuelo2018, Multimodal deep learning is about learning features over multiple modalities. Impressive progress has been made in deep learning solutions that rely on a single sensor modality for advanced driving. However these approaches are limited to cover certain functionalities. The potential of multimodal sensor fusion has been very little exploited although research vehicles are commonly provided with various sensor types. How to combine their data to achieve a complex scene analysis and improve therefore robustness in driving is still an open question. While different surveys have been done for intelligent vehicles or deep learning to date no survey on multimodal deep learning for advanced driving exists. This paper attempts to narrow this gap by providing the first review that analyzes existing literature and two indispensable elements: sensors and datasets. We also provide our insights on future challenges and work to be done.
297,Leite2018, The architecture engineering construction and facility management (AECFM) industry has been experiencing many changes since inexpensive networked mobile computing devices have become ubiquitous. With the rising amount of information and data generated in the life cycle of capital projects information modeling and data interoperability have become a critical element in design engineering construction and maintenance of capital facilities. Recent advances in Visualization Information Modeling and Simulation (VIMS) have the potential to address a number of these pressing challenges. The objective of this concept paper is to discuss challenges and ongoing research in three areas of study in VIMS: capturing experiential knowledge in building information modeling (BIM)-based design coordination; 4-dimensional modeling for site-specific safety planning; and automating the BIM upkeep process in the facility operations phase leveraging deep learning and computer vision.
298,Jabeen2018, Ribonucleic acid sequencing (RNA-Seq) measures the expression levels of several transcripts simultaneously. The readings can be gene exon or other regions of interest. Various computational tools have been developed for studying pathogens or viruses from RNA-Seq data by classifying them according to the attributes in several pre-defined classes. However computational tools and approaches to analyzing complex datasets are still lacking. The development of classification models is highly recommended for the diagnosis and classification of diseases disease monitoring at the molecular level and research into potential disease biomarkers. In this chapter we discuss various machine learning approaches for RNA-Seq data classification and their implementation. These advancements in bioinformatics along with developments in machine learning-based classification would provide powerful toolboxes for the classification of transcriptome information available through RNA-Seq data.
299,Wang2018n, We are in the midst of the industry 4.0 or the fourth industry revolution a transformation revolving around intelligent sensors machines networks and business. Some of the newer concepts are overwhelming by their impact and transformational technologies are just the tip of the iceberg. Artificial Intelligence mainly Computational Intelligence will greatly affect future human’s life economics business industries and even political systems. In this paper we only discuss about the impact of AI to future predictive maintenance which is an important parts of future advanced production systems. Specially we focus on Deep Learning (DL) technology which is one branch of Artificial Neural Networks (ANN) and try to answer some questions on what DL is and why we are interested in applying DL in predictive maintenance strategy today.
300,Liang2018c, Failure of civil infrastructure such as bridges and pipelines can cause large public safety and economic consequences. Structural health monitoring (SHM) plays a significant role in preventing and mitigating the course of structural damage. In this work a multi-scale SHM framework based on Hadoop Ecosystem (MS-SHM-Hadoop) to monitor and evaluate the serviceability of civil infrastructure is proposed. Through utilizing fault-tolerant distributed file system called Hadoop Distributed File System (HDFS) and high-performance parallel data processing engine called MapReduce programming paradigm MS-SHM-Hadoop has high scalability and robustness in data ingestion fusion processing retrieval and analytics. MS-SHM-Hadoop is a multi-scale reliability analysis framework including nationwide civil infrastructure survey global structural integrity analysis and structural components’ reliability analysis. The nationwide civil infrastructure survey uses deep-learning techniques to evaluate their serviceability according to real-time sensory data or archived civil infrastructure related data such as traffic status weather conditions and civil infrastructure’s structural configuration. The global structural integrity analysis of a targeted civil infrastructure is made by processing and analyzing the measured vibration signals incurred by external loads such as wind and traffic flow. Component-wise reliability analysis is also enabled by deep learning technique where the input data is derived from the measured structural load effect hyper-spectral and 3D point cloud images and moisture measurement about structural component. As one of its major contributions this work employs Bayesian network to formulate the integral serviceability of a civil infrastructure according to components’ serviceability and inter-component correlations. Here the inter-component correlations are jointly specified using statistics-oriented machine learning method (e.g. association rule learning) or structural mechanics modeling and simulation.
301,Li2018h, With the ever-increasing demand in urban mobility and modern logistics sector the vehicle population has been steadily growing over the past several decades. One natural consequence of the vehicle population growth is the increase in traffic congestion. Almost all (metropolitan) cities including the major ones like Los Angeles Beijing New York are suffering from heavy traffic congestion. Statistics show that in 2015 43 cities in China are suffering a prolonged travel time of more than 1.5 h every day during rush hours. In the meanwhile traffic accidents are plaguing the economic development as well.
302,Chahar2018, In video surveillance person re-identification (re-id) is a popular technique to automatically finding whether a person has been already seen in a group of cameras. In the recent years availability of large-scale datasets the deep learning-based approaches have made significant improvement in the accuracy over the years as compared to hand-crafted approaches. In this paper we have distinguished the person re-id approaches into two categories i.e. image-based and video-based approaches; deep learning approaches are reviewed in both categories. This paper contains the brief survey of deep learning approaches on both image and video person re-id datasets. We have also presented the current ongoing works issues and future directions in large-scale datasets.
303,CoxJr.2018, Countless books and articles on data science and analytics discuss descriptive analytics predictive analytics and prescriptive analytics. An additional analytics area that is much less discussed links this world of analytics with its statistical model-based descriptions and predictions to the world of practical decisions in which actions have consequences that decision-makers and perhaps other stake-holders care about and about which they are often uncertain. This is the area of causal analytics. How causal analytics relates to other analytics areas and how its methods can be used to predict what to expect next explain past outcomes and observations prescribe what to do next to improve future outcomes and evaluate how well past or current policies accomplish their intended goals for whom and under what conditions are the main topics of this book.
304,Vieira2018a, Because of the recent achievements of deep learning [GBC16] benefiting from big data powerful computation and new algorithmic techniques you have been witnessing the renaissance of reinforcement learning especially the combination of reinforcement learning and deep neural networks such as deep reinforcement learning (deep RL). Deep Q-networks (DQNs) have ignited the field of deep RL [MKS+15] by allowing machines to achieve superhuman performance in Atari games and the very hard board game of Go.
305,Kiani2018, In this review we address to what extent computational techniques can augment our ability to predict toxicity. The first section provides a brief history of empirical observations on toxicity dating back to the dawn of Sumerian civilization. Interestingly the concept of dose emerged very early on leading up to the modern emphasis on kinetic properties which in turn encodes the insight that toxicity is not solely a property of a compound but instead depends on the interaction with the host organism. The next logical step is the current conception of evaluating drugs from a personalized medicine point of view. We review recent work on integrating what could be referred to as classical pharmacokinetic analysis with emerging systems biology approaches incorporating multiple omics data. These systems approaches employ advanced statistical analytical data processing complemented with machine learning techniques and use both pharmacokinetic and omics data. We find that such integrated approaches not only provide improved predictions of toxicity but also enable mechanistic interpretations of the molecular mechanisms underpinning toxicity and drug resistance. We conclude the chapter by discussing some of the main challenges such as how to balance the inherent tension between the predicitive capacity of models which in practice amounts to constraining the number of features in the models versus allowing for rich mechanistic interpretability i.e. equipping models with numerous molecular features. This challenge also requires patient-specific predictions on toxicity which in turn requires proper stratification of patients as regards how they respond with or without adverse toxic effects. In summary the transformation of the ancient concept of dose is currently successfully operationalized using rich integrative data encoded in patient-specific models.
306,Dreossi2018, Fueled by massive amounts of data models produced by machine-learning (ML) algorithms especially deep neural networks are being used in diverse domains where trustworthiness is a concern including automotive systems finance health care natural language processing and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS) such as self-driving cars and aviation where an adversary can cause serious consequences.However existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example in an autonomous vehicle using deep learning for perception not every adversarial example for the neural network might lead to a harmful consequence. Moreover one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines existing algorithms for constructing robust ML algorithms ignore the specification of the overall system. In this paper we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.
307,Wicker2018, Despite the improved accuracy of deep neural networks the discovery of adversarial examples has raised serious safety concerns. Most existing approaches for crafting adversarial examples necessitate some knowledge (architecture parameters etc) of the network at hand. In this paper we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge. Our algorithm employs object detection techniques such as SIFT (Scale Invariant Feature Transform) to extract features from an image. These features are converted into a mutable saliency distribution where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system. We formulate the crafting of adversarial examples as a two-player turn-based stochastic game where the first player’s objective is to minimise the distance to an adversarial example by manipulating the features and the second player can be cooperative adversarial or random. We show that theoretically the two-player game can converge to the optimal strategy and that the optimal strategy represents a globally minimal adversarial image. For Lipschitz networks we also identify conditions that provide safety guarantees that no adversarial examples exist. Using Monte Carlo tree search we gradually explore the game state space to search for adversarial examples. Our experiments show that despite the black-box setting manipulations guided by a perception-based saliency distribution are competitive with state-of-the-art methods that rely on white-box saliency matrices or sophisticated optimization procedures. Finally we show how our method can be used to evaluate robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.
308,Higano2018, In 2006 I launchedLeadership Education an undergraduate leadership programActive learning the very first one in Japan that included mandatory courses (Higano 2013). Although I was not familiar with the concept of active learningActive learning until around 2010 in retrospect what I had intended and what I actually implemented were both truly active learningActive learning experiences for students. In this chapter I would like to discuss the following propositions: (a) that the new leadership educationLeadership education is a good example of active learningActive learning (b) that almost all active learningActive learning and leadership education are in fact homologous (c) that (deep) active learningActive learning can be redefined from the perspective of students’ leadership (d) that learning being “deep” means learning achievements can be utilized anytime anywhere without the support of the teacher (“training wheels”)Training wheels and (e) that new leadership education theory can be a powerful tool for (deep) active learningActive learning theory.
309,Bahi2018, Drug repositioning or repurposing refers to identifying new indications for existing drugs and clinical candidates. Predicting new drug-target interactions (DTIs) is of great challenge in drug repositioning. This tricky task depends on two aspects. The volume of data available on drugs and proteins is growing in an exponential manner. The known interacting drug-target pairs are very scarce. Besides it is hard to select the negative samples because there are not experimentally verified negative drug-target interactions. Many computational methods have been proposed to address these problems. However they suffer from the high rate of false positive predictions leading to biologically interpretable errors. To cope with these limitations we propose in this paper an efficient computational method based on deep semi-supervised learning (DeepSS-DTIs) which is a combination of a stacked autoencoders and a supervised deep neural network. The objective of this approach is to predict potential drug targets and new drug indications by using a large scale chemogenomics data while improving the performance of DTIs prediction. Experimental results have shown that our approach outperforms state-of-the-art techniques. Indeed the proposed method has been compared to five machine learning algorithms applied all on the same reference datasets of DrugBank. The overall accuracy performance is more than 98%. In addition the DeepSS-DTIs has been able to predict new DTIs between approved drugs and targets. The highly ranked candidate DTIs obtained from DeepSS-DTIs are also verified in the DrugBank database and in literature.
310,Abeyratne2018, An air freight service is performed either by air carriers providing a multi-service or by all-cargo carriers and became popular in the post world war era in the 1940s and 1950s. Here the distinction between “freight” and “cargo” become relevant. Technically there is no difference and the terms could be used interchangeably. However usually “freight” is used for property transported by aircraft dedicated exclusively to property. “Cargo” is used for property transported in an aircraft which transports both passengers (their baggage) and other property. Air cargo effectively connects markets distant from each other creating global supply chains with speed and efficiency. This makes businesses deal easily with inventory management and built-to-order production. The speed and efficiency inherent in air transport that makes transport of cargo by air more efficient is dependant on various factors including market access and liberalization as well as fair competition rules (which is discussed in the chapter to follow).
311,Blecic2018, We present a method for automatic assessment of perceived walkability by pedestrans using a machine learning technique with deep convolutional neural networks (CNNs) trained on a dataset of georeferenced street-level images obtained from Google Street View. On a dataset of more than 17000 human-assessed images used for training validation and testing of CNN out method yields an accuracy of 78% of correct and 99% of correct or 1-class-off predictions. These are quite promising even encouraging results paving the way for seamless large-scale applications of perceived walkability assessment on large metropolitan areas and for a mass assessment and comparisons of walkability over many cities across regions.
312,Wang2018o, In human-robot collaborativeHuman-Robot Collaborative (HRC) manufacturing industrial robots would work alongside the human workers who jointly perform the assigned tasks. Recent research work revealed that recognised human motions could be used as input for industrial robots control. However the human-robot collaboration team still cannot work symbiotically. In response to the requirement this chapter explores the potential of establishing context awareness between a human worker and an industrial robot for human-robot collaborative assemblyAssembly. The context awarenessAwareness between the human worker and the industrial robot is established by applying gesture recognitionGesture recognition human motion recognition and Augmented Reality (AR) based worker instruction technologies. Such a system works in a cyber-physical environment and is demonstrated by case studies.
313,Gu2018, This paper presents a solution for an integrated object-centric event recognition problem for intelligent traffic supervision. We propose a novel event-recognition framework using deep local flow in a fast region-based convolutional neural network (R-CNN). First we use a fine-tuned fast R-CNN to accurately extract multi-scale targets in the open environment. Each detected object corresponds to an event candidate. Second a deep belief propagation method is proposed for the calculation of local fast R-CNN flow (LFRCF) between local convolutional feature matrices of two non-adjacent frames in a sequence. Third by using the LFRCF features we can easily identify the moving pattern of each extracted object and formulate a conclusive description of each event candidate. The contribution of this paper is to propose an optimized framework for accurate event recognition. We verify the accuracy of multi-scale object detection and behavior recognition in extensive experiments on real complex road-intersection surveillance videos.
314,PresaReyes2018, To raise awareness in disaster situations the quality and analysis of disaster-related big data are essential. Recent developments in the collection analysis and visualization of multimedia data have led to a significant enhancement in disaster management systems. Crowdsourcing tools for instance allow citizens to perform an active role in reporting information relevant to disaster events at a global scale through popular social media sites such as Twitter and Facebook. As multimedia data analysis becomes further advanced it can augment the disaster situation awareness and provide an efficient and timely response. This paper describes how multimedia data management plays a prominent role in improving the capabilities to readily manage disaster situations. Specifically visualization provides a more convenient and user-friendly means for individuals who have limited experience in disaster situations. A case study introducing a 3D animation system is presented which simulates the impacts of storm surge near coastal areas.
315,Curiel-Lewandrowski2018, The purpose of this chapter is to present a broad spectrum of approaches for melanoma detection. From the perspective of morbidity mortality quality of life and cost there is a compelling need to improve the early detection of melanoma. Fortunately because melanoma is usually visible on the surface of the skin it is well suited for the application of early detection approaches and noninvasive technologies. Here we systematically review several state-of-the-art methods from screening processes of populations and individuals to screening of suspicious lesions with novel technologies including advanced imaging modalities machine learning and consumer-driven technologies such as mobile devices and applications as well as the use of molecular assays for both diagnosis and prognosis. Technologies and techniques that capitalize on the cutaneous location of melanoma are rapidly evolving to reach the right populations to identify the individuals in greatest need of screening and to apply the right approach at the right time to identify melanoma before it reaches metastatic competency. A combination of approaches that bridge medical science responsible adoption of technology public health challenges and behavior modification will be necessary in order for progress in melanoma early detection and resultant minimization of melanoma morbidity and mortality to continue.
316,Ng2018, This chapter provides a brief review of current research models on learning engagement highlighting the distinction between indicators and facilitators of engagement. In relation to engagement indicators this chapter discusses research that has examined behavioral cognitive and affective dimensions of engagement. Alongside these indicators we add social engagement wherein students collaborate and work with others as an important indicator of engagement. In relation to facilitators of engagement this chapter briefly discusses the importance of a list of cognitive enablers for promoting learning engagement. These cognitive enablers include achievement goals self-efficacy self-determination self-regulation and personal interest. It also discusses the importance of social influences derived from sociocultural institutional and classroom dimensions on sustaining learning engagement. Linking to the discussion in Chap. 1 we argue that many students from disadvantaged backgrounds lack these cognitive enablers and specific attention is needed to create learning opportunities to engage these students in meaningful participation. To do this we argue that there is a need to consider the dynamic interplay of cognitive and social influences disadvantaged students’ perspectives and the negotiated nature of engagement.
317,Kondo2018, Floating and non-floating objects such as other ships buoys and so on must be alarmed before becoming obstacles for ship navigations. In this research we have aimed to predict obstacles around a ship from maritime navigation images using an image recognition method and display them effectively to its operator. Faster R-CNN was used as detection method. We prepared a dataset composed of three categories for training and testing machine learning. We enumerated parameter values to obtain the best detection rate of obstacles by CNN. Then we employed the best set of parameters for further experiments. The results are summarized as follows: (1) the detection rate of buoys is about 55 [%]; (2) large ships are sometimes mistaken for small boats. It remains to improve the detection rate and to decrease misclassifications; (3) the detection rate of small boats with distance of about 3 nautical mile(nm) from the ship is 86 [%] the detection rate of buoys with distance of about 2 [nm] from the ship is 100 [%].
318,Cios2018, In this chapter we describe Deep Neural Networks (DNN) their history and some related work.
319,Skilton2018, The most important thing is you can’t talk about artificial intelligence without talking about intelligence. One of the big mistakes people make is that they are making AI into specific things like deep learning or Arnold Schwarzenegger terminator movie but the point is that any word tends to be used in lots of different ways when defining definitions.
320,Gomez-Garay2018, The inclusion of visually impaired people to daily life is a challenging and active area of research. This work studies how to bring information about the surroundings to people delivered as verbal descriptions in Spanish using wearable devices. We use a neural network (DenseCap) for both identifying objects and generating phrases about them. DenseCap is running on a server to describe an image fed from a smartphone application and its output is the text which a smartphone verbalizes. Our implementation achieves a mean Average Precision (mAP) of 5.0 in object recognition and quality of captions and takes an average of 7.5 s from the moment one grabs a picture until one receives the verbalization in Spanish.
321,Marchegiani2018, This paper is about robot ego-motion estimation relying solely on acoustic sensing. By equipping a robot with microphones we investigate the possibility of employing the noise generated by the motors and actuators of the vehicle to estimate its motion. Audio-based odometry is not affected by the scene’s appearance lighting conditions and structure. This makes sound a compelling auxiliary source of information for ego-motion modelling in environments where more traditional methods such as those based on visual or laser odometry are particularly challenged. By leveraging multi-task learning and deep architectures we provide a regression framework able to estimate the linear and the angular velocity at which the robot has been travelling. Our experimental evaluation conducted on approximately two hours of data collected with an unmanned outdoor field robot demonstrated an absolute error lower than 0.07 m/s and 0.02 rad/s for the linear and angular velocity respectively. When compared to a baseline approach making use of single-task learning scheme our system shows an improvement of up to 26% in the ego-motion estimation.
322,CoxJr.2018a, It is an important truism that association is not causation. For example people living in low-income areas may have higher levels of exposure to an environmental hazard and also higher levels of some adverse health effect than people living in wealthier areas. Yet this observed association no matter how strong consistent statistically significant biologically plausible and well documented by multiple independent teams does not necessarily tell a policy maker anything about whether or by how much a proposed costly reduction in exposure would reduce adverse health effects. Perhaps only increasing income or something that income can buy would reduce adverse health effects. Or maybe factors that cannot be changed by policy interventions increase both the probability of living in low-income areas and the probability of adverse health effects. Whatever the truth is about opportunities to improve health by changing policy variables it typically cannot be determined by studying correlations regression coefficients relative risks or other measures of association between exposures and health effects (Pearl 2009). Observed associations between variables can contain both causal and non-causal (“spurious”) components. In general the effects of policy changes on outcomes of interest can only be predicted and evaluated correctly by modeling the network of causal relationships by which effects of exogenous changes propagate among variables. The chapter reviews current causal concepts principles and algorithms for carrying out such causal modeling and compares them to other approaches.
323,Bennaceur2018, Machine Learning (ML) is the discipline that studies methods for automatically inferring models from data. Machine learning has been successfully applied in many areas of software engineering including: behaviour extraction testing and bug fixing. Many more applications are yet to be defined. Therefore a better fundamental understanding of ML methods their assumptions and guarantees can help to identify and adopt appropriate ML technology for new applications.In this chapter we present an introductory survey of ML applications in software engineering classified in terms of the models they produce and the learning methods they use. We argue that the optimal choice of an ML method for a particular application should be guided by the type of models one seeks to infer. We describe some important principles of ML give an overview of some key methods and present examples of areas of software engineering benefiting from ML. We also discuss the open challenges for reaching the full potential of ML for software engineering and how ML can benefit from software engineering methods.
324,Dutta2018, Given a neural network (NN) and a set of possible inputs to the network described by polyhedral constraints we aim to compute a safe over-approximation of the set of possible output values. This operation is a fundamental primitive enabling the formal analysis of neural networks that are extensively used in a variety of machine learning tasks such as perception and control of autonomous systems. Increasingly they are deployed in high-assurance applications leading to a compelling use case for formal verification approaches. In this paper we present an efficient range estimation algorithm that iterates between an expensive global combinatorial search using mixed-integer linear programming problems and a relatively inexpensive local optimization that repeatedly seeks a local optimum of the function represented by the NN. We implement our approach and compare it with Reluplex a recently proposed solver for deep neural networks. We demonstrate applications of our approach to computing flowpipes for neural network-based feedback controllers. We show that the use of local search in conjunction with mixed-integer linear programming solvers effectively reduces the combinatorial search over possible combinations of active neurons in the network by pruning away suboptimal nodes.
325,Davis2018a, This chapter reviews ICT and education policy in the United States and Canada; both countries have populations with diverse cultures languages socioeconomics and ideologies. Generally local school authorities provide education while state/territory and national bodies exert influence. Focus areas in this chapter include ICT infrastructure protections for student data and privacy data interoperability personalized learning online assessment and updates to national technology standards and frameworks. In addition to evolving infrastructure issues related to supporting “future-ready” learning perhaps of equal importance is the ongoing evidence-based evaluation of educational technology and effort to increase the pace and rigor of evidence associated with educational technology purchases and renewals by school districts. Some school systems are moving toward a competency-based education (CBE) model which often requires policy shifts. There is growing interest in exploring new models for teacher preparation and development as well as calls for increased personalization of student learning. Policy debates continue around the degree of equal access to technology that government should require including whether parity should be required in terms of device age and availability quality and availability of digital learning materials and classroom bandwidth and whether the technology is being used actively or passively by students across all socioeconomic groups and across all schools. As policy makers grapple with the displacement of jobs and loss of employment due to advances in technology and continued growth in income inequality they may turn to policies that encourage better alignment between K-12 education and the skills needed for future careers.
326,Lu2018, Traffic Sign Recognition (TSR) is very important for driverless systems and driver assistance systems. Due to the small size of traffic signs in the wild the traffic sign becomes very challenging. In this paper an inception convolutional neural network is designed to solve the traffic sign classification problem. A large receptive field is generated by multiple small filters instead of a single large filter. Moreover Inspired by Inception V3 inception block is used which makes the combination of multiple convolution output be optimized. Thus the coarse cue in the shallow layer and the fine cue in the deeper layer are fused to improve the visual expression capability of the model. The proposed method is evaluated on three famous traffic sign datasets: the German Traffic Sign Recognition Benchmark (GTSRB) the Swedish Traffic Signs Dataset (STSD) and the 2015 Traffic Sign Recognition Competition Dataset. The experimental results demonstrate the effectiveness and robustness of our methods.
327,Kim2018c, The study is dedicated to solving the target issues of the ground traffic monitoring aided by the Unmanned Aerial Vehicles (UAV) based on applying the on-board computer vision systems. The classification of the road situations using images obtained after Traffic Accident (TA) is based on the feature set facts and attributes specified directly and/or indirectly on a possible situation class. The hierarchical structure of description of a road situation observable after the TA event is developed. For decision making the production model of knowledge representation and corresponding Knowledge Base (KB) is offered to use. The issues related to decision making for recognition of the occurring traffic situations have been considered. The analysis of the strategies have been carried out based on the principles of minimizing the overall losses limiting the admissible UAV flight altitude and ensuring the required class recognition reliability. The models describing the functional criteria of the losses flight safety of the UAV and reliability of class recognition have been proposed. It has been shown that applying the minimum loss criterion ensures considerable savings of resources under different ratio of the loss quotients. The example for classification of a road incident using the real images is given.
328,Madni2018," The word ""autonomous"" means having the ability for self-governance and independent operation. Autonomous vehicles (AVs) are systems that are required to exhibit requisite performance for extended durations with the desired level of reliability and safety under significant uncertainties in the environment while compensating for system failures without external intervention. AVs today are network-enabled. Soon they will be able to communicate with other vehicles and structures in the immediate vicinity (for collision avoidance) and distant vehicles and structures (for congestion management). Most AV concepts employ a network connection to the cloud other vehicles and built-up structures. Most vehicle concepts today do not make autonomous decisions with respect to destination and route selection. This chapter briefly reviews AV trends and presents a system-of-systems (SoS) perspective for connected AVs. It then discusses the high-reliability imperative for AVs and presents a model-based approach for AV SoS engineering. It stresses the importance of formal modeling of AV-SoS in light of the need to formally verify system models and test system (model) behavior while assuring requisite flexibility to respond to failures and disruptions. It concludes with a discussion of lingering misconceptions the issue of liability management and outlook for the future."
329,Zepke2018, The needs of learners in technical and vocational education and training (TVET) have been widely researched and documented. The same is true of ways to support such learning needs. This chapter addresses one specific question: what support do TVET learners need to enable them acquire behaviors knowledge and attitudes to succeed in TVET employment and life  Success appears in many guises. It can mean achieving officially desired outcomes such as retention completion and employment. It can also mean achieving less measurable outcomes such as deep learning well-being and active citizenship. The chapter first introduces an overarching success framework before exploring how the widely used student engagement pedagogy can support learners to achieve both official and personal success outcomes. It then develops two specific constructs applicable to TVET and found in success frameworks and student engagement: facilitated peer learning and active citizenship. Peer learning is here connected to teacher-facilitated but peer-run mentoring; active citizenship to educational experiences in classrooms institutions and workplaces that support flexibility resilience openness to change and diversity. Finally practical applications of support strategies from one form of peer learning are provided.
330,Hevener2018, The discovery of molecular toxicity in a clinical drug candidate can have a significant impact on both the cost and timeline of the drug discovery process. Early identification of potentially toxic compounds during screening library preparation or alternatively during the hit validation process is critical to ensure that valuable time and resources are not spent pursuing compounds that may possess a high propensity for human toxicity. This chapter focuses on the application of computational molecular filters applied either prescreening or postscreening to identify and remove known reactive and/or potentially toxic compounds from consideration in drug discovery campaigns.
331,Garcia-Garcia2018, We propose a non-invasive method to detect sleep deprivation by evaluating a short video sequence of a subject. Computer Vision techniques are used to crop the face from every frame and classify it (within a Deep Learning framework) into two classes: “rested” or “sleep deprived”. The system has been trained on a database of subjects recorded under severe sleep deprivation conditions. A prototype has been implemented in a low-cost Android device proving its viability for real-time driver monitoring applications. Tests on real world data have been carried out and show encouraging performances but also reveal the need of larger datasets for training.
332,Kim2018d, The AA manufacturing factory is a form of mass-producing and selling products in order to respond to customer’s needs. This means an excessive amount of material supply and demand for companies to reduce losses associated with short inventory. This results in products that fail to respond to demand accumulating in managed warehouses resulting in higher inventory maintenance costs. In this paper as a measure to reduce costs and inventory shortage to complement these problems we propose a plan that predicts future demand. In order to solve the problem ARIMA model which is a time series analysis technique is used to predict demand in the temporal variability or seasonal factor and to develop a demand-forecasting model based on the EOQ model. We also ran simulation to evaluate the effectiveness of the model and in future research we will apply it to small and medium enterprises to demonstrate the effectiveness of the model.
333,Huang2018e, Automated detection tools can enable both the study of online harassment and technology that mitigates its harm. Machine learning methods allow these tools to adapt and improve using data. Yet current well-established machine learning approaches require amounts of data that are often unmanageable by practitioners aiming to train harassment detectors. Emerging methods that learn models from weak supervision represent one important avenue to address this challenge. In contrast to the full supervision used in most traditional machine learning methods weak supervision does not require annotators to label individual examples of the target concept. Instead annotators provide approximate descriptions of the target concept sucsh as rule-of-thumb indicators. In this chapter we describe the weak supervision paradigm and some general principles that drive emerging methods. And we detail a weakly supervised method for detection of online harassment that uses key-phrase indicators as the form of weak supervision. This method considers multiple aspects of the online harassment phenomenon using interplay between these aspects to bolster the weak supervision into a useful model. We describe experimental results demonstrating this approach on detecting harassment in social media data. Finally we discuss the ongoing challenges for using machine learning methods to build harassment detectors.
334,Zin2018, In these days the population of elderly people grows faster and faster and most of them are rather preferred independent living at their homes. Thus a new and better approaches are necessary for improving the life quality of the elderly with the help of modern technology. In this chapter we shall propose a video based monitoring system to analyze the daily activities of elderly people with independent living at their homes. This approach combines data provided by the video cameras with data provided by the multiple environmental data based on the type of activity. Only normal activity or behavior data are used to train the stochastic model. Then decisions are made based on the variations from the model results to detect the abnormal behaviors. Some experimental results are shown to confirm the validity of proposed method in this paper.
335,Nguyen2018c, This paper presents a framework for robust lane detection towards automated driving using multiple sensors. Since every single source (e.g. camera digital map etc.) can fail in certain situations several independent sources need to be combined. Moreover the reliability of each source strongly depends on environmental conditions e.g. existence or visibility of lane markings. Thus we introduce a concept of estimating and incorporating reliability into the fusion. First a new sensor-independent error metric is applied to assess the quality of the estimated ego-lanes based on the angle deviation. Secondly we deploy a boosting algorithm to select the highly discriminant features among the extracted information. Based on the selected features we apply different classifiers to learn the reliabilities of the sources. Thirdly we use Dempster-Shafer evidence theory to stabilize the estimated reliabilities over time. Using a big collection of real data recordings from different situations the experimental results support our concept.
336,Xu2018c, In this paper we propose a user-friendly system that can create a facial image from a corresponding image in the user’s mind. Unlike most of the existing methods which require a sketch as input or the tedious work of selecting similar facial components from an example database our method can synthesise a satisfying result without questioning the user on the explicit features of the face in his or her mind. Through a dialogic approach based on a relevance feedback strategy to translate facial features into input the user only needs to look at several candidate face images and judge whether each image resembles the face that he or she is imagining. A set of sample face images that are based on users’ feedbacks are used to dynamically train an Optimum-Path Forest algorithm to classify the relevance of face images. Based on the trained Optimum-Path Forest classifier candidate face images that best reflect the user’s feedback are retrieved and interpolated to synthesise new face images that are similar to those the user had imagined. The experimental results show that the proposed technique succeeded in generating images resembling a face a user had imagined or memorised.
337,Wu2018c, Rehabilitation exoskeleton system is mainly developed for patients who suffered from hemiplegia and other sequelae caused by stroke. The system can assist or finally replace doctors to provide continuous and effective rehabilitation treatment. The paper presents an upper-limb rehabilitation exoskeleton system with 7 degrees of freedom using the Bowden cable actuation system. And an introduction about the mechanical structure is given. The kinematics and accessible workspace of exoskeleton is analyzed via Denavit-Hartenburg (D-H) approach and Monte Carlo method. Kane method is used to analyze the dynamic character of the robot. An admittance control algorithm is proposed to provide patient-active rehabilitation training in virtual environment. A preliminary comparison experiment is implemented to verify the effectiveness of the developed system and control strategy.
338,Bender2018, Up to the present day GPS signals are the key component in almost all outdoor navigation tasks of robotic platforms. To obtain the platform pose comprising the position as well as the orientation and receive information at a higher frequency the GPS signals are commonly used in a GPS-corrected inertial navigation system (INS). However the GPS is a critical single point of failure for unmanned aircraft systems (UAS). We propose an approach which creates a metric map of the overflown area by fusing camera images with inertial and GPS data during normal UAS operation and use this map to steer the system efficiently to its home position in the case of an GPS outage. A naive approach would follow the previously traveled path and get accurate pose estimates by comparing the current camera image with the previously created map. The presented procedure allows the usage of shortcuts through unexplored areas to minimize the travel distance. Thereby we ensure to reach the starting point by taking into consideration the maximal positional drift while performing pure visual navigation in unknown areas. We achieved close to optimal results in intensive numerical studies and demonstrate the usage of the algorithm in a realistic simulation environment and the real-world.
339,DeAngelis2018, Transformative learning describes the change process that an individual undergoes as they come to question their values and beliefs in such a way that they experience a fundamental shift in their interpretation of experiences and bases for their actions. Mezirow (1981 A critical theory of adult learning and education. Adult Educ Q 32:3 24) coined this term transformative learning more than a quarter century ago and laid the foundation for subsequent development of this concept. Concurrently thinkers such as Parker Palmer (1998 The courage to teach: exploring the inner landscape of a teacher’s life. Jossey-Bass San Francisco) have begun to articulate a notion of transformative teaching. Central to this concept is the idea that the educator approaches their teaching from a holistic and integrated sense of self. The question that remains is what is the impact of a transformational teacher on the learner  In this chapter I summarize these literature streams and use these insights to consider how educators are able to foster learning environments and practices that encourage transformative learning.
340,Ramirez2018, We consider the problem of inferring the operational state of a reactor facility by using measurements from a radiation sensor network which is deployed around the facility’s ventilation stack. The radiation emissions from the stack decay with distance and the corresponding measurements are inherently random with parameters determined by radiation intensity levels at the sensor locations. We fuse measurements from network sensors to estimate the intensity at the stack and use this estimate in a one-sided Sequential Probability Ratio Test (SPRT) to infer the on/off state of the reactor facility. We demonstrate the superior performance of this method over conventional majority vote fusers and individual sensors using (i) test measurements from a network of NaI sensors and (ii) emulated measurements using radioactive effluents collected at a reactor facility stack. We analytically quantify the performance improvements of individual sensors and their networks with adaptive thresholds over those with fixed ones by using the packing number of the radiation intensity space.
341,Wang2018p, In recent years the study of Unmanned Aerial Vehicle (UAV) autonomous landing has been a hot research topic. Aiming at UAV’s landmark localization the computer vision algorithms have excellent performance. In the computer vision research field the deep learning methods are widely employed in object detection and localization. However these methods rely heavily on the size and quality of the training datasets. In this paper we propose to exploit the Landmark-Localization Network (LLNet) to solve the UAV landmark localization problem in terms of a deep reinforcement learning strategy with small-sized training datasets. The LLNet learns how to transform the bounding box into the correct position through a sequence of actions. To train a robust landmark localization model we combine the policy gradient method in deep reinforcement learning algorithm and the supervised learning algorithm together in the training stage. The experimental results show that the LLNet is able to locate the landmark precisely.
342,Korzh2018, Nowadays image classification is a core task for many high impact applications such as object recognition self-driving cars national security (border monitoring assault detection) safety (fire detection distracted driving) geo-monitoring (cloud rock and crop-disease detection). Convolutional Neural Networks(CNNs) are effective for those applications. However they need to be trained with a huge number of examples and a consequently huge training time. Unfortunately when the training set is not big enough and when re-train the model several times is needed a common approach is to adopt a transfer learning procedure. Transfer learning procedures use networks already pretrained in other context and extract features from them or retrain them with a small dataset related to the specific application (fine-tuning). We propose to fine-tuning an ensemble of models combined together from multiple pretrained CNNs (AlexNet VGG19 and GoogleNet). We test our approach on three different benchmark datasets: Yahoo! Shopping Shoe Image Content UC Merced Land Use Dataset and Caltech-UCSD Birds-200-2011 Dataset. Each one represents a different application. Our suggested approach always improves accuracy over the state of the art solutions and accuracy obtained by the returning of a single CNN. In the best case we moved from accuracy of 70.5% to 93.14%.
343,Turban2018, 1.Understand the reasons for intelligent e-commerce systems2.Become familiar with the essentials of artificial intelligence3.Cite the major AI applications in e-commerce4.Understand knowledge systems and their management5.Understand intelligent computerized personal assistants and their availability6.Gain knowledge about IoT7.Describe self-driving cars smart homes and appliances and smart cities
344,Waseem2018, Accurately detecting hate speech using supervised classification is dependent on data that is annotated by humans. Attaining high agreement amongst annotators though is difficult due to the subjective nature of the task and different cultural geographic and social backgrounds of the annotators. Furthermore existing datasets capture only single types of hate speech such as sexism or racism; or single demographics such as people living in the United States which negatively affects the recall when classifying data that are not captured in the training examples. End users of websites where hate speech may occur are exposed to risk of being exposed to explicit content due to the shortcomings in the training of automatic hate speech detection systems where unseen forms of hate speech or hate speech towards unseen groups are not captured. In this paper we investigate methods for bridging differences in annotation and data collection of abusive language tweets such as different annotation schemes labels or geographic and cultural influences from data sampling. We consider three distinct sets of annotations namely the annotations provided by Waseem (2016) Waseem and Hovy (2016) and Davidson et al. (2017). Specifically we train a machine learning model using a multi-task learning (MTL) framework where typically some auxiliary task is learned alongside a main task in order to gain better performance on the latter. Our approach distinguishes itself from most previous work in that we aim to train a model that is robust across data originating from different distributions and labeled under differing annotation guidelines and that we understand these different datasets as different learning objectives in the way that classical work in multi-task learning does with different tasks. Here we experiment with using fine-grained tags for annotation. Aided by the predictions in our models as well as the baseline models we seek to show that it is possible to utilize distinct domains for classification as well as showing how cultural contexts influence classifier performance as the datasets we use are collected either exclusively from the U.S. Davidson et al. (2017) or collected globally with no geographic restriction (Waseem 2016; Waseem and Hovy 2016). Our choice for a multi-task learning set-up is motivated by a number of factors. Most importantly MTL allows us to share knowledge between two or more objectives such that we can leverage information encoded in one dataset to better fit another. As shown by Bingel and S gaard (2017) and Mart nez Alonso and Plank (2017) this is particularly promising when the auxiliary task has a more coarse-grained set of labels in comparison to the main task. Another benefit of MTL is that it lets us learn lower-level representations from greater amounts of data when compared to a single-task setup. This in connection with MTL being known to work as a regularizer is not only promising when it comes to fitting the training data but also helps to prevent overfitting especially when we have to deal with small datasets.
345,Aziz2018, The robust recognition of objects is an essential element of many maritime video surveillance systems. This paper builds on recent advances in convolutional neural networks (CNN) and proposes a new visible-infrared spectrum architecture for ship recognition. Our architecture is composed of two separate CNN processing streams which will be consecutively combined with a merge network. This merge allows the classification to be performed and provide a rich semantic information such as appearance. It also allows to remedy some problems related to the quality of the visible images due to the weather conditions (rain fog etc.) and very complex maritime environment (foam etc.). Using this architecture we are able to achieve an average recognition accuracy of 87%.
346,Jiao2018, As Internet economy and e-commerce develop China’s demands for express logistics rise briskly; with the continually improving environment of industrial development the express logistics industry has made steady progress in terms of business volume service innovations and synergetic development. As relevant technologies and management level continue to elevate express logistics in China is accelerating the pace in industrial transformation market integration and service upgrading.
347,Cai2018, In this paper we propose an effective deep-learned and hand-crafted features fusion network (DHFFN) for pedestrian gender recognition. In the proposed DHFFN the deep-learned and hand-crafted (i.e. HOG) features are extracted for the input image followed by the feature fusion process that is to combine these two features together for fully exploring the merits from both deep-learned and HOG features. Extensive experiments on multiple public datasets have demonstrated that the proposed DHFFN method is superior to the state-of-the-art pedestrian gender recognition methods.
348,Sharp2018, A critical mass of disruptive technologies is revolutionizing the workplace challenging the role of human beings in enterprise organizations. This chapter explores three emerging forces that are transforming the nature of work: (1) people analyticsPeople analytics (2) artificial intelligenceArtificial intelligence and related software algorithms and (3) robotics. As humans become less employable in the emerging automated political economy what role (if any) might government fill redressing ensuing human economic dislocations  The key to producing the right kind of policies in relation to the latest advances in data analytics artificial intelligenceArtificial intelligence (AI) and robotics is simple in theory: (1) minimize regulation so that new innovations and industries may flourish and at the same time (2) provide effective oversight to prevent and mitigate negative externalities. More difficult in application.
349,Joly2018, The recent and rapid advancements of digital technologies as well as the progress of digital cameras and other various connected objects have resulted in a great increase of multimedia data production worldwide. Such data becomes more and more crucial for understanding environmental issues and phenomena such as the greenhouse effect global warming and biodiversity loss. Therefore there is an increasing need for the development of advanced methods techniques and tools for collecting managing analyzing and understanding environmental and biodiversity data. The goal of this introductory chapter is to give a global picture of that domain and to overview the research works presented in this book.
350,Yan2018, The integration system of face and ID card is used to detect person in the important sections such as government place and road. In order to do the effective security of the important region in this paper we propose the integration system of face and ID card. The basic framework modules and case system are given.
351,Widdowson2018, Two issues that are crucial to the integration of flying robotic systems into human populated environments include: how humans perceive autonomous flying robots and how to design and control flying robots to improve the level of comfort and perceived safety for collocated others. This work represents a comprehensive virtual reality test environment to explore scripted and unscripted interactions with flying robots. We employ a multimethod approach by incorporating behavioral measures self-report questionnaires and physiological data to characterize human arousal during a variety of predetermined and real-time scenarios in both indoor and outdoor environments. By combining complementary methodological techniques we can converge on a data-driven model of social etiquette for flying robots; this model can then be reparametrized in terms of planning and control solutions to govern the robot’s behavior in a real-world context.
352,Zhang2018g, Traffic sign recognition is an important research area in intelligent transportation which is especially important in autopilot system. Convolutional Neural Network (CNN) is the main research method of traffic sign recognition. However the convolution neural network is easily affected by the spatial diversity of the image. With regard to this in this paper a multi-column spatial transformer convolution neural network named MC-STCNN is proposed to solve the problem when Convolutional Neural Network (CNN) can’t adapt to the spatial diversity of the image very well. The MC-STCNN network consisted of CNN and STN is formed by training pictures of different sizes. It can be well adapted to the spatial diversity and the images input of different sizes. It achieves an accuracy of 99.75% on GTSRB traffic sign recognition exceeding the current highest accuracy of 99.65%.
353,Zhong2018, Recently palmprint representation using different descriptors under the incorporation of deep neural networks always achieves significant recognition performance. In this paper we proposed a novel method to achieve end-to-end palmprint recognition by using Siamese network. In our network two parameter-sharing VGG-16 networks were employed to extract two input palmprint images’ convolutional features and the top network directly obtained the similarity of two input palmprints according to their convolutional features. This method had a good performance on PolyU dataset and achieved a high recognition outcome with an Equal Error Rate (EER) of 0.2819%. To test the robustness of the proposed algorithm we collected a palmprint dataset called XJTU from the practical daily environment. On XJTU the EER of our method is 4.559% which highlighted a promising potential of the usage of palmprint in personal identification system.
354,Cao2018, In principle data science can be applied to any application area or business domain. However data science projects are intrinsically different from ordinary software development and IT projects.
355,Bieger2018, A generally intelligent machine (AGI) should be able to learn a wide range of tasks. Knowledge acquisition in complex and dynamic task-environments cannot happen all-at-once and AGI-aspiring systems must thus be capable of cumulative learning: efficiently making use of existing knowledge during learning supporting increases in the scope of ability and knowledge incrementally and predictably   without catastrophic forgetting or mangling of existing knowledge. Where relevant expertise is at hand the learning process can be aided by curriculum-based teaching where a teacher divides a high-level task up into smaller and simpler pieces and presents them in an order that facilitates learning. Creating such a curriculum can benefit from expert knowledge of (a) the task domain (b) the learning system itself and (c) general teaching principles. Curriculum design for AI systems has so far been rather ad-hoc and limited to systems incapable of cumulative learning. We present a task analysis methodology that utilizes expert knowledge and is intended to inform the construction of teaching curricula for cumulative learners. Inspired in part by methods from knowledge engineering and functional requirements analysis our strategy decomposes high-level tasks in three ways based on involved actions features and functionality. We show how this methodology can be used for a (simplified) arrival control task from the air traffic control domain where extensive expert knowledge is available and teaching cumulative learners is required to facilitate the safe and trustworthy automation of complex workflows.
356,Dalianis2018, This chapter presents various applications of clinical text mining that all use the electronic patient record text as input data.
357,Martellini2018, In the era of global communication and increasingly sophisticated technologies cyber security is becoming a primary line of defense in the modern world (alongside with state security nuclear security biomedical sciences etc.) aiming to virtually ensure safety and stability of humanity at the global level.The cyberwarfare is neither a science fiction nor an emerging concept. We live in the world where covert operations are carried out across the world without the deployment of human operatives. Offensive cyber-security tools are accessible globally; many of them are free of charge.Current manuscript offers an insight into the global field of cyber security under various angles going beyond the security of personal computers laptops and smartphones. The Handbook on Security Sciences reviews selected issues in Computer and Network Security Wireless Security Cryptography Critical Infrastructure Cyber Security Confidence and Security Building Measures Industrial Control Systems Security and last but not least Physical Security.The proposed guidelines can be used by cyber-security experts security managers and general office staff to enhance the cyber security of their nation organization and personal lives.
358,Ortwine2018, The practice of computational chemistry in an industrial setting poses unique opportunities and challenges. Industrial computational chemists must manage large amounts of data master modeling software write scripts to perform custom calculations and stay abreast of scientific advances in the field. Just as importantly because computational chemists are full partners in the drug discovery effort at companies in order to influence and streamline the drug discovery process they must communicate effectively with medicinal chemists and other scientists to deliver results of their calculations in a timely fashion. The skills necessary to play this role require education that emphasizes a combination of chemistry programming and communication skills. Professors are encouraged to incorporate such training in their curriculum.
359,Stroemblad2018, Cell migration is a dynamic process that emerges from fine-tuned networks coordinated in three-dimensional space spanning molecular subcellular and cellular scales and over multiple temporal scales from milliseconds to days. Understanding how cell migration arises from this complexity requires data collection and analyses that quantitatively integrate these spatial and temporal scales. To meet this need we have combined quantitative live and fixed cell fluorescence microscopy customized image analysis tools multivariate statistical methods and mathematical modeling. Collectively this constitutes the systems microscopy strategy that we have applied to dissect how cells organize themselves to migrate. In this overview we highlight key principles concepts and components of our systems microscopy methodology and exemplify what we have learnt so far and where this approach may lead.
360,Hanne2018, One of the biggest trends nowadays is to make IT systems more intelligent in order to solve problems which were previously too complex to be solved or where the computing power prevented solving them within reasonable time. This trend is the renaissance of Artificial Intelligence (AI). In this chapter we first discuss some general issues related to mathematical modelling simulation and optimization for practical applications. After this we review certain related techniques referred to as Computational Intelligence (CI) which are particularly useful for dealing with complex mathematical models. We discuss the foundations of CI techniques and relations to AI. In the third section we discuss selected areas of application of CI and AI for business improvement on a more detailed level i.e. in transportation planning in warehouse management and in robotics in order to motivate their modeling complexity and potential for CI applications. We conclude with an outlook of CI techniques concerning their expected forthcoming practical importance.
361,Larkham2018, This chapter develops from an academic literature on the benefits of fieldwork in geography and observation of the benefits for professional disciplines such as town planning and urban design. Both disciplines claim to recognise the need for understanding urban form to inform design and management decisions. Recent advances in technology and data manipulation though have led some apparently to rely more on the virtual than the real. I argue that deep engagement with the messy complexities of real-world urban form has benefits including a better understanding of smaller features that cumulatively create a character the factors that shape the lived experience and the genius loci. So in a range of cultural contexts and urban forms what is important to observe and measure and how should we do so 
362,Wen2018a, User attribute classification plays an important role in the Internet advertising public opinion monitoring. While the user points of interest prediction helps the online social media services creating more value. In this paper aiming at solving the user attributes classification tasks we combine the feature engineering and deep Learning method to reach a higher rank. User attribute classification task is divided into two sub-tasks in sub-task one we use the user’s POI (point of interest) check-in history and popular POI location information to predict the next POI that user may visit the future. Sub-task 2 needs to predict the gender of the user. We use the Stacking method to carry out the feature fusion method to complete the feature extraction based on the output of the logistic regression model then features will be sent to XGBoost model to perform the prediction. In addition we also used the Convolution neural network model to dig out the user tweets information. Here we replace the conventional Max Pooling method with Attention Pooling in order to minimum the information lost in neural network training. Finally two methods are given to give a more accurate result.
363,Robbins2018, In the case of medicine the role of doctor can be understood as a cultural hero-project. When the self-esteem of the physician is weakened he or she is exposed to increased risk of burnout. To protect self-esteem physicians can become prone to cognitive biases to protect self-worth which can lead to “medical narcissism” and “defensive medicine” based on self-serving attributions that raise the risk of medical error. Worldview defense in medicine is linked to racism sexism and stigmatization of certain out-groups by medical professionals. Finally medical scientism can operate as a form of existential dogmatism that while serving as a worldview defense for medical professionals can nevertheless undermine the ends of science as an open-ended inquiry.
364,Natale2018, In this chapter we describe the history and evolution of the iCub humanoid platform. We start by describing the first version as it was designed during the RobotCub EU project and illustrate how it evolved to become the platform that is adopted by more than 30 laboratories worldwide. We complete the chapter by illustrating some of the research activities that are currently carried out on the iCub robot i.e. visual perception event-driven sensing and dynamic control. We conclude the chapter with a discussion of the lessons we learned and a preview of the upcoming next release of the robot iCub 3.0.
365,Ruseti2018, Summarization enhances comprehension and is considered an effective strategy to promote and enhance learning and deep understanding of texts. However summarization is seldom implemented by teachers in classrooms because the manual evaluation requires a lot of effort and time. Although the need for automated support is stringent there are only a few shallow systems available most of which rely on basic word/n-gram overlaps. In this paper we introduce a hybrid model that uses state-of-the-art recurrent neural networks and textual complexity indices to score summaries. Our best model achieves over 55% accuracy for a 3-way classification that measures the degree to which the main ideas from the original text are covered by the summary
366,Fernandez2018, The integration of geolocation big data and cognitive agents has become one of the most boosting business tools of the digital era. By definition geolocation represents the use of different technologies in a variety of applications to help locate humans and objects. To really achieve smart services companies also require accessing huge volumes of related information to draw meaningful conclusions. With big data it is possible to establish connections between a wide range of associated information and use it to improve available services or create new ones. Today the influence of geolocation cloud data science and involved cognitive agents impacts many application fields which include: safety and security marketing beacon technology geofencing location-sensitive services transportation and logistics healthcare urban governance intelligent buildings and smart cities intelligent transport systems advanced driver assistance systems and autonomous and semi-autonomous vehicles. To address these challenges this paper presents a general associative-cognitive architecture framework to develop goal-oriented hybrid human-machine situation-awareness systems focused on the perception and comprehension of the elements of an environment and the estimation of their future state for decision-making activities. The architecture framework presented emphasizes the role of the associated reality as a novel cognitive agent and the involved semantic structures to improve the capabilities of the corresponding system processes and services. As a proof of concept a particular situation awareness agent for geolocation of vehicles in tunnels is shown that uses cloud data association vision-based detection of traffic signs and landmarks and semantic roadmaps.
367,Zand2018, Drug development which includes clinical trials is a lengthy and expensive process that could significantly benefit from predictive modeling and in silico testing. Additionally current treatments were designed based on the average patient using the “one size fits all” protocol. Therefore they can be effective on some patients but not for others. There is an urgent need to replace such generalized approaches with personalized and predictive strategies that capture and analyze human diversity and variation at a resolution sufficient to identify and clinically validate personalized treatment paradigms. Utilization of heterogenous datasets such as Electronic Health Records (EHRs) to build synthetic populations of patients and personalized predictive models of response to therapy holds enormous promise in precipitating a revolution in precision medicine for IBD. In silico trials can be designed to include multi-modal data sources including clinical trial data at the individual and aggregated levels pre-clinical data from animal studies as well as data from EHR. In silico clinical trials can help inform the design of clinical trials and make prediction at the population and individual level to increase the chances of success. This chapter discusses pioneering work on the use of in silico clinical trials to accelerate the development of new drugs.
368,Ghazizadeh2018, Vehicular clouds have become an active area of research with tens of papers written and a large number of documented applications. We feel this is a good moment to summarize the main research trends in the area of vehicular clouds and to map out various research challenges and possible applications. With this in mind the primary objective of this chapter is to present a survey of the state of the art in vehicular clouds of the current research topics and future directions. We will take a critical look at the various vehicular cloud models proposed in the literature and their applications.
369,Dirks2018, Machine learning exploits data to learn but when not enough data is available (often due to increasingly complex models) or the quality of the data is insufficient then prior domain knowledge from experts can be incorporated to guide the learner. Prior knowledge typically employed in machine learning tends to be concise single statements. But for many problems knowledge is much more messy requiring in-depth discussions with domain experts to extract and often takes many iterations of model development and feedback from experts to collect all the relevant knowledge. In the Bayesian learning paradigm we learn which hypotheses are most likely given the data as evidence. How can we refine this model when new feedback is given by domain experts  We are working with domain experts on a problem where data is expensive but we also have prior knowledge. This research has two objectives: (1) automatically refine models using prior knowledge and (2) handle various forms of prior knowledge elicited from experts in a unified framework.
370,P szor2018, Autonomous cars robotic platforms and other devices capable of unassisted movement are becoming widely considered as superior to human-based control in many areas. Such platforms however often are constructed using expensive equipment. We investigate the possibility of using simple setup consisting of an embedded computer module such as smartphone single camera and inertial measurement unit along with the concept of the optical flow to detect possible collisions given real-time onboard processing as an alternative to compound systems based on radar and lidar devices. While most optical flow algorithms are not applicable for real-time processing our findings prove that those which sacrifice accuracy to gain speed can still be upgraded while remaining accurate enough for the field of collision avoidance. We propose modifications to further enhance optical flow for given context. Our findings prove that using proposed setup consisting of both hardware and software allows for omitting expensive sensors in the field of collision avoidance.
371,Crawford2018, Loder (The Transforming Moment 1989) remarks on the paradox that although conventional science values knowing that is grounded in demonstrable facts history testifies that deep and transforming truths are inexorably conceived through human imagination. This understanding presents all teachers with a challenge a challenge to fashion an environment of learning where not only is content and process valued but where students are encouraged to ‘draw deeply on personal intuition and the creative unconscious’ (The Transforming Moment p 49 1989). Such an environment speaks of a relationship between teacher and learner that transcends the idea of a teacher containing the knowledge the student needs and imparting that knowledge to the student. Therefore rather than focusing on teaching and learning from a theoretical stance this chapter endeavours to address (The Transforming Moment 1989) challenge by examining the elements of teaching as a relationship that has the capacity to conceive deep transforming knowing. Core to this relationship is the Trinitarian concept of perichoresis the divine dance that embraces all truth. This core is surrounded by such relational concepts as shalom and agape that create a covenantal space where the learner may not only flourish but also experience the joy of transformational knowing.
372,Oriot2018, This section of the pocket book covers the actual facilitation aspect of the debriefing based on a model including introduction reactions analysis summary and closing/conclusion. It presents the investigational techniques that can be used during the debriefing analysis phase such as the non-judgemental debriefing the good-judgement debriefing and the advocacy-inquiry approach. These various approaches aim to demonstrate respect for the participants’ actions and decisions at the same time as more or less probing into the rationale or mental frame behind those in order to close the identified performance gaps which can be cognitive behavioural or technical. The advocated approach that can be used involves individually “repackaging” the identified deficiencies generalising or decontextualising those and asking learners for solutions which forces them to actually fill those performance gaps and promotes deeper learning. The summary phase helps reviewing the important learning points or “take-home messages”. It is a way for the debriefer to ensure that learners actually recall the solutions of all the performance gaps which have been closed through the debriefing and hence that it has been effective (at least in terms of immediate recall). The closing or conclusion phase is more general and provides a further opportunity for learners to express concerns or reveal actual needs regarding additional practical skills training or access to recommended reading material to further their knowledge. It is also a key phase during which to thank the participants for their engagement and reminding them about the confidentiality aspect. Finally some useful debriefing sentences and questions relating to each of the debriefing phases are provided as a guide for debriefers.
373,Alimova2018, Adverse drug reactions can have serious consequences for patients. Social media is a source of information useful for detecting previously unknown side effects from a drug since users publish valuable information about various aspects of their lives including health care. Therefore detection of adverse drug reactions from social media becomes one of the actual tools for pharmacovigilance. In this paper we focus on identification of adverse drug reactions from user reviews and formulate this problem as a binary classification task. We developed a machine learning classifier with a set of features for resolving this problem. Our feature-rich classifier achieves significant improvements on a benchmark dataset over baseline approaches and convolutional neural networks.
374,Pike2018, Higher education students are being encouraged to spend time studying in another country because of its reputed benefits. Claims are made about the positive impact of study abroad in developing global citizens yet the current practice gives rise to several paradoxes. These include: how attempts at cultural adaptation can undermine acceptance of cultural pluralism; how an emphasis on risk management can limit students’ learning potential; and how efforts to increase participation in study abroad may perpetuate global inequities. Some possible strategies are offered to avoid these pitfalls and create strong programs that align with the ideals of global citizenship.
375,Braunstein2018, This first chapter provides a brief history of some but certainly not all of the key subdomains within the health informatics field and further explains the potential significance of the FHIR standard that will occupy much of the rest of the book. To do this the chapter begins with a discussion of early electronic records and clinical decision support tools and then shifts gears to introduce the concept of health information exchange. Later we discuss interoperability challenges that date back decades and the various ways that existing technologies have been used sometimes with limited success to simplify and coordinate the sharing of information among providers. The chapter ends with the premise that widespread adoption of modern web technologies (and FHIR in particular) is transforming health informatics. To help illustrate this the chapter ends with a demonstration FHIR app developed by a team of Georgia Tech students did using these emerging technologies to help predict the onset of a life threatening condition in ICU patients.
376,Vermeulen2018, The Process superstep adapts the assess results of the retrieve versions of the data sources into a highly structured data vault that will form the basic data structure for the rest of the data science steps. This data vault involves the formulation of a standard data amalgamation format across a range of projects.
377,Richter2018, In this work we develop a planner for high-speed navigation in unknown environments for example reaching a goal in an unknown building in minimum time or flying as fast as possible through a forest. This planning task is challenging because the distribution over possible maps which is needed to estimate the feasibility and cost of trajectories is unknown and extremely hard to model for real-world environments. At the same time the worst-case assumptions that a receding-horizon planner might make about the unknown regions of the map may be overly conservative and may limit performance. Therefore robots must make accurate predictions about what will happen beyond the map frontiers to navigate as fast as possible. To reason about uncertainty in the map we model this problem as a POMDP and discuss why it is so difficult given that we have no accurate probability distribution over real-world environments. We then present a novel method of predicting collision probabilities based on training data which compensates for the missing environment distribution and provides an approximate solution to the POMDP. Extending our previous work the principal result of this paper is that by using a Bayesian non-parametric learning algorithm that encodes formal safety constraints as a prior over collision probabilities our planner seamlessly reverts to safe behavior when it encounters a novel environment for which it has no relevant training data. This strategy generalizes our method across all environment types including those for which we have training data as well as those for which we do not. In familiar environment types with dense training data we show an 80% speed improvement compared to a planner that is constrained to guarantee safety. In experiments our planner has reached over 8 m/s in unknown cluttered indoor spaces. Video of our experimental demonstration is available at http://groups.csail.mit.edu/rrg/bayesian_learning_high_speed_nav.
378,Halin2018, Steering control for path tracking and navigation are important for the autonomous vehicle. A good steering control system can determine the success of autonomous navigation through designed paths. Comfort and safety for the passenger are the main concerns in developing a controller for an autonomous electric vehicle (AEV). Comfort and the safe autonomous system can be achieving by imitating human intelligence and decision-making ability into the controller. A GPS module couple with a fuzzy controller to follow the designed path. Steering is control by using brushless DC motor with certain gear configuration. In order to achieve better drive performance for the autonomous vehicle the behaviors of human subjects are studied and investigated. Investigation of steering angle on 3 different paths is designed to study the driving patterns by the human subjects which are straight turn right and turn left. The results show satisfactory outcomes as the subject navigates through the designed path with the similar patterns. The average value of steering wheel angle for the straight right and left path are 13°  151° and 237° respectively. The maximum angle to turning to the left and right are 286° (subject #1) and  226° (subject #1). This paper consists of the construction of a Fuzzy logic controller to control steering wheel and experiments set-up to develop the Fuzzy controller for an autonomous vehicle.
379,Miyata2018, In recent years a drive recorder becomes common and is installed in a car to record sensor data such as images acceleration and speed about driving. The recorded data is useful to confirm and analyze a dangerous driving scene of a traffic accident and an incident. However analyzing such data takes long time because it is done by a person who checks data one by one. Therefore a method of automatic classification of drive recorder data is explored in this study. First we labeled three types of incidents on the recorded data. Then after extracting features from the acceleration and velocity machine learning techniques are applied for the classification. Our preliminary evaluation showed that the classification result achieved about 0.55 of f-measure value.
380,Xing2018a, The Data-collection Problem (DCP) models robotic agents collecting digital data in a risky environment under energy constraints. A good solution for DCP needs a balance between safety and energy use. We develop an Ensemble Navigation Network (ENN) that consists of a Convolutional Neural Network and several heuristics to learn the priorities. Experiments show ENN has superior performance than heuristic algorithms in all environmental settings. In particular ENN has better performance in environments with higher risks and when robots have low energy capacity.
381,Lu2018a, In this paper collision avoidance problem is investigated for differential drive robot running in pedestrian environment which requires for natural and safe interaction between robot and human. Based on deep reinforcement learning a human-aware collision avoidance algorithm is proposed to find a smooth and collision-free path. A well designed reward function ensures the robot navigates without collision and obeys right-pass norm simultaneously. The slow convergence problem during training is addressed by pre-training the neural network using supervised learning. The simulation results show that the proposed algorithm can find a feasible and norm-obeyed path which achieves a natural human-robot interaction compared with traditional method.
382,Forczmanski2018, The paper is devoted to the problem of face detection in thermal imagery. Its aim was to investigate several contemporary general-purpose object detectors known to be accurate when working in visible lighting conditions. Employed classifiers are based on AdaBoost learning method with three types of low-level descriptors namely Haar like features Histogram of Oriented Gradients and Local Binary Patterns. Additionally the performance of recently proposed Max-Margin Object-Detection Algorithm joint with HOG feature extractor and Deep Neural Network-based approach have been investigated. Performed experiments on images taken in controlled and uncontrolled conditions gathered in our own benchmark database and in a few other databases support final observations and conclusions.
383,Guindel2018, Object identification in images taken from moving vehicles is still a complex task within the computer vision field due to the dynamism of the scenes and the poorly defined structures of the environment. This research proposes an efficient approach to perform recognition on images from a stereo camera with the goal of gaining insight of traffic scenes in urban and road environments. We rely on a deep learning framework able to simultaneously identify a broad range of entities such as vehicles pedestrians or cyclists with a frame rate compatible with the strong requirements of onboard automotive applications. The results demonstrate the capabilities of the perception system for a wide variety of situations thus providing valuable information to understand the traffic scenario.
384,Hariharan2018a, In this introductory chapter the need for a comprehensive and in-depth understanding of the underlying physics of batteries is discussed. The layout of the book as well as the major themes are briefly introduced to set the tone for the rest of the chapters that follow.
385,Cao2018a, Crowd count estimation from a still crowd image with arbitrary perspective and density level is one of the challenges in crowd analysis. Techniques developed in the past performed poorly in highly congested scenes with several thousands of people. To resolve the problem we propose a Multi-scale Fully Convolutional Network for robust crowd counting that is achieved through estimating density map. Our approach consists of the following contributions: (1) an adaptive human-shaped kernel is proposed to generate the ground truth of the density map. (2) A deep multi-scale fully convolutional network is proposed to predict crowd counts. Per-scale loss is used to guarantee the effectiveness of multi-scale strategy. (3) Several attempts e.g. de-convolutional and minimizing per-scale loss are tried to improve the counting performance of the proposed approach. Our approach can adapt to not only sparse scenes but also dense ones. In addition it achieves the state-of-the-art counting performance in benchmarking datasets including the World Expo’10 the UCF_CC_50 and the UCSD datasets.
386,Mishra2018, Cognitive NLP systems i.e. NLP systems that make use of behavioral data augment traditional text-based features with cognitive features extracted from eye-movement patterns EEG signals brain imaging etc. Such extraction of features has been typically manual as we have seen in the previous chapter. We now contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets enriched with gaze information to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN-based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.
387,Coward2018, Coward and Rhodes provide an exploration of interprofessional approaches to learning in a School of Health Sciences. These approaches enhance the ability of students to learn from one another to develop understanding of their own and other professions. The approaches favour reflection and experiential learning to solve ‘problems’ together that arise within the clinical setting. These methods help to develop healthcare professionals who show thoughtful and solution focused approaches by becoming reflexive in their practice.The module ‘Utilising interprofessional learning to engender employability’ provides a case study to demonstrate problem solving through reflective group work. This is within the context of an interprofessional group of learners where they are working together and utilising one another’s professional views and values.
388,Li2018k, Intelligent logistics represents a global trend in the evolution of logistics industry and is the focus of transformation and upgrading of logistics industry in China. At present IoT big data automated logistic equipment and other technologies are developing robustly. “Made in China 2025” “Internet+” Action Plan and other national strategies have been put into implementation. Unprecedented reform is happening in China’s logistics industry. Intelligent logistics is taking the lead in supply-side structural reform in logistics industry and entering into the stage of rapid development.
389,London2018, The classroom can be an exciting place full of the potential for transformational learning. However this possibility often remains at the level of wishful thinking. This paper describes transformational teaching and learning in practice and the migration of relationships from teacher/student to mentor/apprentice. Moreover we reimagine the classroom as one that is co-created by instructor and students transforming the learning space and leading to meaningful relationships growth and development.
390,Cohan2018, Most entrepreneurs lack the know-how to turn their idea into a billion dollar company. A well-running Startup Common fills that know-how gap through corporate and individual mentoring.
391,Gao2018b, Face recognition techniques are widely used in many applications such as automatic detection of crime scenes from surveillance cameras for public safety. In these real cases the pose and illumination variances between two matching faces have a big influence on the identification performance. Handling pose changes is an especially challenging task. In this paper we propose the learning warps based similarity method to deal with face recognition across the pose problem. Warps are learned between two patches from probe faces and gallery faces using the Lucas-Kanade algorithm. Based on these warps a frontal face registered in the gallery is transformed into a series of non-frontal viewpoints which enables non-frontal probe face matching with the frontal gallery face. Scale-invariant feature transform (SIFT) keypoints (interest points) are detected from the generated viewpoints and matched with the probe faces. Moreover based on the learned warps the probability likelihood is used to calculate the probability of two faces being the same subject. Finally a hybrid similarity combining the number of matching keypoints and the probability likelihood is proposed to describe the similarity between a gallery face and a probe face. Experimental results show that our proposed method achieves better recognition accuracy than other algorithms it was compared to especially when the pose difference is within 40 degrees.
392,Kwon2018c, A robotic system that consists of only a gripper can be utilized for certain applications such as supporting disabled people. However with a robot manipulator introduced into the system it can achieve far more tasks such as automation of manufacturing and logistics processes. The autonomous track of the IROS2016 Robotic Grasping and Manipulation Competition was designed to bring a robotic system into ordinary everyday tasks involving grasping and manipulation. The main objective of this paper is the evaluation of the autonomous robotic system by comparing the performance against manual human-interacted system in terms of intelligence and robustness.We used UR5 Dora-Hand2 and Realsense SR300 to build an autonomous system for grasping and manipulation. The system has been evaluated by performing ten manipulation tasks and a pick-and-place task. The overall performance was below the manual system. However for the tasks that involved repetitive motion the automated system out-performed the manual system.
393,Xiao2018, Hyperspectral (HS) image is a three dimensional data image where the 3rd dimension carries the wealth of spectrum information. HS image compression is one of the areas that has attracted increasing attention for big data processing and analysis. HS data has its own distinguishing feature which differs with video because without motion also different with a still image because of redundancy along the wavelength axis. The prediction based method is playing an important role in the compression and research area. Reflectance distribution of HS based on our analysis indicates that there is some nonlinear relationship in intra-band. The Multilayer Propagation Neural Networks (MLPNN) with backpropagation training are particularly well suited for addressing the approximation function. In this paper an MLPNN based predictive image compression method is presented. We propose a hybrid Adaptive Prediction Mechanism (APM) with MLPNN model (APM-MLPNN). MLPNN is trained to predict the succeeding bands by using current band information. The purpose is to explore whether MLPNN can provide better image compression results in HS images. Besides it uses less computation cost than a deep learning model so we can easily validate the model. We encoded the weights vector and the bias vector of MLPNN as well as the residuals. That is the only few bytes it then sends to the decoder side. The decoder will reconstruct a band by using the same structure of the network. We call it an MLPNN decoder. The MLPNN decoder does not need to be trained as the weights and biases have already been transmitted. We can easily reconstruct the succeeding bands by the MLPNN decoder. APM constrained the correction offset between the succeeding band and the current spectral band in order to prevent HS image being affected by large predictive biases. The performance of the proposed algorithm is verified by several HS images from Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) reflectance dataset. MLPNN simulation results can improve prediction accuracy; reduce residual of intra-band with high compression ratio and relatively lower bitrates.
394,Erol2018, Robotic navigation in GPS-denied environmentsGPS denied environment requires case specific approaches for controlling a mobile robot to any desired destinations. In general a nominal path is created in an environment described by a set of distinct objects in other words such obstacles and landmarks. Intelligent voiceVoice assistants or digital assistance devices are increasing their importance in today’s smart home. Especially by the help of fast-growing Internet of Things (IoT) applications. These devices are amassing an ever-growing list of features such as controlling states of connected smart devices recording tasks and responding to queries. Assistive robots are the perfect complement to smart voice assistants for providing physical manipulation. A request made by a person can be assigned to the assistive robot by the voiceVoice assistant. In this chapter a new approach for autonomous navigationAutonomous navigation is presented using pattern recognitionPattern recognition and machine learningMachine learning techniques such as Convolutional Neural Networks to identify markers or objects from images and videos. Computational intelligenceComputational intelligence techniques are implemented along with Robot Operating System and object positioning to navigate towards these objects and markers by using RGB-depth cameraDepth camera. Multiple potential matching objects detected by the robot with deep neural networkNeural network object detectors will be displayed on a screen installed on the assistive robot to improve and evaluate Human-Robot Interaction (HRI).
395,Kamencay2018, The following article is dedicated to techniques for recognition of vehicles on the road. By using 3D virtual models of vehicles it is possible to create database of point cloud. The SSCD algorithm for training and testing was used. First for each 3D model the point clouds were created. Then from each point cloud one hundred pictures were rendered from different projections. Creation of filtered dataset was done by selection six angles from these projections. This dataset contains 100 models of vehicles divided into 5 classes. In summary final non-filtered dataset contains 10 000 pictures filtered dataset consist of 600 pictures. Dataset was used in support vector machine (SVM) and convolutional neural network (CNN) for training and testing in ratio 80:20. The result for SVM was 40% this was done because non-filtered dataset contains many similar projections. Moreover the size resulted in long duration of experiment (<90 h). Therefore other experiments were done with filtered dataset. In filtered dataset best result in SVM was 79% with RBF kernel. For the next experiment CNN was used. With data augmentation the result was 80% without 89%.
396,Stepchenkova2018, This interdisciplinary research deals with attractiveness of China as a tourist destination for American pleasure travelers barriers that American tourists perceive as preventing their travel to the destination and their sentiments toward travel to China. The data were obtained through an online panel survey aiming at understanding American leisure travelers’ international travel experiences and their perceptions of China. The obtained sample included 3263 responses and was balanced on region gender age and household income. First the study applies content analysis to textual responses to identify the main categories of risks and barriers associated with travel to China. Second it compares three sentiment analysis algorithms namely Deeply Moving Pattern and SentiStrength against the manual classification of textual responses and applies the best performing software Deeply Moving to quantify the respondents’ sentiments toward travel to China. Finally the study compares socio-demographic groups of survey respondents on attitudes expressed toward travel to China.
397,Tran2018, The paper proposes a solution for pedestrian action prediction from single images. Pedestrian action prediction is based on the analysis of human postures in the context of traffic in traffic systems. Normally other solutions use sequential frames (video) motion properties. Technically these solutions may produce high results but slow performance since the need to analyze the relationship between the frames. This paper takes into account analyzing the relationship between the pedestrian postures and traffic scenes from an image with the expectation that ensures accuracy without analyzing the relationship of motion between frames. This work consists of two phases which are human detection and pedestrian action prediction. First human detection is solved by applying aggregate channel features (ACF) method and then predict pedestrian action by extracting features of this image and use the classifier model which is trained by features extracted of pedestrian image dataset in convolution neural network (CNN) model. The minimum accuracy rate is 82% the maximum is 97% with the average response rate of 0.6 s per pedestrian case has that been identified.
398,Stabler2018, Dr Stabler highlights the importance of surfacing and addressing team difficulties in the National Health Service (NHS) for the benefit of patient safety and staff wellbeing. Stabler provides a guide to engaging and supporting healthcare teams in the use of digital storytelling as a method to develop and promote team resilience. Through a detailed case study of two teams that had experienced difficult circumstances and her interaction with them Stabler illustrates the risks and rewards of digital storytelling for staff in NHS organisations discusses the process of preparation for the workshop and summarises key points for successful introduction of digital storytelling in an NHS context.The stories that were created as part of this project can be seen at www.patientvoices.org.uk/ht.htm.
399,Tan2018, We present one way of constructing a social robot such that it is able to interact with humans using multiple modalities. The robotic system is able to direct attention towards the dominant speaker using sound source localization and face detection it is capable of identifying persons using face recognition and speaker identification and the system is able to communicate and engage in a dialog with humans by using speech recognition speech synthesis and different facial expressions. The software is built upon the open-source robot operating system framework and our software is made publicly available. Furthermore the electrical parts (sensors laptop base platform etc.) are standard components thus allowing for replicating the system. The design of the robot is unique and we justify why this design is suitable for our robot and the intended use. By making software hardware and design accessible to everyone we make research in social robotics available to a broader audience. To evaluate the properties and the appearance of the robot we invited users to interact with it in pairs (active interaction partner/observer) and collected their responses via an extended version of the Godspeed Questionnaire. Results suggest an overall positive impression of the robot and interaction experience as well as significant differences in responses based on type of interaction and gender.
400,Skinner2018, This chapter introduces entrepreneurship as a driver of innovation social change and economic development. It argues that entrepreneurship is critical for sustaining economic prosperity through job creation as it improves the competitiveness of an economy and creates new wealth. However it is equally important to maintain the entrepreneurial spirit even inside established organisations in order to produce a high level of entrepreneurial development and a climate of innovation. The chapter further notes that entrepreneurship should be seen as a core element of sports. Sport enterprises embarking on an entrepreneurial journey for the first time are faced with the need to quickly adapt to situations. Many sport enterprises lack an awareness of the ripple effects that policies norms markets and numerous other factors can have on their intended actions. Learning how to ‘play the game’ means learning how to effectively react and even stimulate the ripples in order to not only survive but thrive in creating new opportunities. The chapter leads to the question of whether sport enterprises can learn how to be entrepreneurial so as to achieve their ambitions. It concludes that entrepreneurship can be seen as a management process wherein entrepreneurial behaviour is crucial to the long-term vitality of sport enterprises.
401,Salkin2018, Industrial Revolution emerged many improvements in manufacturingManufacturing and service systems. Because of remarkable and rapid changes appeared in manufacturing and information technologyInformation technology synergy aroused from the integration of the advancements in information technology services and manufacturing were realized. These advancements conduced to the increasing productivity both in service systems and manufacturing environment. In recent years manufacturing companies and service systems have been faced substantial challenges due to the necessity in the coordination and connection of disruptive concepts such as communication and networking (Industrial Internet)Industrial internet embedded systems (Cyber Physical Systems) adaptive roboticsAdaptive robotics cyber security data analytics and artificial intelligenceArtificial intelligence and additive manufacturing. These advancements caused the extension of the developments in manufacturing and information technology and these coordinated and communicative technologies are constituted to the term Industry 4.0 which was first announced from German government as one of the key initiatives and highlights a new industrial revolution. As a result Industry 4.0 indicates more productive systems; companies have been searching the right adaptation of this term. On the other hand the achievement criteria and performance measurements of the transformation to Industry 4.0 are still uncertain. Additionally a structured and systematic implementation roadmap is still not clear. Thus in this study the fundamental relevance between design principles and technologies is given and conceptual framework for Industry 4.0 is proposed concerning fundamentals of smart products and smart processes development.
402,Varsani2018, Humanity is at a special time in its relationship with technology where there is an increasing likelihood of artificially replicating characteristics which we thought were in the realm of the distinctly human. Artificial Intelligence and Robotics are making the news increasingly often and replication of body parts is also making progress. This chapter looks at senses which although not exclusively human have a powerful potential to support other higher functions and aspects of human life. Technology has been developing nature-inspired artifacts which resemble somehow their human counterparts with specific practical applications. So far these explorations have been mostly isolated. It is only a matter of time however until these become physically and logically connected into a cooperative fashion   in fact Robots typically use them although not always in their full capacity. Such developments will provide machines with interface capacities of a higher order bringing new powerful tools to solve new problems whilst raising unexpected scenarios and challenges for our societies. Regardless of whether we want it or not it seems impossible to stop technological progress in this direction. We assume this development is here to stay. Thus we look at the artificial and human synergies how interaction with machines is influenced by sense-like interface capabilities   namely “sensorial computing”.
403,Moufaddal2018, All our daily digital actions generate data at an alarming velocity volume and variety. To extract meaningful value from big data we need optimal processing power analytics capabilities and skills. Nowadays big data solutions are widely applied in different types of organizations. Such solutions bring multiple benefits in managing supply chains. The aim of this paper is to give an overview of big data analytic techniques used in supply chain management based on the latest version of the SCOR model.
404,Wang2018q, In recent years passport has been paid more and more attention. Passport is not only a certificate of the passport holders but also involves the international anti-terrorism situation. Passport security thread as a security feature which is the most direct and easy to identify is generally used by national passports. However in the passport manufacturer the current inspection method of the passport security thread is manual inspection. Computer vision can be applied in this aspect for automatic inspection through some systems or machines. This paper proposes that a custom-built computer vision system can utilize the reflected light to collect images and detect the buried security thread with the relative high accuracy. After analysis the detection of the security thread can be considered as a class of the object detection. On account of the gorgeous page’s pattern around the security thread in passport the most of object detection algorithms are failed to complete this task. Taking both accuracy and detection speed into account in this work we develop an improved algorithm based on the traditional SURF operator to achieve security line detection in passport. After verification on a sample set containing 134 samples this approach has been a certain ability to detect the security thread with the accuracy of 84.33%.
405,McIntyre2018, This chapter details a case study of the University of Newcastle Bachelor of Communication program’s capstone course. This course takes all the prior learning as its jumping off point and is the final year course in the undergraduate program where little distinction is overtly manifest between theory and practice. We believe this course and all the preceding work that leads to it helps broaden ‘the range of identities available to students from those of dutiful pupil or earnest citizen to more powerful and pleasurable identities of producer director and creator’ (Bragg quoted in Ashton Productive passions and everyday pedagogies: Exploring the industry-ready agenda in higher education. Art Design and Communication in Higher Education 9(1): 41 56. 2010 p. 52). This media production course is where after all the preparatory work the theoretical model shows its full potential in relation to practice. This project based course allows final year students to create an individually based or group project in any one of the media forms they have previously engaged with. They are encouraged to link media forms in an innovative way although this is not mandatory. The choice is theirs. As such the course relies on the student developing an active form of agency allowing for substantial creative collaborative and technical effort in realising the productions they engage in. They must by necessity engage fully with the structures of both the domain and the field pertinent to their project. This development of an idea and carrying it out to fruition within a simulated creative system provides the material they reflect on in a simplified exegetical way. The students have a strong understanding of creativity from a research point of view and are made capable of assessing the success or failure of their own practice in those terms. In this way a deep learning engagement occurs for them. This learning experience is unique to each student and they take this knowledge with them into their future creative productions or in the case of some to further study.
406,Lucas2018, Parking on a college campus is understood to be a challenge for commuters. With a rising matriculation rate in the United States the task of finding parking on an expansive campus grows even more daunting. However the rising prominence of the Internet of Things has initiated a paradigm shift in data-analysis computing. The point of data collection is often outlier locations removed from existing infrastructure and parking lots are no exception. Using proximity sensors solar power and cellular communication we can create such an IoT system to monitor parking lot in- and outflows. The parking data collected can be analyzed to create a smarter more efficient parking experience.
407,Kodama2018, In this chapter I derive common theoretical and empirical knowledge through cross-case analysis of multiple in-depth case studies in Part 2 and at the same time present a new theoretical framework derived from case studies while verifying the propositions and hypotheses derived in Chaps. 1 and 2.This chapter first discusses the characteristics of capabilities on knowledge boundaries between stakeholders. Then it discusses how synchronization of stakeholder activities on knowledge boundaries promotes synchronization of the dynamic capabilities (DC) of the various individual players involved which brings about collaborative dynamic capabilities (C-DC) among players. The chapter shows the necessity of synchronizing pragmatic boundaries by forming strategic communities with the main player and partners. Moreover pragmatic boundaries synchronization between main player(s) and partner(s) on the Capabilities Map brings about synchronization of the strategic innovation loop (boundaries synchronization).This chapter also discusses the importance of optimized asset orchestration in companies between companies and between industries the formation of strategic communities within companies between companies and between industries and the acquisition of C-DC in companies between companies and between industries for success in building ecosystems through service innovation and clarifies the potential of these factors in bringing about the construction of health support ecosystems.As an element of C-DC the chapter also clarifies the importance of the concept of “capabilities congruence” among ecosystem partners in achieving capability synthesis to maximize capabilities in ecosystems. As requirements for the five capabilities elements of ecosystem partners (1) strategy capabilities (2) organizational capabilities (3) technology capabilities (4) operational capabilities and (5) leadership capabilities the chapter presents the concept of “congruence among capabilities elements” as well as new theoretical and practical implications.
408,Kayakutlu2018, Low-Carbon Economy policies drive Europe for an integrated approach for utility consumption and management; number of integrated distribution companies are increasing. This new trend will soon cause the need for group decisions collective intelligence approach to the energy industry. This study aims to review the collective intelligence concepts and methods to give a summary of collective intelligence use in energy applications. It can be considered as a foundation for the future of collective intelligence in the energy industry.
409,Lin2018, The elderly may have different aspects of inconvenience in their daily life. Among them many old people have trouble remembering things even just happened hours ago. They often forget whether they have locked the door while leaving so that they may have to return and check. Such situation also happens to many younger people that do not concentrate their mind while locking the door. In this paper an intelligent key system iKey is proposed to solve such problem. It can be deployed on an existing key to detect user’s locking actions and store locking status in the form of time. Related hardware architecture and working process are proposed. The sensing module based on inclination angle sensors is designed to reduce the amount of data generated. Furthermore efficient locking detection algorithms are proposed accordingly. Such system and techniques can also be applied in knobs or rotating handles of machines and facilities to detect illegal operations and to avoid user’s forgetting to operate them.
410,Silvey2018, Autonomous systems embedded in our physical world need real-world interaction in order to function but they also depend on it as a means to learn. This is the essence of artificial Embodied Cognition in which machine intelligence is tightly coupled to sensors and effectors and where learning happens from continually experiencing the dynamic world as time-series data received and processed from a situated and contextually-relative perspective. From this stream our engineered agents must perceptually discriminate deal with noise and uncertainty recognize the causal influence of their actions (sometimes with significant and variable temporal lag) pursue multiple and changing goals that are often incompatible with each other and make decisions under time pressure. To further complicate matters unpredictability caused by the actions of other adaptive agents makes this experiential data stochastic and statistically non-stationary. Reinforcement Learning approaches to these problems often oversimplify many of these aspects e.g. by assuming stationarity collapsing multiple goals into a single reward signal using repetitive discrete training episodes or removing real-time requirements. Because we are interested in developing dependable and trustworthy autonomy we have been studying these problems by retaining all these inherent complexities and only simplifying the agent’s environmental bandwidth requirements. The Multi-Agent Research Basic Learning Environment (MARBLE) is a computational framework for studying the nuances of cooperative competitive and adversarial learning where emergent behaviors can be better understood through carefully controlled experiments. In particular we are using it to evaluate a novel reinforcement learning long-term memory data structure based on probabilistic suffix trees. Here we describe this research methodology and report on the results of some early experiments.
412,Du2018b, In this paper we propose a multi-view fusion 3D model retrieval using convolutional neural network to solve the problem of the local perception in feature descriptor. By view pooling we combine information from multiple views of a 3D model to eliminate the position correlation caused by the viewing angle of camera. In addition integrating pre-processed RGB view-feature with Binary view-feature in the same model is used to generate a single model descriptor. Experiments on ETH dataset demonstrate the superiority of the proposed method.
413,Balakrishnan2018, Intelligent Transportation Systems (ITS) demand driving safety as an eminent design requirement for future generation vehicles. Collision evasion as well as consequent casualties minimization command timely delivery of significant precautionary information to the drivers. Consequently the driver may get a clear view about the present driving situation and be able to adopt timely decision to circumvent the forthcoming dangers. This research work proposes an Adaptive Neuro-Fuzzy Inference System (ANFIS) based situation assessment method that supports the drivers to take up suitable decisions by analyzing the driver behavior of preceding/succeeding cars. The proposed approach models the stability of drivers in the perspective of connected cars and deduce the current stability situations from the sensors which are implanted in the cars. This connected cars scenario for Collision Warning System (CWS) is simulated using three Raspberry Pi boards along with ultrasonic sensor gas sensor and accelerometer sensor. These sensor data are transmitted to other preceding or succeeding cars using visible light communication. Subsequently these data are processed using both Mamdani and ANFIS model for situation assessment which provides the stability level of drivers. The result concludes though the Mamdani model quickly computes the stability of driver by analyzing the sensor data it suffers from low sensitivity and precision when compared to ANFIS which showcases higher sensitivity and precision.
414,Cheung-Judge2018, This chapter discusses the application of different types of values in shaping organization development (OD) practice in a global context. The types of values that inform and shape OD practice in supporting organizations are examined as well as how value dilemmas and conflictual situations are handled. Three case situations are used to illustrate these issues.
415,Nirmal2018, International trading activities are increasing at a great pace. National governments now a days often find it difficult to evolve policies to regulate the implications arising out of new world economic order. International environmental law Legal Education and Information technology law are the areas which experience the change the most. These areas may seem to be distinct but are in fact closely interrelated.
416,Rettinger2018, Semantic technologies are a key enabler for Knowledge 4.0. Specifically knowledge graphs have caused significant practical implications for managing knowledge in the digital economy. While most semantic technologies originate from the vision of representing the existing Web in a machine-processable format it’s most notable success so far are large cross-domain knowledge graphs. They are created by collaborative human modelling and linking of structured and semi-structured data. So far they exhibit only little but still very powerful semantics which have shown benefits for numerous applications. This chapter introduces the latest innovations in modelling knowledge using knowledge graphs and explains how those knowledge graphs enable value creation by making unstructured content like text documents accessible by machines and humans. Finally we show how semantic technologies help to make hard- and software components in cyber physical systems interoperable.
417,Grimes-MacLellan2018, This chapter explores international study abroad (SA) students who participated in volunteer and service-learning (SL) activities in the northeastern Tohoku region of Japan. It examines how the international students benefitted in many ways from this specific form of SA engagement as they worked alongside their Japanese student peers during a three-day study tour to complete authentic meaningful and useful collaborative activities in the service of others. These included an active use of the target language as students engaged in task-based activities that also allowed for a broader range of communicative opportunities. While this chapter discusses the specific context of Japan this strategy for promoting participant self-efficacy intercultural understanding and global awareness through participation in volunteer and SL opportunities has widespread applicability to other SA contexts.
418,Katiyar2018, Fragment-based drug design strategies have been used in drug discovery since it was first demonstrated using experimental structural biology techniques such as nuclear magnetic resonance (NMR) and X-ray crystallography. The underlying idea is that existing or new chemical entities with known desirable properties may serve both as tool compounds and as starting points for hit-to-lead expansion. Despite the recent advancements there remain challenges to overcome such as assembly of the synthetically feasible structures development of scoring functions to correlate structure and their activities and fine tuning of the promising molecules. This chapter first covers the theoretical background needed to understand the concepts and the challenges related to the field of study followed by the description of important protocols and related software. Case studies are presented to demonstrate practical applications.
419,Sad2018, We are witnessing the evolution of the concepts capturing the dynamics of learning. Starting with “following the rules” in the single-loop learning model the concepts have evolved showing the need for “changing the rules” and for “thinking outside the box” with significant changes also at the “mental model”. The idea of continuous improvement is summarized in “learning about learning” the essence of triple-loop learning model “reflexive thought and outside contribution” in order to change and transform organizational practices and learning processes. In this paper we discuss the particular case of the students at risk and their learning improvement possibilities using ICT tools namely blogs.
420,Li2018l, Taking the characteristic value as the core a population abnormality detection algorithm is used to process the crowd surveillance video. Using density detection the density of the population is first obtained. Object-based feature extraction is used in low-density scenes and pixel-based feature extraction in high-density scenes. So as to obtain the crowd of exercise intensity trajectory gradient entropy and local density and other characteristic value. Finally identify the abnormal behavior of the population based on characteristic value. The experimental results show that the characteristic value is obvious when the abnormality occurs. The algorithm’s performance index is superior to the traditional crowd behavior recognition algorithm with high recognition rate.
421,Yang2018c, Vehicle active safety technology (VAST) is the precondition of intelligent vehicle. VAST is useful to reduce traffic accidents as well as to guarantee the personal and property security of the drivers. Vehicle detection plays as the foundation of VAST however it suffers from many challenges e.g. partial occlusion severe weather conditions and perspective distortion. We employ the faster region convolution neural network (Faster RCNN) model in this work. We specialize it to vehicle detection through fine-tuning the model with vehicle samples. Full convolution layers and region proposal network are improved to enhance the detection performance of small-size vehicles. A small region up-sampling strategy is proposed to further improve the detection performance if the image is captured by a camera mounted on a vehicle. The effectiveness of the proposed approach is demonstrated through experiments in our own dataset and two benchmarking ones. Comparisons with baseline Faster RCNN indicate the superiority of our approach.
422,Spalter-Roth2018, The labor market is described as a set of queuing practices in which employers rank workers in terms of their views of who is likely to be productive who they can pay the least who will not complain about working conditions and who is likely to fit in. Workers rank jobs in terms of wages benefits autonomy and workplace conditions as well as who they know. These queues are neither gender neutral nor color blind despite laws that prohibit deliberate discrimination. The data in this chapter reveal a stratified queue with substantial differences in unemployment rates occupational participation and earnings in 2000 among racial ethnic and gender groups. By 2014 more workers fell back in the queue as employers used race- ethnicity- and gender-based strategies to restructure the workforce to increase profits and control. Not surprisingly in a capitalist economy worker strategies had substantially less impact especially because of lack of unity among race ethnic and immigrant groups. The chapter concludes with a discussion of how we can expect the queue to change in the next decade.
423,Taylor2018a, Drawing on a range of international examples this chapter examines the impact that customer operated payment systems (COPS) are having on the retail industry and in particular on crime. Organised broadly into four parts it first provides an overview of new developments in COPS and maps their future trajectory exploring the emergence of self-service checkout (SCO) scan-as-you-go and mobile payment systems. It then turns to mapping the known impact on customer theft before outlining some of the key concerns and vulnerabilities about their implementation. The final section considers the future of point of sale (POS) technology including sensor-based retailing and the impact on retail crime providing recommendations to the industry on how to embrace customer autonomy in the age of automation and deliver retail solutions that are cognisant of potential vulnerabilities and risks.
424,Trentesaux2018, Data management architectures are key elements to support the improvement of the availability and the maintainability of fleets of transportation systems such as trains cars planes and boats during their use. In this context this chapter proposes the foundations of a specific data management architecture named “Surfer”. The design of this architecture follows a set of specifications named “the Surfer way” that translates an original way to consider the issue of big data aiming to transform raw data into high level knowledge usable by engineers and managers. An application of the Surfer architecture to train transportation is presented. First results are encouraging the train constructor expects a gain up to 2% of the availability of a fleet of trains.
425,Jones2018, Information and communications technologies (ICTs) are pervasive in twenty-first-century society something that subsequently warrants their importance in twenty-first-century education. Whilst the use of ICTs in education is fairly ubiquitous the ways in which they are utilised are however quite varied. A range of factors influence the ways in which ICTs are used in education including those related to access socio-economic status ethics cyber safety and teacher confidence and competence. These influential factors have contributed to technology use in schools becoming an important topic for consideration particularly in regard to the what where and how of ICT for learning. Fitting with the theme of this manuscript the considerations privileged in this chapter are those concerned with the relationship between ICT pedagogy and personalising learning. We explore ICT’s potential to be a vehicle for personalising learning and argue that the achievement of such potential lies in understanding the perspectives of and relationships with pedagogy in deeper thinking about technology and learning that teachers and teacher educators need to undertake.
426,Baniasadi2018, Relying upon machine intelligence with reductions in the supervision of human beings requires us to be able to count on a certain level of ethical behavior from it. Formalizing ethical theories is one of the plausible ways to add ethical dimensions to machines. Rule-based and consequence-based ethical theories are proper candidates for Machine Ethics. It is debatable that methodologies for each ethical theory separately might result in an action that is not always justifiable by human values. This inspires us to combine the reasoning procedure of two ethical theories deontology and utilitarianism in a utilitarian-based deontic logic which is an extension of STIT (Seeing To It That) logic. We keep the knowledge domain regarding the methodology in a knowledge base system called IDP. IDP supports inferences to examine and evaluate the process of ethical decision making in our formalization. To validate our proposed methodology we perform a Case Study for some real scenarios in the domain of robotics and automatous agents.
427,Hsu2018, Detecting humans in video is becoming essential for monitoring crowd behavior. Head detection is proven as a promising way to realize detecting and tracking crowd. In this paper a novel learning strategy called Deep Motion Information Network (abbr. as DMIN) is proposed for head detection. The concept of DMIN is to borrow the traditional well-developed head detection approaches which are composed of multiple stages and then replace each stages in the pipeline into a cascade of sub-deep-networks to simulate the function of each stage. This learning strategy can lead to many benefits such as preventing many trial and error in designing deep networks achieving global optimization for each stage and reducing the amount of training dataset needed. The proposed approach is validated using the PETS2009 dataset. The results show the proposed approach can achieve impressive speedup of the process in addition to significant improvement in recall rates. A very high F-score of 85% is achieved using the proposed network that is by far higher than other methods proposed in literature.
428,Kulbacki2018, The intelligent video monitoring system SAVA has been implemented as a prototype at the 9th Technology Readiness Level. The source of data are video cameras located in the public space that provide HD video streaming. The aim of the study is to present an overview of the SAVA system enabling identification and classification in the real time of such behaviors as: walking running sitting down jumping lying getting up bending squatting waving and kicking. It also can identify interactions between persons such as: greeting passing hugging pushing and fighting. The system has module-based architecture and is combined of the following modules: acquisition compression path detection path analysis motion description action recognition. The effect of the modules operation is a recognized behavior or interaction. The system achieves a classification correctness level of 80% when there are more than ten classes.
429,Yang2018d, A major challenge in video surveillance is how to accurately detect anomalous behavioral patterns that may indicate public safety incidents. In this work we address this challenge by proposing a novel architecture to translate the crowd status problem in videos into a graph stream analysis task. In particular we integrate crowd density monitoring and graph stream mining to identify anomalous crowd behavior events. A real-time tracking algorithm is proposed for automatic identification of key regions in a scene and at the same time the pedestrian flow density between each pair of key regions is inferred over consecutive time intervals. These key regions are represented as the nodes of a graph and the directional pedestrian density flow between regions is used as the edge weights in the graph. We then use Graph Edit Distance as the basis for a graph stream analysis approach to detect time intervals of anomalous flow activity and to highlight the anomalous regions according to the heaviest subgraph. Based on the experimental evaluation on four real-world datasets and a benchmark dataset (UCSD) we observe that our proposed method achieves a high cross correlation coefficient (approximately 0.8) for all four real-world datasets and 82% AUC with 28% EER for the UCSD datasets. Further they all provide easily interpretable summaries of events using the heaviest subgraphs.
430,Jonas2018, Digital Transformation is a significant enabler for generating business value. Many of these technologies have been used in alumina refining by leading producers and these leaders have realized step changed improvements in operational and business performance. The technologies deployed have included Industrial Internet of things (IIoT) big data cloud computingComputing and deep analytics. Other alumina producers are considering these digital technologies to sustain or improve their competitiveness and bottom line. This paper will discuss the value that digital technologies and solutions can deliver to alumina refineries. The author is employed by the leading provider of these technologies and discuss proven technologies and methods used that ensure success of digital transformation to deliver of business value.
431,Siavvas2018, Software security is a matter of major concern for software development enterprises that wish to deliver highly secure software products to their customers. Static analysis is considered one of the most effective mechanisms for adding security to software products. The multitude of static analysis tools that are available provide a large number of raw results that may contain security-relevant information which may be useful for the production of secure software. Several mechanisms that can facilitate the production of both secure and reliable software applications have been proposed over the years. In this paper two such mechanisms particularly the vulnerability prediction models (VPMs) and the optimum checkpoint recommendation (OCR) mechanisms are theoretically examined while their potential improvement by using static analysis is also investigated. In particular we review the most significant contributions regarding these mechanisms identify their most important open issues and propose directions for future research emphasizing on the potential adoption of static analysis for addressing the identified open issues. Hence this paper can act as a reference for researchers that wish to contribute in these subfields in order to gain solid understanding of the existing solutions and their open issues that require further research.
432,Dilokthanakul2018, In most contemporary work in deep reinforcement learning (DRL) agents are trained in simulated environments. Not only are simulated environments fast and inexpensive they are also ‘safe’. By contrast training in a real world environment (using robots for example) is not only slow and costly but actions can also result in irreversible damage either to the environment or to the agent (robot) itself. In this paper we consider taking advantage of the inherent safety in computer simulation by extending the Deep Q-Network (DQN) algorithm with an ability to measure and take risk. In essence we propose a novel DRL algorithm that encourages risk-seeking behaviour to enhance information acquisition during training. We demonstrate the merit of the exploration heuristic by (i) arguing that our risk estimator implicitly contains both parametric uncertainty and inherent uncertainty of the environment which are propagated back through Temporal Difference error across many time steps and (ii) evaluating our method on three games in the Atari domain and showing that the technique works well on Montezuma’s Revenge a game that epitomises the challenge of sparse reward.
433,Elias2018, The VOCADOM research project aims to design a new technology (voice control usable at home) to encourage the well-being and autonomy of the elderly with loss of autonomy at home. The user-centered design that we apply must implement methods adapted to these profiles. Which methods are appropriate for this profile to understand their real problems and needs at home and which methods can be mobilised that these people can project themselves into the use of an innovative device.
434,Shih2018, Advances in data analytics and human computation are transforming how researchers conduct science in domains like bioinformatics computational social science and digital humanities. However data analytics requires significant programming knowledge or access to technical experts while human computation requires in-depth knowledge of crowd management and is error-prone due to lack of scientific domain expertise. The goal of this research is to empower a broader range of scientists and end-users to conduct data analytics by adopting the End-User DevelopmentEnd-User Development (EUD) models commonly found in today’s commercial software platforms like Microsoft Excel Wikipedia and WordPress. These EUD platforms enable people to focus on producing content rather than struggling with a development environment and new programming syntax or relying on disciplinary non-experts for essential technical help. This research explores a similar paradigm for scientists and end-users that can be thought of as End-User Data AnalyticsEnd-User Data Analytics (EUDA) or Transparent Machine LearningTransparent machine learning (TML).
435,Starkey2018, The chapter presents a summary of developments in relation to Information and Communication Technologies (ICT) in educational policies in Australia and New Zealand. It provides important insights into how policy decisions are influenced by the national and local contexts in those countries and how policies are influencing the integration of ICT in teaching and learning in primary and secondary education. Trends and developments are identified through the analysis of policy documents and published research to highlight differences and similarities between the two countries. The chapter indicates that both countries have introduced digital technologies in schooling in ways which reflect learning with digital technologies and are shown to be influenced by international trends. Both Australia and New Zealand have been developing curriculum to guide the teaching of students to learn about digital technologies through computational thinking and coding. A key challenge continues to be providing equitable access to opportunities for teachers and students to integrate digital technologies effectively into teaching and learning. However New Zealand has a nationally funded policy of providing universal access to Broadband Internet to all schools which research has found to correlate with improved achievement in the primary years and similar infrastructure priorities were evident in Australia. A further challenge is building teacher capability with and dispositions about digital technologies for effective implementation of policies in both countries.
436,Wang2018r, Word vector and topic model can help retrieve information semantically to some extent. However there are still many problems. (1) Antonyms share high similarity when clustering with word vectors. (2) Number of all kinds of name entities such as person name location name and organization name is infinite while the number of one specific name entity in corpus is limited. As the result the vectors for these name entities are not fully trained. In order to overcome above problems this paper proposes a word vector computation model based on implicit expression. Words with the same meaning are implicitly expression based on dictionary and part of speech. With the implicit expression the sparsity of corpus is reduced and word vectors are trained deeper.
437,Bosmans2018, With the rise of internet-connected devices a novel approach for application testing and validation is required. This paper explores the possibility to test hyper-scale Internet of Things environments using state-of-the-art simulation techniques. More specifically a Software In The Loop simulation (SIL) method is introduced which enables the interaction between virtual simulated models and actual real-life Internet of Things devices. Furthermore we present Acsim a custom hyper scalable and distributed agent-based simulation framework to test and validate Internet Of Things applications. Finally a use-case developed in the Acsim framework is presented to demonstrate both the SIL and hyper-scalable capabilities.
438,Austin2018, This chapter outlines the government’s view of national policy for S&T and industrial development in the field of cybersecurity. It draws on a roadmap for S&T development developed in 2011 by the Chinese Academy of Sciences. It documents vigorous leadership engagement in these issues beginning in 2016 and their policy imperative of indigenization. It discussed the emergence of the private sector in cybersecurity relative to the better established state-owned sector.
439,Foster2018, The objective of this design-based research study was to develop implement and refine Philadelphia Land Science (PLS) an interactive web-based experience designed to support learning framed as identity exploration over time leading to identity change around environmental science and urban planning careers. PLS was developed using Projective Reflection (PR) and tested with high school students at a science museum in Philadelphia as part of a larger on-going study funded by the National Science Foundation (Foster 2014). Projective Reflection (PR) frames learning as identity exploration and change to inform the design of games and game-based learning curricula to facilitate intentional change in learners’ (a) knowledge (b) interest and valuing (c) self-organization and self-control and d) self-perceptions and self-definitions in academic domains/careers. Change is tracked from a learner’s initial current self through exploration of possible selves (measured repeatedly) to a learner’s new self at a desired specific end-point (Shah et al. 2017). PLS was constructed through the modification of the virtual internship Land Science and capitalized on the strengths of its design features which were informed by the Epistemic Frames Theory (Shaffer 2006). The paper introduces two iterations of PLS and concludes with implications for design and implementation of games for facilitating identity change. Implications are discussed for advancing research on learning and identity in immersive virtual environments.
440,Lughofer2018, We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria the likelihood increases (1) of gaining more funded insights into the behavior of the system and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new hybrid interactive model building paradigm which is based on subjective knowledge versus objective data and thus integrates the “expert-in-the-loop” aspect.
441,Almeida2018, Traffic lights detection and recognition research has grown every year. Time is coming when autonomous vehicle can navigate in urban roads and streets and intelligent systems aboard those cars would have to recognize traffic lights in real time. This article proposes a traffic light recognition (TLR) device prototype using a smartphone as camera and processing unit that can be used as a driver assistance. A TLR device has to be able to visualize the traffic scene from inside of a vehicle generate stable images and be protected from adverse conditions. To validate this layout prototype a dataset was built and used to test an algorithm that uses an adaptive background suppression filter (AdaBSF) and Support Vector Machines (SVMs) to detect traffic lights. The application of AdaBSF and subsequent classification with SVM to the dataset achieved 100% precision rate and recall of 65%. Road testing shows that the TLR device prototype meets the requirements to be used as a driver assistance device.
442,Curteanu2018, Different regression algorithms are applied for predicting the sublimation rate of naphthalene in various working conditions: time temperature trainer rate and shape of the sample. The original Large Margin Nearest Neighbor Regression (LMNNR) algorithm is applied and its performance is compared to other well-established regression algorithms such as support vector regression multilayer perceptron neural networks classical k-nearest neighbor random forest and others. The experimental results obtained show that the LMNNR algorithm provides better results than the other regression algorithms.
443,Laere2018, A literature study has identified the major impacts of important design choices in simulation models and simulation-games that model critical infrastructure resilience. The four major groups of design choices discussed in this article are: (1) the chosen learning goal (system understanding or collaboration training) (2) realism and time scale of the scenario (3) design of player roles and communication rules (4) number of action alternatives replay-ability and richness of performance feedback while playing. Researchers and practitioners who build simulation-games for studying critical infrastructure resilience can use the accumulated insights on these four aspects to improve the quality of their game design and the quality of the simulation models the game participants interact with.
444,Hafford-Letchfield2018, Learning for the Fourth Age (L4A) is a social enterprise which recruits trains places and matches volunteers (‘learning mentors’) to older people living in care settings or domiciliary settings. We report on the findings of an independent evaluation drawing on funding from the Big Lottery Silver Dreams programme. Our paper focuses on key findings from the generative aspects of L4A’s work by highlighting the rich experiences of ‘learning mentors’ who were unexpected mutual beneficiaries of L4A’s work. This paper aims to stimulate further debate about the complexity of the landscape in which members of the community interact with opportunities to volunteer. In addition the challenge to capitalise on their contributions contravening traditional notions about volunteering embedded within policy.
445,Do2018, Split-gate embedded flash memory technology has been around for a couple of decades and has become a de facto standard for embedded products such as microcontrollers and smart cards. The majority of the large microcontroller and smartcard chip-makers and a series of fabless companies are now using some form of a split-gate embedded flash-memory technology because of its advantages in power performance and cost compared with traditional EEPROMElectrically Erasable Programmable Read Only Memory (EEPROM) or stacked-gate solutions. This chapter covers the fundamentals of split-gate embedded flash memories with an emphasis on SST’s widely adopted SuperFlash  memory technology as an example to demonstrate the benefits of a split-gate embedded flash-memory technologies. The fundamentals of SuperFlash technology design reliability and scalability are discussed in detail in various sections which would provide a detailed understanding of a split-gate embedded flash-memory technology.
446,Bouchrika2018, In spite of the increasing concerns raised by privacy advocates against the intrusive deployment of large scale surveillance cameras the research community has progressed with a remarkable pace into the area of smart visual surveillance. The automation for surveillance systems becomes an obligation to avoid human errors and ensure an efficient strategy for tackling crimes and preventing further terrorist attacks. In this research article we survey the recent studies in computer vision on gait recognition for covert identification and its application for surveillance scenarios and forensic investigation. The integration of biometric technologies into surveillance systems is a major step milestone to improve the automation process in order to recognize criminal offenders and track them across different places. The suitability of gait biometrics for surveillance applications emerges from the fact that the walking pattern can be captured and perceived from a distance even with poor resolution video as opposed to other biometric modalities which their performance deteriorates in surveillance scenarios.
447,Oppitz2018, Peer-to-peer cognitive computing and quantum computing seem to be the major disruptive trends for the near and distant future. Given that they are the candidates for disruption in economy technology and society we will analyze these three paradigms in more detail.
448,Kwok2018, Life-long learning and whole person development are becoming more critical than just academic performance particularly in career development perspective. Employers are looking for soft skills more than hard skills from employees. It is one of the responsibilities of universities to develop students’ work readiness. A smart university as part of a smart city is no longer limited to provide technologies inside and outside classrooms. In this chapter we discuss how a smart university may facilitate self-regulated learning of learners through the introduction of personal development e-Portfolio which assists learners in planning their development path and reflecting upon their own learning. An implementation example in the City University of Hong Kong is reviewed. Also the way of extending it to lifelong and professional development is discussed.
449,Docka-Filipek2018, I argue the use of instructors’ personal narratives and life experiences via self-disclosure in the classroom is a uniquely effective pedagogical strategy for teaching students about social class inequality as well as intersecting oppressions as they operate under late capitalism. Instructional strategies that include recounting the instructor’s significant life experiences may for example focus on perspective shifts (such as from colorblindness to racial literacy) the onset of sociological consciousness (or learning to use the “sociological imagination” to understand one’s own life trajectory) or especially telling encounters between the body and oppressive structures. Altogether such strategies may serve as powerful antidotes to student subscription to toxic meritocratic ideologies student tendencies to understand the behavior of the poor as inherently criminal or amoral and the widespread mystification of the causes of contemporary inequality. However recounting personal experience in the sociological classroom must always be contextualized with data on larger institutional patterns else one risks what Chimimanda Ngozi Adichie calls “the danger of a single story” that threatens to flatten complex lives into stereotype. Additionally pedagogical methods involving classroom self-disclosure may entail unique professional risks or other detriments for instructors who are members of groups historically underrepresented in the academy. For example students may perceive instructor “bias” they may infer a partisan political agenda on the part of their instructor or they may feel their otherwise unpopular views on contentious subjects silenced in the classroom. Despite such risks most potentially problematic outcomes may be mitigated with the use of a handful of specific and conscientious corrective pedagogical strategies. Overall I believe each instructor must decide for themselves how much self-disclosure is effective or comfortable given unique institutional circumstances emotional histories and strengths and vulnerabilities we all bring with us when we enter the classroom. I present data on student assessment of the costs and benefits of pedagogical strategies of self-disclosure so that interested sociology instructors may make a more informed decision about the appropriate most beneficial role of their personal experiences in the classroom.
450,Cattaneo2018, The combination of different learning locations such as schools and workplaces is a core concern in nowadays discussion regarding the requirements of digitalized and globalized economies. The alternation of school-based and work-based tracks is also a key characteristic of many well-developed dual vocational education and training (VET) systems for example Austria Denmark Germany and Switzerland. However even in these systems it is not always easy for the actors to conceive and fully understand the relationships among the different learning locations and there still persists a gap between learning at school and learning at the workplace. The aim of this chapter is to show how technologies in general and visual technologies in particular can help to bridge this gap. More specifically we adopt a connectivity and boundary-crossing perspective of teaching and learning in VET. Against this background we outline a VET-specific pedagogical model the so-called Erfahrraum (space for reflection on experiences) which views technologies as boundary objects that could support teaching learning and communication across the sites. In addition we exemplify how this model was put into practice in initial VET of chefs butchers and scrub nurses. The chapter ends with a discussion in which we will summarize our considerations outline key learnings and provide an outlook for future research and development.
451,Spiro2018, The chapter offers an overview of key insights emerging from the five case studies. We return to our central findings and invite readers to review them in the light of their own context and through the lens of school leader teacher and researcher. The chapter concludes with comments from both authors tracing their starting point with each school and their insights at the end of the research process as they look towards the future.
453,Bosman2018, The purpose of this section is to provide an easy understanding of how to incorporate the entrepreneurial mindset into the engineering curriculum.
454,Tawadros2018, In this article we begin by defining key terms such  virtual reality“ and  avatars“ and describe the  ProReal“ system. We set out the rationale behind the design of  ProReal“   a virtual world avatar-based software designed for coaching. We describe how a coach would use  ProReal“ showing its potential to accelerate learning insight and profound change. We suggest that coaching in  ProReal“ requires practices that privilege the  client-in-charge“ and working in a  trialogue“. That is in a three-way conversation between the client the world the client sets out in  ProReal“ and the coach. The coach acts as a facilitator enabling the client to use the softwrae render thoughts feelings and relationships in a visual world enriched by narrative and symbol; to find alternative ways of perceiving thinking and feeling - rehearsing new behaviours in the virtual world before taking action. We present the science that underpins  ProReal“ and emerging evidence about its effectiveness. Finally we propose a brave new world of coaching where the avatar is prominent and which democratises and enriches work with clients in hitherto unimagined ways.
457,Chowdhury2017, Wireless sensor networks can be deployed in remote areas for monitoring rainforest bio-diversity detecting forest fire or even surveillance. In such remote monitoring applications sensor nodes are deployed in unattended environments that make them vulnerable to different kind of failures. Hence it is extremely important to perform a reliability analysis as a precursor to WSN deployment. This paper investigates reliability analysis and makes two contributions. First an algorithm based on ordered binary decision diagram is proposed. Second an algorithm based on Monte Carlo simulation is proposed to compute the reliability considering both individual component and common cause failure. There are some earlier works that focuses on either of the failure types but not both. In this work battery model of the nodes are taken into account to have a realistic estimate of node reliability. The proposed model can be readily extended for any outdoor deployment scenario to assess reliability before actual deployment and hence explore meaningful insight regarding network design for instance identifying critical failure sequences. The results of both algorithms for benchmark network configurations are validated for similar setting against existing literature. The results show that with more nodes network reliability gradually reaches a steady state (250 onwards) for a stable environment (low individual component failure due to transient errors) subject to moderate common cause failure probability (30%).
458,Willcocks2017, We present a segmentation software package primarily targeting medical and biological applications with a high level of visual feedback and several usability enhancements over existing packages. Specifically we provide a substantially faster GPU implementation of the local Gaussian distribution fitting energy model which can segment inhomogeneous objects with poorly defined boundaries as often encountered in biomedical images. We also provide interactive brushes to guide the segmentation process in a semiautomated framework.
459,Li2017, Backlash error occurs in a machining center may lead to a series of changes in the geometry of the components and subsequently deteriorate the overall performance of the equipment. Due to the uncertainty of mechanical wear between kinematic pairs it is challenging to predict backlash error through physical models directly. An alternative method is to leverage data-driven models to map the degradation. This paper proposes a data-driven method for backlash error predication through Deep Belief Network (DBN). The proposed method focuses on the assessment of both current and future geometric errors for backlash error prediction and subsequent maintenance in machining centers. During the process of prognosis a DBN via stacking Restricted Boltzmann Machines is constructed for backlash error prediction. Energy-based models enable DBN to mine information hidden behind highly coupled inputs which makes DBN a feasible method for fault diagnosis and prognosis when the target condition is beyond the historical data. In the experiment to confirm the effectiveness of deep learning for backlash error prediction similar popular regression methods including Support Vector Machine Regression and Back Propagation Neural Network are employed to present a comprehensive comparison in both diagnosis and prognosis. The experimental results show that the performances of all these regression methods are acceptable in the diagnostic stage. In the prognostic stage DBN demonstrates its superiority and significantly outperforms the other models for backlash error prediction in machining centers.
460,Niioka2017, In the field of regenerative medicine tremendous numbers of cells are necessary for tissue/organ regeneration. Today automatic cell-culturing system has been developed. The next step is constructing a non-invasive method to monitor the conditions of cells automatically. As an image analysis method convolutional neural network (CNN) one of the deep learning method is approaching human recognition level. We constructed and applied the CNN algorithm for automatic cellular differentiation recognition of myogenic C2C12 cell line. Phase-contrast images of cultured C2C12 are prepared as input dataset. In differentiation process from myoblasts to myotubes cellular morphology changes from round shape to elongated tubular shape due to fusion of the cells. CNN abstract the features of the shape of the cells and classify the cells depending on the culturing days from when differentiation is induced. Changes in cellular shape depending on the number of days of culture (Day 0 Day 3 Day 6) are classified with 91.3% accuracy. Image analysis with CNN has a potential to realize regenerative medicine industry.
461,Hong2017, Drug-induced liver injury (DILI) presents a significant challenge to drug development and regulatory science. The FDA’s Liver Toxicity Knowledge Base (LTKB) evaluated >1000 drugs for their likelihood of causing DILI in humans of which >700 drugs were classified into three categories (most-DILI less-DILI and no-DILI). Based on this dataset we developed and compared 2-class and 3-class DILI prediction models using the machine learning algorithm of Decision Forest (DF) with Mold2 structural descriptors. The models were evaluated through 1000 iterations of 5-fold cross-validations 1000 bootstrapping validations and 1000 permutation tests (that assessed the chance correlation). Furthermore prediction confidence analysis was conducted which provides an additional parameter for proper interpretation of prediction results. We revealed that the 3-class model not only had a higher resolution to estimate DILI risk but also showed an improved capability to differentiate most-DILI drugs from no-DILI drugs in comparison with the 2-class DILI model. We demonstrated the utility of the models for drug ingredients with warnings very recently issued by the FDA. Moreover we identified informative molecular features important for assessing DILI risk. Our results suggested that the 3-class model presents a better option than the binary model (which most publications are focused on) for drug safety evaluation.
462,Wang2017, Face recognition has great theory research and application value. It is a very complicated problem which often suffers from variations in lighting condition facial expression head pose glasses background and so on. This paper realizes an effective recognition for 108 face images of 27 individuals in complex background through a biological inspired emergent developmental network (DN). To decrease the influence of complex background on the recognition of the foreground object another biological inspired mechanism synapse maintenance which can dynamically determine which synapse should be removed weaken or strengthened is introduced to enhance the image recognition rate. To prevent the quick decay of the learning rate with the increasement of the neuron age simulating the learning principle of the human brain a new learning rate is proposed to determine the neuron learning process. Moreover to exploit the network resource efficiently neuron regenesis mechanism is designed to regulate the neuron resource dynamically. First we design two kinds of neuron states to depict the neuron action then simulating the work mechanism of the human brain to produce new neurons continuously to learn new knowledge we design the neuron regenesis mechanism to activate the suppressed old neurons in the developmental network to regenerate and learn new feature thus to enhance the network usage efficiency. In order to demonstrate the effect of DN on face recognition we compare and analyze the performances of DN with/without synapse maintenance mechanism with the neuron regenesis mechanism. Experiment results in semi-constrained dataset and unconstrained dataset illustrate that DN with the synapse maintenance and neuron regenesis mechanism can effectively improve the face recognition rate in complex background. Further comparing the performance of DN with some state-of-the-art algorithms experimental results demonstrate the superior performance of the proposed method.
463,Liao2017, The hesitant fuzzy linguistic term set (HFLTS) has gained great success as it can be used to represent several linguistic terms or comparative linguistic expressions together with some context-free grammars. This new approach has enabled the analysis and computing of linguistic expressions with uncertainties and opened the door for the possibility to develop more comprehensive and powerful decision theories and methods based on linguistic knowledge. Lots of new approaches and proposals for decision-making problems have been proposed to overcome the limitations of previous linguistic decision-making approaches. Now and in the future decision-making methodologies and algorithms with hesitant fuzzy linguistic models would be a quite promising research line representing a high-quality breakthrough in this topic. To facilitate the study on HFLTS theory this paper makes a state-of-the-art survey on HFLTSs based on the 134 selected papers from Web of Sciences published from January 2012 to October 2017. We justify the motivation definitions operations comparison methods and measures of HFLTSs. We also summarize the different extensions of HFLTSs. The studies on multiple criteria decision making (MCDM) with HFLTSs in terms of aggregation operators and MCDM methods are clearly reviewed. We also conduce some overviews on decision making with hesitant fuzzy linguistic preference relations. The applications research challenges and future directions are also given.
464,Vlot2017, Selectivity is an important attribute of effective and safe drugs and prediction of in vivo target and tissue selectivity would likely improve drug development success rates. However a lack of understanding of the underlying (pharmacological) mechanisms and availability of directly applicable predictive methods complicates the prediction of selectivity. We explore the value of combining physiologically based pharmacokinetic (PBPK) modeling with quantitative structure-activity relationship (QSAR) modeling to predict the influence of the target dissociation constant (KD) and the target dissociation rate constant on target and tissue selectivity. The KD values of CB1 ligands in the ChEMBL database are predicted by QSAR random forest (RF) modeling for the CB1 receptor and known off-targets (TRPV1 mGlu5 5-HT1a). Of these CB1 ligands rimonabant CP-55940 and Δ8-tetrahydrocanabinol one of the active ingredients of cannabis were selected for simulations of target occupancy for CB1 TRPV1 mGlu5 and 5-HT1a in three brain regions to illustrate the principles of the combined PBPK-QSAR modeling. Our combined PBPK and target binding modeling demonstrated that the optimal values of the KD and koff for target and tissue selectivity were dependent on target concentration and tissue distribution kinetics. Interestingly if the target concentration is high and the perfusion of the target site is low the optimal KD value is often not the lowest KD value suggesting that optimization towards high drug-target affinity can decrease the benefit-risk ratio. The presented integrative structure-pharmacokinetic-pharmacodynamic modeling provides an improved understanding of tissue and target selectivity.
465,Lee2017, Reliable electroencephalography (EEG) signatures of transitions between consciousness and unconsciousness under anaesthesia have not yet been identified. Herein we examined network changes using graph theoretical analysis of high-density EEG during patient-titrated propofol-induced sedation. Responsiveness was used as a surrogate for consciousness. We divided the data into five states: baseline transition into unresponsiveness unresponsiveness transition into responsiveness and recovery. Power spectral analysis showed that delta power increased from responsiveness to unresponsiveness. In unresponsiveness delta waves propagated from frontal to parietal regions as a traveling wave. Local increases in delta connectivity were evident in parietal but not frontal regions. Graph theory analysis showed that increased local efficiency could differentiate the levels of responsiveness. Interestingly during transitions of responsive states increased beta connectivity was noted relative to consciousness and unconsciousness again with increased local efficiency. Abrupt network changes are evident in the transitions in responsiveness with increased beta band power/connectivity marking transitions between responsive states while the delta power/connectivity changes were consistent with the fading of consciousness using its surrogate responsiveness. These results provide novel insights into the neural correlates of these behavioural transitions and EEG signatures for monitoring the levels of consciousness under sedation.
466,Powles2017, Data-driven tools and techniques particularly machine learning methods that underpin artificial intelligence offer promise in improving healthcare systems and services. One of the companies aspiring to pioneer these advances is DeepMind Technologies Limited a wholly-owned subsidiary of the Google conglomerate Alphabet Inc. In 2016 DeepMind announced its first major health project: a collaboration with the Royal Free London NHS Foundation Trust to assist in the management of acute kidney injury. Initially received with great enthusiasm the collaboration has suffered from a lack of clarity and openness with issues of privacy and power emerging as potent challenges as the project has unfolded. Taking the DeepMind-Royal Free case study as its pivot this article draws a number of lessons on the transfer of population-derived datasets to large private prospectors identifying critical questions for policy-makers industry and individuals as healthcare moves into an algorithmic age.
467,Vayena2017, Empirical evidence suggests that while people hold the capacity to control their data in high regard they increasingly experience a loss of control over their data in the online world. The capacity to exert control over the generation and flow of personal information is a fundamental premise to important values such as autonomy privacy and trust. In healthcare and clinical research this capacity is generally achieved indirectly by agreeing to specific conditions of informational exposure. Such conditions can be openly stated in informed consent documents or be implicit in the norms of confidentiality that govern the relationships of patients and healthcare professionals. However with medicine becoming a data-intense enterprise informed consent and medical confidentiality as mechanisms of control are put under pressure. In this paper we explore emerging models of informational control in data-intense healthcare and clinical research which can compensate for the limitations of currently available instruments. More specifically we discuss three approaches that hold promise in increasing individual control: the emergence of data portability rights as means to control data access new mechanisms of informed consent as tools to control data use and finally new participatory governance schemes that allow individuals to control their data through direct involvement in data governance. We conclude by suggesting that despite the impression that biomedical big data diminish individual control the synergistic effect of new data management models can in fact improve it.
468,Sun2017, It is highly likely that to achieve full human machine symbiosis truly intelligent cognitive systems human-like (or even beyond) may have to be developed first. Such systems should not only be capable of performing human-like thinking reasoning and problem solving but also be capable of displaying human-like motivation emotion and personality. In this opinion article I will argue that such systems are indeed possible and needed to achieve true and full symbiosis with humans. A computational cognitive architecture (named Clarion) is used in this article to illustrate in a preliminary way what can be achieved in this regard. It is shown that Clarion involves complex structures representations and mechanisms and is capable of capturing human cognitive performance (including skills reasoning memory and so on) as well as human motivation emotion personality and other relevant aspects. It is further argued that the cognitive architecture can enable and facilitate true human machine symbiosis.
469,Lauschke2017, Much of the inter-individual variability in drug efficacy and risk of adverse reactions is due to polymorphisms in genes encoding proteins involved in drug pharmacokinetics and pharmacodynamics or immunological responses. Pharmacogenetic research has identified a multitude of gene-drug response associations which have resulted in genetically guided treatment and dosing decisions to yield a higher success rate of pharmacological treatment. The rapid technological developments for genetic analyses reveal that the number of genetic variants with importance for drug action is much higher than previously thought and that a true personalized prediction of drug response requires attention to millions of rare mutations. Here we review the evolutionary background of genetic polymorphisms in drug-metabolizing enzymes provide some important examples of current use of pharmacogenomic biomarkers and give an update of germline and somatic genome biomarkers that are in use in drug development and clinical practice. We also discuss the current technology development with emphasis on complex genetic loci review current initiatives for validation of pharmacogenomic biomarkers and present scenarios for the future taking rare genetic variants into account for a true personalized genetically guided drug prescription. We conclude that pharmacogenomic information for patient stratification is of value to tailor optimized treatment regimens particularly in oncology. However the routine use of pharmacogenomic biomarkers in clinical practice in other therapeutic areas is currently sparse and the prospects of its future implementation are being scrutinized by different international consortia.
470,Wang2017a, Prostate cancer (PCa) is a major cause of death since ancient time documented in Egyptian Ptolemaic mummy imaging. PCa detection is critical to personalized medicine and varies considerably under an MRI scan. 172 patients with 2602 morphologic images (axial 2D T2-weighted imaging) of the prostate were obtained. A deep learning with deep convolutional neural network (DCNN) and a non-deep learning with SIFT image feature and bag-of-word (BoW) a representative method for image recognition and analysis were used to distinguish pathologically confirmed PCa patients from prostate benign conditions (BCs) patients with prostatitis or prostate benign hyperplasia (BPH). In fully automated detection of PCa patients deep learning had a statistically higher area under the receiver operating characteristics curve (AUC) than non-deep learning (P 0.0007 < 0.001). The AUCs were 0.84 (95% CI 0.78 0.89) for deep learning method and 0.70 (95% CI 0.63 0.77) for non-deep learning method, respectively. Our results suggest that deep learning with DCNN is superior to non-deep learning with SIFT image feature and BoW model for fully automated PCa patients differentiation from prostate BCs patients. Our deep learning method is extensible to image modalities such as MR imaging, CT and PET of other organs.
471,Reinkensmeyer2017, Over 50 million United States citizens (1 in 6 people in the US) have a developmental acquired or degenerative disability. The average US citizen can expect to live 20% of his or her life with a disability. Rehabilitation technologies play a major role in improving the quality of life for people with a disability yet widespread and highly challenging needs remain. Within the US a major effort aimed at the creation and evaluation of rehabilitation technology has been the Rehabilitation Engineering Research Centers (RERCs) sponsored by the National Institute on Disability Independent Living and Rehabilitation Research. As envisioned at their conception by a panel of the National Academy of Science in 1970 these centers were intended to take a “total approach to rehabilitation” combining medicine engineering and related science to improve the quality of life of individuals with a disability. Here we review the scope achievements and ongoing projects of an unbiased sample of 19 currently active or recently terminated RERCs. Specifically for each center we briefly explain the needs it targets summarize key historical advances identify emerging innovations and consider future directions. Our assessment from this review is that the RERC program indeed involves a multidisciplinary approach with 36 professional fields involved although 70% of research and development staff are in engineering fields 23% in clinical fields and only 7% in basic science fields; significantly 11% of the professional staff have a disability related to their research. We observe that the RERC program has substantially diversified the scope of its work since the 1970’s addressing more types of disabilities using more technologies and in particular often now focusing on information technologies. RERC work also now often views users as integrated into an interdependent society through technologies that both people with and without disabilities co-use (such as the internet wireless communication and architecture). In addition RERC research has evolved to view users as able at improving outcomes through learning exercise and plasticity (rather than being static) which can be optimally timed. We provide examples of rehabilitation technology innovation produced by the RERCs that illustrate this increasingly diversifying scope and evolving perspective. We conclude by discussing growth opportunities and possible future directions of the RERC program.
472,Liang2017, Software multimedia anomaly detection model based on neural network and optimization driven support vector machine is discussed in this paper. For multimedia information most traditional information security technology has its limitations. For example the limitation of the encryption technology is that on the one hand the encrypted files resulting from the incomprehension of attributes interfere with the transfer of multimedia information. On the other hand the encrypted multimedia information is likely to attract the attacker’s curiosity and attention and is likely to be cracked and once it is cracked the system loses control of the information. To deal with these challenges this study integrates soft computing techniques to finalize the enhanced multimedia anomaly detection model. With respect to the neural network a random system with random factors is referred to as a random system. These practical systems are generally described and modeled by stochastic differential equations. In this study we combined the double support vector machine and decision tree support vector machine to construct a new double support vector machine decision tree classifier. Kernel function and convex optimization were integrated to guarantee an optimal solution. Experimental results demonstrated the robustness of the model compared with other recent techniques.
473,Fan2017, During cellular reprogramming the mesenchymal-to-epithelial transition is accompanied by changes in morphology which occur prior to iPSC colony formation. The current approach for detecting morphological changes associated with reprogramming purely relies on human experiences which involve intensive amounts of upfront training human error with limited quality control and batch-to-batch variations. Here we report a time-lapse-based bright-field imaging analysis system that allows us to implement a label-free non-invasive approach to measure morphological dynamics. To automatically analyse and determine iPSC colony formation a machine learning-based classification segmentation and statistical modelling system was developed to guide colony selection. The system can detect and monitor the earliest cellular texture changes after the induction of reprogramming in human somatic cells on day 7 from the 20 24 day process. Moreover after determining the reprogramming process and iPSC colony formation quantitatively a mathematical model was developed to statistically predict the best iPSC selection phase independent of any other resources. All the computational detection and prediction experiments were evaluated using a validation dataset and biological verification was performed. These algorithm-detected colonies show no significant differences (Pearson Coefficient) in terms of their biological features compared to the manually processed colonies using standard molecular approaches.
474,Lawn2017, BackgroundE-learning involves delivery of education through Information and Communication Technology (ITC) using a wide variety of instructional designs including synchronous and asynchronous formats. It can be as effective as face-to-face training for many aspects of health professional training. There are however particular practices and skills needed in providing patient self-management support such as partnering with patients in goal-setting which may challenge conventional practice norms. E-learning for the delivery of self-management support (SMS) continuing education to existing health professionals is a relatively new and growing area with limited studies identifying features associated with best acquisition of skills in self-management support.MethodsAn integrative literature review examined what is known about e-learning for self-management support. This review included both qualitative and quantitative studies that focused on e-learning provided to existing health professionals for their continuing professional development. Papers were limited to those published in English between 2006 and 2016. Content analysis was used to organize and focus and describe the findings.ResultsThe search returned 1505 articles with most subsequently excluded based on their title or abstract. Fifty-two full text articles were obtained and checked with 42 excluded because they did not meet the full criteria. Ten peer-reviewed articles were included in this review. Seven main themes emerged from the content analysis: participants and professions; time; package content; guiding theoretical framework; outcome measures; learning features or formats; and learning barriers. These themes revealed substantial heterogeneity in instructional design and other elements of e-learning applied to SMS indicating that there is still much to understand about how best to deliver e-learning for SMS skills development.ConclusionsFew e-learning approaches meet the need for high levels of interactivity reflection practice and application to practice for health professionals learning to deliver effective SMS. Findings suggest that the context of SMS for patients with chronic condition matters to how health professional training is delivered to ensure partnership and person-centred care. Further creative approaches and their rigorous evaluation are needed to deliver completely online learning in this space. Blended learning that combines e-learning and face-to-face methods is suggested to support SMS skills development for health professionals.
475,Myers2017, The accurate assessment of a patient’s risk of adverse events remains a mainstay of clinical care. Commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history presenting signs and symptoms and lab values. More sophisticated methods such as Artificial Neural Networks (ANN) form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data yielding classifiers with improved performance. Using two cohorts consisting of patients admitted with a non-ST-segment elevation acute coronary syndrome we constructed an ANN that identifies patients at high risk of cardiovascular death (CVD). The ANN was trained and tested using patient subsets derived from a cohort containing 4395 patients (Area Under the Curve (AUC) 0.743) and validated on an independent holdout set containing 861 patients (AUC 0.767). The ANN 1-year Hazard Ratio for CVD was 3.72 (95% confidence interval 1.04 14.3) after adjusting for the TIMI Risk Score left ventricular ejection fraction and B-type natriuretic peptide. A unique feature of our approach is that it captures small changes in the ST segment over time that cannot be detected by visual inspection. These findings highlight the important role that ANNs can play in risk stratification.
476,Schindler2017, Computer-based technology has infiltrated many aspects of life and industry yet there is little understanding of how it can be used to promote student engagement a concept receiving strong attention in higher education due to its association with a number of positive academic outcomes. The purpose of this article is to present a critical review of the literature from the past 5 years related to how web-conferencing software blogs wikis social networking sites (Facebook and Twitter) and digital games influence student engagement. We prefaced the findings with a substantive overview of student engagement definitions and indicators which revealed three types of engagement (behavioral emotional and cognitive) that informed how we classified articles. Our findings suggest that digital games provide the most far-reaching influence across different types of student engagement followed by web-conferencing and Facebook. Findings regarding wikis blogs and Twitter are less conclusive and significantly limited in number of studies conducted within the past 5 years. Overall the findings provide preliminary support that computer-based technology influences student engagement however additional research is needed to confirm and build on these findings. We conclude the article by providing a list of recommendations for practice with the intent of increasing understanding of how computer-based technology may be purposefully implemented to achieve the greatest gains in student engagement.
477,Sohn2017, Complicated structures consisting of multi-layers with a multi-modal array of device components i.e. so-called patterned multi-layers and their corresponding circuit designs for signal readout and addressing are used to achieve a macroscale electronic skin (e-skin). In contrast to this common approach we realized an extremely simple macroscale e-skin only by employing a single-layered piezoresistive MWCNT-PDMS composite film with neither nano- micro- nor macro-patterns. It is the deep machine learning that made it possible to let such a simple bulky material play the role of a smart sensory device. A deep neural network (DNN) enabled us to process electrical resistance change induced by applied pressure and thereby to instantaneously evaluate the pressure level and the exact position under pressure. The great potential of this revolutionary concept for the attainment of pressure-distribution sensing on a macroscale area could expand its use to not only e-skin applications but to other high-end applications such as touch panels portable flexible keyboard sign language interpreting globes safety diagnosis of social infrastructures and the diagnosis of motility and peristalsis disorders in the gastrointestinal tract.
478,Shin2017, Precision oncology is described as the matching of the most accurate and effective treatments with the individual cancer patient. Identification of important gene mutations such as BRCA1/2 that drive carcinogenesis helped pave the way for precision diagnosis in cancer. Oncoproteins and their signaling pathways have been extensively studied leading to the development of target-based precision therapies against several types of cancers. Although many challenges exist that could hinder the success of precision oncology cutting-edge tools for precision diagnosis and precision therapy will assist in overcoming many of these difficulties. Based on the continued rapid progression of genomic analysis drug development and clinical trial design precision oncology will ultimately become the standard of care in cancer therapeutics.
479,Ranschaert2017, Good communication is essential in medicine and particularly in radiology. Radiology is a supporting specialty with a mainly consultative function. The service rendered by radiologists can be roughly divided into two distinct but inseparable parts: analyzing the images on the one hand and reporting the findings on the other. The radiology report is the core of the communication by the radiologist (Flanders and Lakhani 2012) and in a way our product the end stage of the workflow.
480,Heinzmann2017, Complex molecular and metabolic phenotypes depict cancers as a constellation of different diseases with common themes. Precision imaging of such phenotypes requires flexible and tunable modalities capable of identifying phenotypic fingerprints by using a restricted number of parameters while ensuring sensitivity to dynamic biological regulation. Common phenotypes can be detected by in vivo imaging technologies and effectively define the emerging standards for disease classification and patient stratification in radiology. However for the imaging data to accurately represent a complex fingerprint the individual imaging parameters need to be measured and analysed in relation to their wider spatial and molecular context. In this respect targeted palettes of molecular imaging probes facilitate the detection of heterogeneity in oncogene-driven alterations and their response to treatment and lead to the expansion of rational-design elements for the combination of imaging experiments. In this Review we evaluate criteria for conducting multiplexed imaging and discuss its opportunities for improving patient diagnosis and the monitoring of therapy.This Review discusses imaging approaches that benefit from the combination of signals and modalities to enhance patient diagnosis and the monitoring of therapy.
481,Ishii2017, This paper undertakes a comparative legal study to analyze the challenges of privacy and personal data protection posed by Artificial Intelligence (“AI”) embedded in Robots and to offer policy suggestions. After identifying the benefits from various AI usages and the risks posed by AI-related technologies I then analyze legal frameworks and relevant discussions in the EU USA Canada and Japan and further consider the efforts of Privacy by Design (“PbD”) originating in Ontario Canada. While various AI usages provide great convenience many issues including profiling discriminatory decisions lack of transparency and impeding consent have emerged. The unpredictability arising from the AI machine learning function poses further difficulties which have only been partially addressed by legal frameworks in the aforementioned jurisdictions. However analyzing the relevant discussions yielded several suggestions. The first priority is adopting PbD as the most flexible soft-legal and preferable approach toward AI-oriented issues. Implementing PbD will protect individual privacy and personal data without specific efforts and achieve both the development of AI and the advancement of privacy and personal data protection. Technical measures that can adapt to an individual’s dynamic choices according to the “context” should be further developed. Furthermore alternative technical measures including those to solve the “algorithmic black box” or achieve differential privacy warrant thorough examination. If AI surpasses human intelligence a terminating function such as a “kill switch” will be the last resort to preserve individual choice. Despite numerous difficulties we must prepare for the coming AI-prevalent society by taking a flexible approach.
482,Radiya-Dixit2017, Misclassification of breast lesions can result in either cancer progression or unnecessary chemotherapy. Automated classification tools are seen as promising second opinion providers in reducing such errors. We have developed predictive algorithms that automate the categorization of breast lesions as either benign usual ductal hyperplasia (UDH) or malignant ductal carcinoma in situ (DCIS). From diagnosed breast biopsy images from two hospitals we obtained 392 biomarkers using Dong et al.’s (2014) computational tools for nuclei identification and feature extraction. We implemented six machine learning models and enhanced them by reducing prediction variance extracting active features and combining multiple algorithms. We used the area under the curve (AUC) of the receiver operating characteristic (ROC) curve for performance evaluation. Our top-performing model a Combined model with Active Feature Extraction (CAFE) consisting of two logistic regression algorithms obtained an AUC of 0.918 when trained on data from one hospital and tested on samples of the other a statistically significant improvement over Dong et al.’s AUC of 0.858. Pathologists can substantially improve their diagnoses by using it as an unbiased validator. In the future our work can also serve as a valuable methodology for differentiating between low-grade and high-grade DCIS.
483,Cha2017, Cross-sectional X-ray imaging has become the standard for staging most solid organ malignancies. However for some malignancies such as urinary bladder cancer the ability to accurately assess local extent of the disease and understand response to systemic chemotherapy is limited with current imaging approaches. In this study we explored the feasibility that radiomics-based predictive models using pre- and post-treatment computed tomography (CT) images might be able to distinguish between bladder cancers with and without complete chemotherapy responses. We assessed three unique radiomics-based predictive models each of which employed different fundamental design principles ranging from a pattern recognition method via deep-learning convolution neural network (DL-CNN) to a more deterministic radiomics feature-based approach and then a bridging method between the two utilizing a system which extracts radiomics features from the image patterns. Our study indicates that the computerized assessment using radiomics information from the pre- and post-treatment CT of bladder cancer patients has the potential to assist in assessment of treatment response.
484,Lepri2017, The combination of increased availability of large amounts of fine-grained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed prejudice fatigue or hunger. However algorithmic decision-making has been criticized for its potential to enhance discrimination information and power asymmetry and opacity. In this paper we provide an overview of available technical solutions to enhance fairness accountability and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers practitioners policy-makers and citizens to co-develop deploy and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so we describe the Open Algortihms (OPAL) project as a step towards realizing the vision of a world where data and algorithms are used as lenses and levers in support of democracy and development.
485,Waldstein2017, Vitreomacular adhesion (VMA) represents a prognostic biomarker in the management of exudative macular disease using anti-vascular endothelial growth factor (VEGF) agents. However manual evaluation of VMA in 3D optical coherence tomography (OCT) is laborious and data on its impact on therapy of retinal vein occlusion (RVO) are limited. The aim of this study was to (1) develop a fully automated segmentation algorithm for the posterior vitreous boundary and (2) to study the effect of VMA on anti-VEGF therapy for RVO. A combined machine learning/graph cut segmentation algorithm for the posterior vitreous boundary was designed and evaluated. 391 patients with central/branch RVO under standardized ranibizumab treatment for 6/12 months were included in a systematic post-hoc analysis. VMA (70%) was automatically differentiated from non-VMA (30%) using the developed method combined with unsupervised clustering. In this proof-of-principle study eyes with VMA showed larger BCVA gains than non-VMA eyes and received a similar number of retreatments. However, this association diminished after adjustment for baseline BCVA, also when using more fine-grained VMA classes. Our study illustrates that machine learning represents a promising path to assess imaging biomarkers in OCT.
486,Tao2017, In order to effectively deal with the APT and 0 day attacks a new classified protection model of information system is proposed by combining the big data analysis and the threat intelligence technologies. And immune factors network algorithm is proposed based on the classified model. So that the useful information can be actively accessed and extracted from a large number of security information. The consequences of the threat information and the effective measures can be timely analysis and the threat intelligence of classified protection can be timely shared. So the emergency response bulletins and early warning can be timely done.
487,Kim2017, Adoption of Electronic Health Record (EHR) systems has led to collection of massive healthcare data which creates oppor- tunities and challenges to study them. Computational phenotyping offers a promising way to convert the sparse and complex data into meaningful concepts that are interpretable to healthcare givers to make use of them. We propose a novel su- pervised nonnegative tensor factorization methodology that derives discriminative and distinct phenotypes. We represented co-occurrence of diagnoses and prescriptions in EHRs as a third-order tensor and decomposed it using the CP algorithm. We evaluated discriminative power of our models with an Intensive Care Unit database (MIMIC-III) and demonstrated superior performance than state-of-the-art ICU mortality calculators (e.g. APACHE II SAPS II). Example of the resulted phenotypes are sepsis with acute kidney injury cardiac surgery anemia respiratory failure heart failure cardiac arrest metastatic cancer (requiring ICU) end-stage dementia (requiring ICU and transitioned to comfort-care) intraabdominal conditions and alcohol abuse/withdrawal.
488,Tien2017, In an earlier paper (Tien 2015) the author defined the concept of a servgood which can be thought of as a physical good or product enveloped by a services-oriented layer that makes the good smarter or more adaptable and customizable for a particular use. Adding another layer of physical sensors could then enhance its smartness and intelligence especially if it were to be connected with each other or with other servgoods through the Internet of Things. Such sensed servgoods are becoming the products of the future. Indeed autonomous vehicles can be considered the exemplar servgoods of the future; it is about decision informatics and embraces the advanced technologies of sensing (i.e. Big Data) processing (i.e. real-time analytics) reacting (i.e. real-time decision-making) and learning (i.e. deep learning). Since autonomous vehicles constitute a huge quality-of-life disruption it is also critical to consider its policy impact on privacy and security regulations and standards and liability and insurance. Finally just as the Soviet Union inaugurated the space age on October 4 1957 with the launch of Sputnik the first man-made object to orbit the Earth the U. S. has inaugurated an age of automata or autonomous vehicles that can be considered to be the U. S. Sputnik of servgoods with the full support of the U. S. government the U. S. auto industry the U. S. electronic industry and the U.S. higher educational enterprise.
489,Bauder2017, From its infancy in the 1910s healthcare group insurance continues to increase creating a consistently rising burden on the government and taxpayers. The growing number of people enrolled in healthcare programs such as Medicare along with the enormous volume of money in the healthcare industry increases the appeal for and risk of fraudulent activities. One such fraud known as upcoding is a means by which a provider can obtain additional reimbursement by coding a certain provided service as a more expensive service than what was actually performed. With the proliferation of data mining techniques and the recent and continued availability of public healthcare data the application of these techniques towards fraud detection using this increasing cache of data has the potential to greatly reduce healthcare costs through a more robust detection of upcoding fraud. Presently there is a sizable body of healthcare fraud detection research available but upcoding fraud studies are limited. Audit data can be difficult to obtain limiting the usefulness of supervised learning; therefore other data mining techniques such as unsupervised learning must be explored using mostly unlabeled records in order to detect upcoding fraud. This paper is specific to reviewing upcoding fraud analysis and detection research providing an overview of healthcare upcoding and a review of the current data mining techniques used therein.
490,Zheng2017, The long-term goal of artificial intelligence (AI) is to make machines learn and think like human beings. Due to the high levels of uncertainty and vulnerability in human life and the open-ended nature of problems that humans are facing no matter how intelligent machines are they are unable to completely replace humans. Therefore it is necessary to introduce human cognitive capabilities or human-like cognitive models into AI systems to develop a new form of AI that is hybrid-augmented intelligence. This form of AI or machine intelligence is a feasible and important developing model. Hybrid-augmented intelligence can be divided into two basic models: one is human-in-the-loop augmented intelligence with human-computer collaboration and the other is cognitive computing based augmented intelligence in which a cognitive model is embedded in the machine learning system. This survey describes a basic framework for human-computer collaborative hybrid-augmented intelligence and the basic elements of hybrid-augmented intelligence based on cognitive computing. These elements include intuitive reasoning causal models evolution of memory and knowledge especially the role and basic principles of intuitive reasoning for complex problem solving and the cognitive learning framework for visual scene understanding based on memory and reasoning. Several typical applications of hybrid-augmented intelligence in related fields are given.
491,Kumar2017, Recently deep learning approaches have achieved more attention for recognition of species or individual animal using visual features. In this chapter the deep learning-based recognition system is proposed for identification of different cattle based on their primary muzzle point (nose pattern) image pattern characteristics to solve major problem of missed or swapped animal and false insurance claims. The major contributions of the research work are as follows: (1) preparation of muzzle point image database which are not publically available; (2) extraction of the salient set of texture features and representation of muzzle point image of cattle using the deep learning-based convolution neural network and deep belief neural network proposed approaches. The stacked denoising auto-encoder technique is applied to encode the extracted feature of muzzle point images; and (3) experimental results and analysis of proposed approach. Extensive experimental results illustrate that the proposed deep learning approach outperforms state-of-the-art methods for recognition of cattle on muzzle point image database. The efficacy of the proposed deep learning approach is computed under different identification settings. With multiple test galleries rank-1 identification accuracy of 98.99% is achieved.
492,Goh2015, This chapter examines the relationship between problems and learning in a problem-based learning (PBL) environment using the context of a case study of a polytechnic in Singapore. The authors detail how curriculum is problematised (into different types of problems) around a set of desired educational outcomes and explicate how problems are used for the purpose of triggering interest and engagement as well as promoting deep understanding guiding classroom facilitation and informing student assessment in the learning process. Empirical evidence of the effectiveness of problems in learning in three disciplines is shared with suggestions of how the use of problems in learning can be supported by academic policies and professional development for academic staff. An overarching theme of the chapter focuses on how PBL is a method for learning that facilitates deep learning and develops life skills such as collaboration sense-making and problem-solving capabilities for the work place.
493,Lance2015, Brain-computer interface (BCI) technologies or technologies that use online brain signal processing have a great promise to improve human interactions with computers their environment and even other humans. Despite this promise there are no current serious BCI technologies in widespread use due to the lack of robustness in BCI technologies. The key neural aspect of this lack of robustness is human variability which has two main components: (1) individual differences in neural signals and (2) intraindividual variability over time. In order to develop widespread BCI technologies it will be necessary to address this lack of robustness. However it is currently unknown how neural variability affects BCI performance. To accomplish these goals it is essential to obtain data from large numbers of individuals using BCI technologies over considerable lengths of time. One promising method for this is through the use of BCI technologies embedded into games with a purpose (GWAP). GWAP are a game-based form of crowdsourcing which players choose to play for enjoyment and during which the player performs key tasks which cannot be automated but that are required to solve research questions. By embedding BCI paradigms in GWAP and recording neural and behavioral data it should be possible to much more clearly understand the differences in neural signals between individuals and across different time scales enabling the development of novel and increasingly robust adaptive BCI algorithms.
494,Ip2015, Virtual Reality (VR) technologies bring new opportunities and challenges to teaching and learning. Virtual Reality Learning Environment (VRLE) a VR-based interactive environment incorporating instructional design for educational purposes nowadays draws great attention of interdisciplinary scholars. In this paper we first introduce the current status of VRLE-based research studies from various perspectives and then summarise the on-going challenges based on previous research studies and our own experience in this research area.
495,Bonissone2015, We describe the process of building computational intelligence (CIcomputationalintelligence (CI)) models for machine learning (MLmachinelearning (ML)) applications. We use offline metaheuristics to design the models’ run-time architectures and online metaheuristics to control/aggregate the object-level models (base models) in these architectures. CI techniques complement more traditional statistical techniques which are the core of ML for unsupervised and supervised learning. We analyze CI/ML industrial applications in the area of prognostics and health management (PHMprognostichealth management (PHM)) for industrial assets and describe two PHM case studies. In the first case we address anomaly detection for aircraft engines; in the second one we rank locomotives in a fleet according to their expected remaining useful life. Then we illustrate similar CIriskmanagementportfolio optimizationmodelensemble-enabled capabilities as they are applied to risk management for commercial and financial assets. In this context we describe three case studies in insurance underwriting mortgage collateral valuation and portfolio optimization. We explain the current trend favoring the use of model ensemble and fusion over individual models and emphasize the need for injecting diversity during the model generation phase. We present a model agnostic fusion mechanism which can be used with commoditized models obtained from crowdsourcing cloud-based evolution and other sources. Finally we explore research trends and future challenges/opportunities for ML techniques in the emerging context of big data and cloud computing.
496,Biin2015, The ANCESTOR (AborigiNal Computer Education through Storytelling) program was developed to explore computer science as a career option through digital storytelling and address cultural literacy with Aboriginal youth in British Columbia Canada. A team of educators from Camosun College collaborated with post-secondary students secondary school educators and First Nation communities to build and deliver a culturally responsive program. Indigenous knowledge pedagogy and holistic learning practice guided the delivery of the program in First Nation communities middle and secondary schools and First Nation operated secondary schools on the West Coast. The program success can be attributed to the Indigenous values and principles of reciprocity relationship ritual respect relevance reverence and repetition.
497,Chau2015, This chapter presents the project action learning (PAL) implementation experience as real-life cases. PAL uses real-life projects to align individual and team learning with organizational learning (OL). The case company adopted a wavelike approach in its PAL implementation. PAL became the OL vehicle of the case company. An enabling IT-based infrastructure was developed to provide a platform for easy communication knowledge sharing and information interchange. The knowledge gained or generated throughout the PAL-driven OL processes could be captured and retained as retrievable organization knowledge. OL facilitation is another vital pillar for PAL implementation which provides cognitive coaching and coordination to guide the PAL teams especially their new members through the established PAL process.
498,Hazarika2015, Business’s interest for social concern is not new. But businesses core function is always driven towards financial performance. Businesses concern for social causes is evident from the philanthropic exercises that grew along with the growth of industries. To assess the financial and the social performance of businesses a systematic reporting mechanism was very much essential. Social audit is a mechanism for monitoring measuring or appraising social performance largely in economic terms primarily based on improving social activities increasing public relations and Corporate Social Responsibility also evolved as a philanthropic exercise and as consciousness grew it became an integral element of the core business policy. This paper explores the patterns business adopts as an efficient Social Audit System and tries to reach the aspirations of the stakeholders discloses the information and also develops a system of accountability which is best suited to the need of business and society. This paper systematically analyses guidelines and standards set by trans-governmental and private bodies to arrive at a regulatory perspective for Corporate Social Responsibility (CSR) and Social Auditing in Business.
499,Zhang2015, Mobile technology is changing everyone’s life and the way people learning too. It has been recognized as one of the most important innovations that influenced teaching and learning. Scholars found there are still several problems in mobile teaching and learning which including technical learning digital content development curriculum design education for both teachers and learners IP protection and stable of networks. To fulfill a real “anytime” and “anywhere” mobile learning it needs many efforts and collaborations. The following chapters will present the design towards a successful mobile teaching and learning program through literacy experience and case study.
500,Vlahogianni2015, With the overwhelming amount of transportation data being gathered worldwide Intelligent Transportation Systems (ITS) are faced with several modeling challenges. New modeling paradigms based on Computational Intelligence (CI) that take advantage of the advent of big datasets have been systematically proposed in literature. Transportation optimization problems form a research field that has systematically benefited from CI. Nevertheless when it comes to big data applications research is still at an early stage. This work attempts to review the unique opportunities provided by ITS and big data and discuss the emerging approaches for transportation modeling. The literature dedicated to big data transportation applications related to CI and optimization is reviewed. Finally the challenges and emerging opportunities for researchers working with such approaches are also acknowledged and discussed.
501,Cheek2015, Despite the evident success of current learning systems in Pre-K-12 education higher education and workforce training it is increasingly clear that these systems have failed to keep pace with what we know about how and under what circumstances human beings learn. Learning systems are highly impervious to the kinds of systemic changes required for twenty-first century learners including reliably learning and mastering new skills and knowledge on demand across their entire lifespans for both work and general success in life. At least five key areas must be worked on in an iterative and dynamic manner if we are going to make substantial progress in creating a sufficiently disruptive environment for learning to be substantially accelerated for ALL human learners in this new century. These five areas are: (1) the nature of learning across the human lifespan (2) recent and emerging technologies (3) assessment and evaluation (4) transitions from the old systems to the new “system” and (5) sociocultural dimensions from a global perspective. I briefly outline each of these areas. Design(ers) can and must play many important roles in helping to move this agenda forward. An R & D effort of unprecedented proportions is proposed and preliminary steps outlined.
502,ElNaqa2015, Machine learning is an evolving branch of computational algorithms that are designed to emulate human intelligence by learning from the surrounding environment. They are considered the working horse in the new era of the so-called big data. Techniques based on machine learning have been applied successfully in diverse fields ranging from pattern recognition computer vision spacecraft engineering finance entertainment and computational biology to biomedical and medical applications. More than half of the patients with cancer receive ionizing radiation (radiotherapy) as part of their treatment and it is the main treatment modality at advanced stages of local disease. Radiotherapy involves a large set of processes that not only span the period from consultation to treatment but also extend beyond that to ensure that the patients have received the prescribed radiation dose and are responding well. The degrees of the complexity of these processes can vary and may involve several stages of sophisticated human-machine interactions and decision making which would naturally invite the use of machine learning algorithms into optimizing and automating these processes including but not limited to radiation physics quality assurance contouring and treatment planning image-guided radiotherapy respiratory motion management treatment response modeling and outcomes prediction. The ability of machine learning algorithms to learn from current context and generalize into unseen tasks would allow improvements in both the safety and efficacy of radiotherapy practice leading to better outcomes.
503,Newman2015, What will students need to know and be able to do in order to thrive in our fast-changing complex and interconnected world  For educators today this is a driving question and one which social and emotional learning (SEL) addresses in a systematic way. SEL also provides a structure for organizing education to ensure students are equipped for the global future they will inherit. The purpose of this chapter is to explore how SEL can serve as an organizing framework to optimize education. We define social and emotional competencies and review evidence-based SEL programming in schools including recent research on the efficacy of SEL programs and specific program design characteristics. We conclude with case examples and anecdotes of implementation of SEL in school systems.
504,Moldovan2015, The objective of this paper is to identify the relationship between corporate governance variables and firm performance by employing data mining methods. We choose two dependent variables Tobin’s Q ratio and Altman Z-score as measures for the companies’ performances and apply machine learning techniques on the data collected from the components companies of three major stock indexes: S&P 500 STOXX Europe 600 and STOXX Eastern Europe 300. We use decision trees and logistic regressions as learning algorithms and then we compare their performances. For the US components we found a positive connection between the presence of women in the board and the company performance while in Western Europe that it is better to employ a larger audit committee in order to lower the bankruptcy risk. An independent chairperson is a positive factor related to Altman Z-score for the companies from Eastern Europe.
505,Bencze2015, The wellbeing of many individuals societies and environments is either dire or under serious threat due to decisions involving fields of science and technology. Arguably the most significant of these are linked to increasing global climate change but many of us are concerned for example with health problems (e.g. diabetes cardiovascular diseases and cancer) associated with manufactured foods and beverages death and destruction due to military invasions and conflicts and invasion of privacy through electronic technologies controlled by governments and corporations. From an actor network perspective culpability for such problems is complex diffuse and uncertain. However many suggestthat much fault lies with our current neoliberal capitalist system which is now highly globalized and strategic and to a great extent uses fields of technoscience to semiotically convince a relatively small fraction of the world’s population to repeatedly consume and discard goods and services and associated idealized conceptions while much of the rest of the world labours on their behalf and suffers a range of personal social and environmental problems. Such a vast and powerful system controlled largely by and mainly benefiting an elite cohort of financiers and corporations at the expense of much of the world needs dramatic reform leading to great improvements in social justice and environmental sustainability. Although it appears to be a mechanism for (re)producing problematic capitalist systems a site of such possible reform may be science education given its potential influences as a nearly ubiquitous social programme on public consciousness surrounding a key capitalist instrument that is fields of technoscience. Although the inertial nature of science education resists many reforms concepts and principles outlined in this chapter may contribute to positive change.
506,Rees2015, Mobile learning is a very exciting approach to learning that has the possibility of changing nursing education providing learning to nurses when and where they need it and in a manner that will achieve positive learning outcomes. Coming from an apprenticeship model in the military nurses have traditionally learned by seeing and then doing. Mobile learning through means such as YouTube and augmented reality offers the best of this traditional way of learning combined with time- and cost-efficient means of technology use and greater theoretical knowledge. Reaching nurses in rural and isolated communities is also possible through these means. This is achieved through the use of SMS and online learning that is able to be used at a time and place suitable for the nurse enabling them to include learning within their lives in a way that suits them. Many isolated trials have occurred in nursing education over the years starting with the use of PDAs and although many have shown success there is not a great deal of research that has been conducted in the use of mobile education in nursing. Considering this research was conducted using a grounded theory approach that investigated nurse’s current use of mobile technology and their beliefs around mobile learning. The study also explored how and when nurses are undertaking continuing education with the discovery of how they personally resource their learning. When looking at trials of mobile learning within nursing education it is apparent from these trials and the study that nurses are ready for mobile learning and that mobile learning shows great potential as a method for education within the nursing profession.
507,Zhang2015a, A challenging problem of image retrieval is the similarity learning between images. To improve similarity search in Content-Based Image Retrieval (CBIR) many studies on distance metric learning have been published. Despite their success most existing methods are limited in two aspects: (i) they usually attempt to learn a linear distance metric which limits their capacity of measuring similarity for complex applications; (ii) they are often designed for learning metrics on unique-modal data which could be suboptimal for similarity learning on multimedia objects with multiple feature representations. To overcome these limitations in this paper we investigate the online kernel-based multimodal similarity learning which aims to integrate multiple kernels for learning multimodal similarity functions and conduct experiments to evaluate the performance of the proposed method for CBIR on several different image datasets. The experiment results are encouraging and verify the effectiveness and the superiority of the proposed method.
508,Desell2015, This paper presents a novel strategy for using ant colony optimization (ACO) to evolve the structure of deep recurrent neural networks. While versions of ACO for continuous parameter optimization have been previously used to train the weights of neural networks to the authors’ knowledge they have not been used to actually design neural networks. The strategy presented is used to evolve deep neural networks with up to 5 hidden and 5 recurrent layers for the challenging task of predicting general aviation flight data and is shown to provide improvements of 63 % for airspeed a 97 % for altitude and 120 % for pitch over previously best published results while at the same time not requiring additional input neurons for residual values. The strategy presented also has many benefits for neuro evolution including the fact that it is easily parallizable and scalable and can operate using any method for training neural networks. Further the networks it evolves can typically be trained in fewer iterations than fully connected networks.
509,Koh2015, With the rapid development of ICT in education demand for teachers to be able to design ICT-infused lessons is also on the rise. As such preservice teachers need to be equipped with design-thinking skills so that they can design effective lessons. This chapter discusses the relationships between design thinking and preservice teacher preparation. It then examines two case studies that focus on promoting Taiwanese preservice teachers’ design-thinking capacity with the help of an innovative pedagogical approach called knowledge building. Potentials and challenges associated with knowledge-building practices for fostering design thinking among preservice teachers are discussed.
510,Moeed2015, When what and how students learn is influenced by motivation (Schunk 1991). Motivation is needed initially to get the students to engage in learning and it is needed throughout the knowledge construction process. Motivation is a much researched area in teaching and learning but little is known about motivation to learn through science investigation beyond teachers’ view that students enjoy it and students saying that it is a better alternative to written work. Findings reported here are the insights provided by the students through a survey focus group interviews conversations in the classroom and observations. Students’ engagement with the task was high during the practical part of the investigation. The findings suggest that motivation came from: the variety of task; finding out something ‘new’; having fun; getting good grades in assessment; and not having to write notes was motivational. Students found demotivating: repetition; lack of cognitive challenge; and investigating something where the answers were obvious.
511,Wiesner2015, Engineering of systems is highly influenced by rapid technological changes such as the emergence of Cyber-Physical Systems (CPS). These are interconnected embedded systems enabled by human-machine interaction. Realisation of CPS require a collaboration between mechanical engineering electrical engineering and computer science since different components are delivered and developed by different disciplines. Consequently the Requirements Engineering (RE) process needs to consider multi-disciplinary perspectives. The objective of this paper is to elaborate the specific challenges of RE for CPS in a distributed environment and to identify knowledge sources and targets in CPS engineering. It describes the relevant types of knowledge and defines appropriate exchange mechanisms and standards.
512,Lynch2014, Neurotechnology is used to understand and influence the brain and nervous system for the purposes of improving health education entertainment and information technology. Emerging areas of neurotech development that will create substantial value in the coming decade include therapeutic optogenetic modulation neuromorphic computing neurogenomics brain computer interfaces neural stem cells transcranial electrical modulation and neurogaming. As these enabling technologies develop and converge they will make possible completely novel applications including tools that create neurocompetitive advantages; therapeutic restoration technologies; self-learning hyperefficient neuromorphic computing systems; neuroexperience marketplaces; human resiliency solutions; neurorobotic interfaces; and many others. Achieving these breakthroughs will require sustained support from both public and private capital sources.
513,Christensen2014, Traditional and emerging forms of assessment for measuring technology readiness are  presented in the context of society’s need for assessing twenty-first century skills. Workforce preparation is identified as a driving force for new forms of assessment while rapid advances in information technologies offer opportunities for new techniques to emerge. Recent approaches to learning such as digital game environments demonstrate that alternative forms of assessment are emerging to fulfill these changing needs. In this chapter the need for technology readiness is introduced in the context of assessing twenty-first century skills. Conceptual and practical considerations are addressed within the categories of foundation skills technology applications attitudes toward technology communicating with technology and digital citizenship. A presentation of emerging assessment techniques leads to discussion of the importance of technology readiness for preparing a productive workforce. In addition this chapter includes prospects for forms of assessment unique to new digital media.
514,Crick2012, This chapter develops a definition of engagement which is underpinned by a participatory enquiry paradigm and invites an exploration of patterns and relationships between variables rather than a focus on a single variable. It suggests that engagement is best understood as a complex system including a range of interrelated factors internal and external to the learner in place and in time which shape his or her engagement with learning opportunities. The implications of this approach are explored first in terms of student identity learning power and competences and second in terms of student participation in the construction of knowledge through authentic enquiry. Examples are used to illustrate the arguments which have been generated from research into the theory and practice of Learning Power and from the Learning Futures programme in the UK and Australia. The chapter argues that what is necessary for deep engagement in the twenty-first century is a pedagogy and an assessment system which empower  individuals to become aware of their identity as learners through making choices about what where and how they learn and to make meaningful connections with their life stories and aspirations in authentic pedagogy. In this context the teacher is a facilitator or coach for learning rather than a purveyor of expert knowledge.
